"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[904],{1165:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"topics/deep-learning/nlp/transformer-architecture","title":"Transformer Architecture ","description":"---","source":"@site/docs/topics/deep-learning/nlp/transformer-architecture.md","sourceDirName":"topics/deep-learning/nlp","slug":"/topics/deep-learning/nlp/transformer-architecture","permalink":"/docs/topics/deep-learning/nlp/transformer-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/nlp/transformer-architecture.md","tags":[{"inline":true,"label":"transformers","permalink":"/docs/tags/transformers"},{"inline":true,"label":"nlp","permalink":"/docs/tags/nlp"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"transformer-architecture","title":"Transformer Architecture ","tags":["transformers","nlp","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"What Is Self-Attention?","permalink":"/docs/topics/deep-learning/nlp/self-attention"},"next":{"title":"Transformers and Attention \u2013 Explained with Visuals","permalink":"/docs/topics/deep-learning/nlp/transformer-attention"}}');var t=r(4848),i=r(8453);const o={id:"transformer-architecture",title:"Transformer Architecture ",tags:["transformers","nlp","neural networks"]},l="Transformer Architecture",d={},a=[{value:"\ud83c\udfaf Objective: What Does a Transformer LLM Do?",id:"-objective-what-does-a-transformer-llm-do",level:2},{value:"\ud83d\udd24 Step 1: Tokenizer \u2013 Turning Text into Tokens",id:"-step-1-tokenizer--turning-text-into-tokens",level:2},{value:"\ud83d\udd22 Step 2: Token Embeddings \u2013 IDs Become Vectors",id:"-step-2-token-embeddings--ids-become-vectors",level:2},{value:"\ud83e\udde0 Step 3: Stack of Transformer Blocks",id:"-step-3-stack-of-transformer-blocks",level:2},{value:"\ud83e\udde0 How Attention Works Inside Blocks",id:"-how-attention-works-inside-blocks",level:2},{value:"\ud83e\udde0 Step 4: Language Modeling Head (LM Head)",id:"-step-4-language-modeling-head-lm-head",level:2},{value:"\ud83d\udd04 Full Pipeline: From Prompt to Response",id:"-full-pipeline-from-prompt-to-response",level:2},{value:"\ud83e\udde9 Summary Diagram Components",id:"-summary-diagram-components",level:2}];function c(e){const n={blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"transformer-architecture",children:"Transformer Architecture"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-objective-what-does-a-transformer-llm-do",children:"\ud83c\udfaf Objective: What Does a Transformer LLM Do?"}),"\n",(0,t.jsx)(n.p,{children:"A Transformer-based Large Language Model (LLM) takes a natural language input (like a question or instruction) and generates meaningful output (a word, phrase, sentence, or full story)."}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Example Input: ",(0,t.jsx)(n.em,{children:'"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened."'})]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Output: ",(0,t.jsx)(n.em,{children:'"Dear Sarah, I\u2019m so sorry..."'})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This process is powered by ",(0,t.jsx)(n.strong,{children:"three major components"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Tokenizer"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transformer Blocks"})," (the core brain)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LM Head"})," (for word prediction)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-step-1-tokenizer--turning-text-into-tokens",children:"\ud83d\udd24 Step 1: Tokenizer \u2013 Turning Text into Tokens"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"tokenizer"})," is the first step. It breaks raw text into chunks the model can understand \u2014 usually ",(0,t.jsx)(n.strong,{children:"subword tokens"}),"."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["Input: ",(0,t.jsx)(n.em,{children:'"Explain how it happened."'})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Tokenized as:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'["Explain", "how", "it", "happen", "##ed", "."]\n'})}),"\n",(0,t.jsx)(n.p,{children:'Why subwords? Because it helps handle rare or unseen words (e.g., "gardener" \u2192 "garden" + "##er").'}),"\n",(0,t.jsxs)(n.p,{children:["The tokenizer also maps tokens to ",(0,t.jsx)(n.strong,{children:"IDs"})," using a ",(0,t.jsx)(n.strong,{children:"token vocabulary"})," \u2014 like a dictionary that assigns each token a number."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-step-2-token-embeddings--ids-become-vectors",children:"\ud83d\udd22 Step 2: Token Embeddings \u2013 IDs Become Vectors"}),"\n",(0,t.jsxs)(n.p,{children:["Each token ID is mapped to a ",(0,t.jsx)(n.strong,{children:"learned vector"})," (called a token embedding)."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Vocabulary: 50,000+ tokens"}),"\n",(0,t.jsx)(n.li,{children:"Embedding size: typically 768, 1024, etc."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"So the sentence becomes a matrix of token embeddings:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'["Explain"] \u2192 [0.56, -0.12, ..., 0.77]\n["how"]     \u2192 [...]\n...\n'})}),"\n",(0,t.jsxs)(n.p,{children:["These embeddings are now ",(0,t.jsx)(n.strong,{children:"fed into the Transformer Blocks"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-step-3-stack-of-transformer-blocks",children:"\ud83e\udde0 Step 3: Stack of Transformer Blocks"}),"\n",(0,t.jsx)(n.p,{children:"This is where the model \u201cthinks.\u201d"}),"\n",(0,t.jsx)(n.p,{children:"Each block contains:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Self-Attention Layer"})," \u2013 lets each word attend to others"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedforward Neural Network"})," \u2013 transforms and mixes information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Layer Norm + Residual Connections"})," \u2013 stabilize and preserve input/output flow"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The blocks are stacked ",(0,t.jsx)(n.strong,{children:"N times"})," (e.g., 12 in BERT base, 96 in GPT-4). Each block refines the understanding of the sentence."]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"Think of it as passing the message through N layers of wise interpreters."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"(Slide 3)"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-how-attention-works-inside-blocks",children:"\ud83e\udde0 How Attention Works Inside Blocks"}),"\n",(0,t.jsxs)(n.p,{children:["Every token looks at every other token and ",(0,t.jsx)(n.strong,{children:"weighs them"})," using dot products (Q \xb7 K). It gets back a score and combines the values (V)."]}),"\n",(0,t.jsx)(n.p,{children:"For example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Token: ",(0,t.jsx)(n.em,{children:'"it"'})]}),"\n",(0,t.jsxs)(n.li,{children:["Might attend more to: ",(0,t.jsx)(n.em,{children:'"happen"'})," and ",(0,t.jsx)(n.em,{children:'"Explain"'})," than ",(0,t.jsx)(n.em,{children:'"Write"'})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This lets the model understand syntax, subject-object relationships, and even tone."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-step-4-language-modeling-head-lm-head",children:"\ud83e\udde0 Step 4: Language Modeling Head (LM Head)"}),"\n",(0,t.jsxs)(n.p,{children:["After processing, the model needs to predict the ",(0,t.jsx)(n.strong,{children:"next token"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["It uses a ",(0,t.jsx)(n.strong,{children:"linear layer"})," to project the final hidden states back to the vocabulary size (e.g., 50,000)"]}),"\n",(0,t.jsxs)(n.li,{children:["Then applies ",(0,t.jsx)(n.strong,{children:"softmax"})," to get probabilities for each possible token"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:'Highest score \u2192 output token (e.g., "Dear")'}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This process repeats token-by-token during generation."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-full-pipeline-from-prompt-to-response",children:"\ud83d\udd04 Full Pipeline: From Prompt to Response"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Text prompt \u2192 Tokenizer \u2192 Tokens (IDs)"}),"\n",(0,t.jsx)(n.li,{children:"Token IDs \u2192 Embedding lookup"}),"\n",(0,t.jsx)(n.li,{children:"Embeddings \u2192 Transformer blocks (with attention)"}),"\n",(0,t.jsx)(n.li,{children:"Output \u2192 LM head \u2192 next word"}),"\n",(0,t.jsx)(n.li,{children:"Feed generated word back in \u2192 repeat"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-summary-diagram-components",children:"\ud83e\udde9 Summary Diagram Components"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Function"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Tokenizer"}),(0,t.jsx)(n.td,{children:"Breaks input text into tokens/IDs"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Token Vocabulary"}),(0,t.jsx)(n.td,{children:"Maps tokens to IDs"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Token Embeddings"}),(0,t.jsx)(n.td,{children:"Turns token IDs into dense vectors"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Transformer Blocks"}),(0,t.jsx)(n.td,{children:"Contextualize and transform input"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"LM Head"}),(0,t.jsx)(n.td,{children:"Predicts next word/token"})]})]})]}),"\n",(0,t.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var s=r(6540);const t={},i=s.createContext(t);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);