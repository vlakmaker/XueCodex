"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[7802],{3931:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"topics/deep-learning/neural-networks/neural-networks","title":"What are Neural Networks?","description":"Understanding Neural Networks, Weights, and Stochastic Gradient Descent","source":"@site/docs/topics/deep-learning/neural-networks/what-are-neural-networks.md","sourceDirName":"topics/deep-learning/neural-networks","slug":"/topics/deep-learning/neural-networks/neural-networks","permalink":"/docs/topics/deep-learning/neural-networks/neural-networks","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/neural-networks/what-are-neural-networks.md","tags":[{"inline":true,"label":"machine-learning","permalink":"/docs/tags/machine-learning"},{"inline":true,"label":"deep learning","permalink":"/docs/tags/deep-learning"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"neural-networks","title":"What are Neural Networks?","tags":["machine-learning","deep learning","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"weights","permalink":"/docs/topics/deep-learning/neural-networks/weights"},"next":{"title":"Language as a Bag-of-Words","permalink":"/docs/topics/deep-learning/nlp/bag-of-words"}}');var r=t(4848),s=t(8453);const a={id:"neural-networks",title:"What are Neural Networks?",tags:["machine-learning","deep learning","neural networks"]},o="Neural Networks",l={},h=[{value:"Understanding Neural Networks, Weights, and Stochastic Gradient Descent",id:"understanding-neural-networks-weights-and-stochastic-gradient-descent",level:2},{value:"What Are Neural Networks?",id:"what-are-neural-networks",level:3},{value:"How a Neural Network Works",id:"how-a-neural-network-works",level:4},{value:"What Are Weights in a Neural Network?",id:"what-are-weights-in-a-neural-network",level:3},{value:"Example: House Price Prediction",id:"example-house-price-prediction",level:4},{value:"How Weights Are Determined",id:"how-weights-are-determined",level:3},{value:"Stochastic Gradient Descent (SGD): How the Model Learns",id:"stochastic-gradient-descent-sgd-how-the-model-learns",level:2},{value:"Understanding the Concept: Climbing Down a Mountain",id:"understanding-the-concept-climbing-down-a-mountain",level:3},{value:"Breaking Down the Process",id:"breaking-down-the-process",level:3},{value:"Why Stochastic Gradient Descent?",id:"why-stochastic-gradient-descent",level:2},{value:"Summary of Key Concepts",id:"summary-of-key-concepts",level:2},{value:"Final Analogy: Making the Perfect Cup of Coffee",id:"final-analogy-making-the-perfect-cup-of-coffee",level:3}];function c(e){const n={h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"neural-networks",children:"Neural Networks"})}),"\n",(0,r.jsx)(n.h2,{id:"understanding-neural-networks-weights-and-stochastic-gradient-descent",children:"Understanding Neural Networks, Weights, and Stochastic Gradient Descent"}),"\n",(0,r.jsx)(n.h3,{id:"what-are-neural-networks",children:"What Are Neural Networks?"}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"neural network"})," is a type of machine learning model inspired by the way human brains process information. It consists of layers of artificial neurons that take in data, process it, and produce an output. These networks are used for a variety of tasks, including image recognition, language translation, and even playing games."]}),"\n",(0,r.jsx)(n.h4,{id:"how-a-neural-network-works",children:"How a Neural Network Works"}),"\n",(0,r.jsx)(n.p,{children:"Think of a neural network as a team of detectives solving a case:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Each detective (neuron) gathers clues (input data)."}),"\n",(0,r.jsx)(n.li,{children:"They communicate with each other, refining the information (hidden layers)."}),"\n",(0,r.jsx)(n.li,{children:"Finally, they determine who the culprit is (output layer)."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:['Just like a detective team might adjust their level of trust in different sources, a neural network adjusts the "importance" of different inputs. This is where ',(0,r.jsx)(n.strong,{children:"weights"})," come in."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"what-are-weights-in-a-neural-network",children:"What Are Weights in a Neural Network?"}),"\n",(0,r.jsxs)(n.p,{children:["Weights are the ",(0,r.jsx)(n.strong,{children:"key decision-making factors"})," in a neural network. They determine how much influence an input has on the final prediction."]}),"\n",(0,r.jsx)(n.h4,{id:"example-house-price-prediction",children:"Example: House Price Prediction"}),"\n",(0,r.jsx)(n.p,{children:"Imagine you're predicting house prices based on three factors:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Size of the house"}),"\n",(0,r.jsx)(n.li,{children:"Number of bedrooms"}),"\n",(0,r.jsx)(n.li,{children:"Location"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Each of these factors contributes differently to the final price. If location is the most important, it should have the highest ",(0,r.jsx)(n.strong,{children:"weight"}),". The formula might look something like this:"]}),"\n",(0,r.jsx)(n.p,{children:"Price = 0.3 * Size + 0.2 * Bedrooms + 0.5 * Location"}),"\n",(0,r.jsxs)(n.p,{children:["Here, ",(0,r.jsx)(n.strong,{children:"0.3, 0.2, and 0.5"})," are the weights, showing how much each factor influences the price. The model learns these weights by trial and error, improving them over time through ",(0,r.jsx)(n.strong,{children:"training"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"how-weights-are-determined",children:"How Weights Are Determined"}),"\n",(0,r.jsx)(n.p,{children:"At first, the network assigns random weights. Then, it makes a prediction and checks how far it is from the correct answer. The model then adjusts the weights slightly to make the prediction more accurate next time."}),"\n",(0,r.jsxs)(n.p,{children:["This process repeats thousands (or millions) of times until the weights are ",(0,r.jsx)(n.strong,{children:"optimized"}),". The goal is to minimize the error as much as possible."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"stochastic-gradient-descent-sgd-how-the-model-learns",children:"Stochastic Gradient Descent (SGD): How the Model Learns"}),"\n",(0,r.jsx)(n.h3,{id:"understanding-the-concept-climbing-down-a-mountain",children:"Understanding the Concept: Climbing Down a Mountain"}),"\n",(0,r.jsx)(n.p,{children:"Imagine you are standing on top of a mountain, blindfolded, trying to find the lowest point in the valley. You take small steps downhill, checking each time if you\u2019re getting lower. If yes, you continue in that direction. If not, you change course."}),"\n",(0,r.jsxs)(n.p,{children:["This is exactly how ",(0,r.jsx)(n.strong,{children:"Stochastic Gradient Descent (SGD)"})," works:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"It looks at the current prediction error."}),"\n",(0,r.jsx)(n.li,{children:"It calculates how much the weights need to change."}),"\n",(0,r.jsx)(n.li,{children:"It takes a small step (adjusting weights slightly)."}),"\n",(0,r.jsx)(n.li,{children:"It repeats until the error is as small as possible."}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"breaking-down-the-process",children:"Breaking Down the Process"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Forward Propagation"})," \u2013 Data passes through the network, and a prediction is made."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loss Calculation"})," \u2013 The model compares the prediction with the actual answer (error is calculated)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Backpropagation"})," \u2013 The model figures out how to change the weights to reduce the error."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Weight Update (Gradient Descent)"})," \u2013 The weights are adjusted slightly in the correct direction."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Repeat"})," \u2013 The process continues until the model becomes highly accurate."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"why-stochastic-gradient-descent",children:"Why Stochastic Gradient Descent?"}),"\n",(0,r.jsxs)(n.p,{children:["The term ",(0,r.jsx)(n.strong,{children:"stochastic"})," means \u201crandom.\u201d Instead of looking at all the data at once, SGD picks small random samples, making learning faster and more efficient. It avoids getting stuck in bad solutions and helps the model learn in a way that works well for large datasets."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary-of-key-concepts",children:"Summary of Key Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Neural Networks"}),": Systems that learn patterns from data, similar to human brains."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Weights"}),": Numbers that determine how much influence an input has on the final prediction."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gradient Descent"}),": A method for adjusting weights to reduce errors."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stochastic Gradient Descent (SGD)"}),": A faster way of updating weights using small random samples of data."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"final-analogy-making-the-perfect-cup-of-coffee",children:"Final Analogy: Making the Perfect Cup of Coffee"}),"\n",(0,r.jsx)(n.p,{children:"Think of training a neural network like perfecting a coffee recipe:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"You start with random amounts of coffee, water, and sugar (random weights)."}),"\n",(0,r.jsx)(n.li,{children:"You take a sip and realize it's too bitter (error calculation)."}),"\n",(0,r.jsx)(n.li,{children:"You adjust the sugar and water slightly (gradient descent updates weights)."}),"\n",(0,r.jsx)(n.li,{children:"You keep tweaking until the taste is just right (minimized error)."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"With this process, the model (or the coffee) keeps getting better over time!"})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(6540);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);