"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[9279],{2058:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"topics/deep-learning/convert-words-features","title":"Converting Words to Features in NLP","description":"\ud83d\udfe2 1. One-Hot Encoding","source":"@site/docs/topics/deep-learning/convert-words-featured.md","sourceDirName":"topics/deep-learning","slug":"/topics/deep-learning/convert-words-features","permalink":"/docs/topics/deep-learning/convert-words-features","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/convert-words-featured.md","tags":[{"inline":true,"label":"deep learning","permalink":"/docs/tags/deep-learning"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"},{"inline":true,"label":"words","permalink":"/docs/tags/words"}],"version":"current","frontMatter":{"id":"convert-words-features","title":"Converting Words to Features in NLP","tags":["deep learning","neural networks","words"]},"sidebar":"tutorialSidebar","previous":{"title":"Activation Functions for Humans: Sigmoid, Tanh, and ReLU (Deep Dive)","permalink":"/docs/topics/deep-learning/activation-functions"},"next":{"title":"Converting Words to Features in NLP","permalink":"/docs/topics/deep-learning/document-categorization-prediction"}}');var i=s(4848),d=s(8453);const t={id:"convert-words-features",title:"Converting Words to Features in NLP",tags:["deep learning","neural networks","words"]},o="Converting Words to Features in NLP",l={},c=[{value:"\ud83d\udfe2 1. <strong>One-Hot Encoding</strong>",id:"-1-one-hot-encoding",level:3},{value:"\ud83d\udfe2 2. <strong>Bag of Words (BoW)</strong>",id:"-2-bag-of-words-bow",level:3},{value:"\ud83d\udfe2 3. <strong>Embeddings</strong>",id:"-3-embeddings",level:3},{value:"\ud83d\udfe2 4. <strong>Embedding Bag</strong>",id:"-4-embedding-bag",level:3},{value:"\ud83d\udd27 5. <strong>Using Embeddings in PyTorch</strong>",id:"-5-using-embeddings-in-pytorch",level:3},{value:"\u2705 <strong>Summary Recap</strong>",id:"-summary-recap",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"converting-words-to-features-in-nlp",children:(0,i.jsx)(n.strong,{children:"Converting Words to Features in NLP"})})}),"\n",(0,i.jsxs)(n.h3,{id:"-1-one-hot-encoding",children:["\ud83d\udfe2 1. ",(0,i.jsx)(n.strong,{children:"One-Hot Encoding"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsx)(n.p,{children:"A way to represent words as vectors with the same length as the vocabulary size. Each word is assigned a unique index. The one-hot vector has a 1 at the index of that word and 0s elsewhere."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.p,{children:'Vocabulary = "I","like","cats""I", "like", "cats""I","like","cats"'}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'"I" \u2192 1,0,01, 0, 01,0,0'}),"\n",(0,i.jsx)(n.li,{children:'"like" \u2192 0,1,00, 1, 00,1,0'}),"\n",(0,i.jsx)(n.li,{children:'"cats" \u2192 0,0,10, 0, 10,0,1'}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it\u2019s used:"})}),"\n",(0,i.jsx)(n.p,{children:"It's easy to understand and implement. It ensures each word has a unique representation."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Doesn\u2019t capture meaning or similarity (e.g., \u201ccat\u201d and \u201ckitten\u201d are just as unrelated as \u201ccat\u201d and \u201cbanana\u201d)."}),"\n",(0,i.jsx)(n.li,{children:"High memory usage with large vocabularies (sparse vectors)."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-2-bag-of-words-bow",children:["\ud83d\udfe2 2. ",(0,i.jsx)(n.strong,{children:"Bag of Words (BoW)"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsx)(n.p,{children:"A way to represent a whole sentence or document as a single vector. You sum or average the one-hot vectors of all the words in the sentence."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.p,{children:'Sentence: "I like cats"'}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"BoW vector = 1,1,11, 1, 11,1,1 (just sum of the individual one-hot vectors)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it\u2019s used:"})}),"\n",(0,i.jsx)(n.p,{children:"Gives a simple summary of word presence/frequency in a document."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Word order is lost."}),"\n",(0,i.jsx)(n.li,{children:"Still uses sparse vectors."}),"\n",(0,i.jsx)(n.li,{children:"No semantic information."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-3-embeddings",children:["\ud83d\udfe2 3. ",(0,i.jsx)(n.strong,{children:"Embeddings"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsx)(n.p,{children:"Instead of sparse one-hot vectors, embeddings map words to dense vectors in a lower-dimensional space. Each word gets a vector of, say, 50 or 100 dimensions, learned during training."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'"cat" \u2192 0.23,\u22121.5,0.88,...0.23, -1.5, 0.88, ...0.23,\u22121.5,0.88,...'}),"\n",(0,i.jsx)(n.li,{children:'"kitten" might have a similar vector, capturing their semantic closeness.'}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implemented via:"})}),"\n",(0,i.jsxs)(n.p,{children:["An ",(0,i.jsx)(n.strong,{children:"embedding matrix"}),", where:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Rows = words"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Columns = embedding dimensions"}),"\n",(0,i.jsx)(n.p,{children:"(If 10,000 words in vocab and 100-dim embeddings, matrix = 10,000 \xd7 100)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Captures similarity between words."}),"\n",(0,i.jsx)(n.li,{children:"Efficient: low-dimensional and dense."}),"\n",(0,i.jsx)(n.li,{children:"Learnable: vectors are trained during model training."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-4-embedding-bag",children:["\ud83d\udfe2 4. ",(0,i.jsx)(n.strong,{children:"Embedding Bag"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsxs)(n.p,{children:["An efficient way to combine multiple word embeddings (from a sentence or document) into one vector by summing or averaging them, ",(0,i.jsx)(n.strong,{children:"without manually handling one-hot vectors"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it\u2019s better than BoW:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Works directly with token indices."}),"\n",(0,i.jsx)(n.li,{children:"Avoids overhead of creating/summing one-hot vectors."}),"\n",(0,i.jsx)(n.li,{children:"Built-in support for batching multiple documents with offsets."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Offsets:"})}),"\n",(0,i.jsx)(n.p,{children:"In a batched setting (e.g., 3 documents):"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Index tensor = all token indices across all docs."}),"\n",(0,i.jsx)(n.li,{children:"Offset tensor = starting index of each doc in the index tensor."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"PyTorch class:"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"nn.EmbeddingBag(num_embeddings, embedding_dim, mode='mean')"})}),"\n",(0,i.jsxs)(n.p,{children:["Mode can be ",(0,i.jsx)(n.code,{children:"'sum'"}),", ",(0,i.jsx)(n.code,{children:"'mean'"}),", or ",(0,i.jsx)(n.code,{children:"'max'"}),"."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-5-using-embeddings-in-pytorch",children:["\ud83d\udd27 5. ",(0,i.jsx)(n.strong,{children:"Using Embeddings in PyTorch"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tokenization:"})," Break text into words."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vocabulary creation:"})," Assign an index to each word."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedding layer:"})," ",(0,i.jsx)(n.code,{children:"nn.Embedding(vocab_size, embed_dim)"})," maps token indices to vectors."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"EmbeddingBag layer (optional):"})," ",(0,i.jsx)(n.code,{children:"nn.EmbeddingBag(...)"})," aggregates word embeddings for whole sentences or documents efficiently."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feeding to a model:"})," These dense vectors go into your neural network as features."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Code Snippet:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'python\nCopyEdit\nembedding = nn.Embedding(num_embeddings=10_000, embedding_dim=100)\nindices = torch.tensor([1, 5, 8])  # e.g., "I like cats"\noutput = embedding(indices)\n\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"With EmbeddingBag:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python\nCopyEdit\nembedding_bag = nn.EmbeddingBag(num_embeddings=10_000, embedding_dim=100, mode='mean')\nindices = torch.tensor([1, 5, 8, 2, 3, 6])  # tokens from multiple docs\noffsets = torch.tensor([0, 3])  # starting positions for each doc\noutput = embedding_bag(indices, offsets)\n\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-summary-recap",children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Summary Recap"})]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Concept"}),(0,i.jsx)(n.th,{children:"Purpose"}),(0,i.jsx)(n.th,{children:"Pros"}),(0,i.jsx)(n.th,{children:"Cons"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"One-Hot Encoding"}),(0,i.jsx)(n.td,{children:"Represent each word as unique ID"}),(0,i.jsx)(n.td,{children:"Simple, fast for small vocab"}),(0,i.jsx)(n.td,{children:"Sparse, doesn\u2019t capture meaning"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Bag of Words"}),(0,i.jsx)(n.td,{children:"Represent a document as word count"}),(0,i.jsx)(n.td,{children:"Easy doc-level summary"}),(0,i.jsx)(n.td,{children:"Loses order, sparse, no context"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Embeddings"}),(0,i.jsx)(n.td,{children:"Dense word representation"}),(0,i.jsx)(n.td,{children:"Captures similarity, efficient, trainable"}),(0,i.jsx)(n.td,{children:"Needs training or pre-trained vectors"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Embedding Bag"}),(0,i.jsx)(n.td,{children:"Efficient aggregated embeddings"}),(0,i.jsx)(n.td,{children:"Great for sentence/doc-level input, fast"}),(0,i.jsx)(n.td,{})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var r=s(6540);const i={},d=r.createContext(i);function t(e){const n=r.useContext(d);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(d.Provider,{value:n},e.children)}}}]);