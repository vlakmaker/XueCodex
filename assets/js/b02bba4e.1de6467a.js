"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[4399],{6460:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"topics/deep-learning/nlp/encoder-decoder","title":"Encoders, Decoders, and Attention","description":"---","source":"@site/docs/topics/deep-learning/nlp/encoder-decoder.md","sourceDirName":"topics/deep-learning/nlp","slug":"/topics/deep-learning/nlp/encoder-decoder","permalink":"/docs/topics/deep-learning/nlp/encoder-decoder","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/nlp/encoder-decoder.md","tags":[{"inline":true,"label":"encoder","permalink":"/docs/tags/encoder"},{"inline":true,"label":"decoder","permalink":"/docs/tags/decoder"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"encoder-decoder","title":"Encoders, Decoders, and Attention","tags":["encoder","decoder","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"What Are Embeddings","permalink":"/docs/topics/deep-learning/nlp/embeddings"},"next":{"title":"Language AI Tasks (NLP Task Overview)","permalink":"/docs/topics/deep-learning/nlp/language-ai-tasks"}}');var s=r(4848),d=r(8453);const o={id:"encoder-decoder",title:"Encoders, Decoders, and Attention",tags:["encoder","decoder","neural networks"]},i="\ud83e\udde0 Encoders, Decoders, and Attention",l={},c=[{value:"\ud83d\udd04 What Are Encoders and Decoders?",id:"-what-are-encoders-and-decoders",level:2},{value:"\ud83d\udd0d What Does an Encoder Do?",id:"-what-does-an-encoder-do",level:2},{value:"Example:",id:"example",level:3},{value:"\ud83d\udce4 What Does a Decoder Do?",id:"-what-does-a-decoder-do",level:2},{value:"Example:",id:"example-1",level:3},{value:"\ud83e\udde0 Why Is Attention So Important?",id:"-why-is-attention-so-important",level:2},{value:"Example:",id:"example-2",level:3},{value:"\ud83e\uddf2 How Attention Works (Simplified)",id:"-how-attention-works-simplified",level:2},{value:"\ud83c\udfd7\ufe0f Encoder-Decoder Architectures in Practice",id:"\ufe0f-encoder-decoder-architectures-in-practice",level:2},{value:"\ud83e\uddfe Classic Seq2Seq (RNN-based)",id:"-classic-seq2seq-rnn-based",level:3},{value:"\ud83d\udd25 Transformer (like T5, BART)",id:"-transformer-like-t5-bart",level:3},{value:"\ud83e\uddd9\u200d\u2642\ufe0f GPT (Decoder-only Transformer)",id:"\ufe0f-gpt-decoder-only-transformer",level:3},{value:"\ud83e\uddf1 BERT (Encoder-only Transformer)",id:"-bert-encoder-only-transformer",level:3},{value:"\ud83d\udca1 Summary",id:"-summary",level:2},{value:"\ud83d\ude80 Why This Matters",id:"-why-this-matters",level:2}];function a(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"-encoders-decoders-and-attention",children:"\ud83e\udde0 Encoders, Decoders, and Attention"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-what-are-encoders-and-decoders",children:"\ud83d\udd04 What Are Encoders and Decoders?"}),"\n",(0,s.jsxs)(n.p,{children:["Encoders and decoders are the ",(0,s.jsx)(n.strong,{children:"core building blocks"})," in many modern neural network architectures, especially for tasks involving sequences \u2014 like translation, summarization, or text generation."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Think of them as a two-part system:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\ud83e\udde0 The ",(0,s.jsx)(n.strong,{children:"encoder"})," reads and understands the input."]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udde3\ufe0f The ",(0,s.jsx)(n.strong,{children:"decoder"})," generates the output based on that understanding."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["They first appeared together in ",(0,s.jsx)(n.strong,{children:"Sequence-to-Sequence (Seq2Seq)"})," models, and are foundational to ",(0,s.jsx)(n.strong,{children:"transformers"}),", ",(0,s.jsx)(n.strong,{children:"BERT"}),", ",(0,s.jsx)(n.strong,{children:"GPT"}),", ",(0,s.jsx)(n.strong,{children:"T5"}),", and more."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-what-does-an-encoder-do",children:"\ud83d\udd0d What Does an Encoder Do?"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"encoder"})," takes the input sequence and compresses it into a ",(0,s.jsx)(n.strong,{children:"meaningful representation"}),' (often called a "context vector" or "embedding").']}),"\n",(0,s.jsx)(n.h3,{id:"example",children:"Example:"}),"\n",(0,s.jsxs)(n.p,{children:["Input: ",(0,s.jsx)(n.code,{children:'"The cat sat on the mat."'})]}),"\n",(0,s.jsx)(n.p,{children:"The encoder:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Tokenizes the sentence"}),"\n",(0,s.jsx)(n.li,{children:"Embeds each token (word \u2192 vector)"}),"\n",(0,s.jsx)(n.li,{children:"Processes the vectors using layers (like RNNs or transformers)"}),"\n",(0,s.jsxs)(n.li,{children:["Outputs a ",(0,s.jsx)(n.strong,{children:"sequence of context-rich vectors"})," that summarize the input"]}),"\n"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["\ud83d\udca1 In transformers, the encoder outputs a ",(0,s.jsx)(n.strong,{children:"vector per input token"}),", all infused with contextual relationships."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-what-does-a-decoder-do",children:"\ud83d\udce4 What Does a Decoder Do?"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"decoder"})," generates the output ",(0,s.jsx)(n.strong,{children:"one token at a time"}),", using the encoded input and previously generated tokens."]}),"\n",(0,s.jsx)(n.h3,{id:"example-1",children:"Example:"}),"\n",(0,s.jsxs)(n.p,{children:["Task: Translate ",(0,s.jsx)(n.code,{children:'"The cat sat on the mat."'})," to French"]}),"\n",(0,s.jsx)(n.p,{children:"Decoder starts with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[",(0,s.jsx)(n.code,{children:"<sos>"}),"] (start of sentence token)"]}),"\n",(0,s.jsx)(n.li,{children:'Predicts "Le"'}),"\n",(0,s.jsx)(n.li,{children:'Feeds "Le" back in, predicts "chat"'}),"\n"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["\ud83d\udd01 It\u2019s ",(0,s.jsx)(n.strong,{children:"autoregressive"}),": each output depends on previous outputs + the encoded input."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-why-is-attention-so-important",children:"\ud83e\udde0 Why Is Attention So Important?"}),"\n",(0,s.jsx)(n.p,{children:"Attention solves a key problem:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["\ud83e\uddf1 ",(0,s.jsx)(n.strong,{children:"Not all words in a sentence are equally important."})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Traditional models (like LSTMs) squish everything into one final vector, which loses detail. Attention lets the model ",(0,s.jsx)(n.strong,{children:"focus on different parts of the input"})," when generating each output token."]}),"\n",(0,s.jsx)(n.h3,{id:"example-2",children:"Example:"}),"\n",(0,s.jsx)(n.p,{children:"To translate \u201cThe cat sat on the mat,\u201d the word \u201csat\u201d in French depends most on \u201ccat\u201d and \u201csat\u201d \u2014 not \u201cthe.\u201d"}),"\n",(0,s.jsxs)(n.p,{children:["Attention lets the decoder ",(0,s.jsx)(n.strong,{children:"dynamically attend"})," to the relevant parts of the input."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-how-attention-works-simplified",children:"\ud83e\uddf2 How Attention Works (Simplified)"}),"\n",(0,s.jsx)(n.p,{children:"Each word in the input gets turned into three vectors:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Query (Q)"})," \u2013 What am I looking for?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Key (K)"})," \u2013 What do I have?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Value (V)"})," \u2013 What can I use if there\u2019s a match?"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The model compares the ",(0,s.jsx)(n.strong,{children:"query"})," to all the ",(0,s.jsx)(n.strong,{children:"keys"})," (via dot products) to get attention ",(0,s.jsx)(n.strong,{children:"scores"}),", then uses those scores to weight the ",(0,s.jsx)(n.strong,{children:"values"}),"."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Output = weighted sum of values (with most attention paid to the relevant inputs)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This is the magic that lets transformers understand complex relationships \u2014 like subject/verb links or nested clauses \u2014 in a single step."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-encoder-decoder-architectures-in-practice",children:"\ud83c\udfd7\ufe0f Encoder-Decoder Architectures in Practice"}),"\n",(0,s.jsx)(n.h3,{id:"-classic-seq2seq-rnn-based",children:"\ud83e\uddfe Classic Seq2Seq (RNN-based)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Encoder = LSTM/GRU processes input"}),"\n",(0,s.jsx)(n.li,{children:"Decoder = LSTM/GRU generates output"}),"\n",(0,s.jsx)(n.li,{children:"Problem: bottleneck in final encoder state"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-transformer-like-t5-bart",children:"\ud83d\udd25 Transformer (like T5, BART)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Encoder: multiple layers of self-attention"}),"\n",(0,s.jsxs)(n.li,{children:["Decoder: layers with self-attention ",(0,s.jsx)(n.strong,{children:"+ encoder-decoder attention"})]}),"\n",(0,s.jsx)(n.li,{children:"More parallelizable, more accurate"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"\ufe0f-gpt-decoder-only-transformer",children:"\ud83e\uddd9\u200d\u2642\ufe0f GPT (Decoder-only Transformer)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No encoder \u2013 only decoder with masked self-attention"}),"\n",(0,s.jsx)(n.li,{children:"Great for generation tasks (chat, stories, code)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-bert-encoder-only-transformer",children:"\ud83e\uddf1 BERT (Encoder-only Transformer)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Only the encoder stack"}),"\n",(0,s.jsx)(n.li,{children:"Great for understanding tasks (classification, QA, embeddings)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-summary",children:"\ud83d\udca1 Summary"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Role"}),(0,s.jsx)(n.th,{children:"Used In"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Encoder"})}),(0,s.jsx)(n.td,{children:"Encodes input into context vectors"}),(0,s.jsx)(n.td,{children:"BERT, T5, translation models"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Decoder"})}),(0,s.jsx)(n.td,{children:"Autoregressively generates output"}),(0,s.jsx)(n.td,{children:"GPT, T5, BART"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Attention"})}),(0,s.jsx)(n.td,{children:"Lets model focus on relevant input parts"}),(0,s.jsx)(n.td,{children:"All modern transformers"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-why-this-matters",children:"\ud83d\ude80 Why This Matters"}),"\n",(0,s.jsx)(n.p,{children:"Understanding encoders, decoders, and attention unlocks your ability to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Read and build transformer architectures"}),"\n",(0,s.jsx)(n.li,{children:"Understand how models like GPT or BERT work under the hood"}),"\n",(0,s.jsx)(n.li,{children:"Design your own LLM workflows (e.g. RAG, fine-tuning, prompt chaining)"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>i});var t=r(6540);const s={},d=t.createContext(s);function o(e){const n=t.useContext(d);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(d.Provider,{value:n},e.children)}}}]);