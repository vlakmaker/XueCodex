"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[4015],{3339:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>c,frontMatter:()=>o,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"topics/ai-fundamentals/tokenization","title":"What is Tokenization?","description":"\ud83d\udd0d What is Tokenization?","source":"@site/docs/topics/ai-fundamentals/tokenization.md","sourceDirName":"topics/ai-fundamentals","slug":"/topics/ai-fundamentals/tokenization","permalink":"/docs/topics/ai-fundamentals/tokenization","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/ai-fundamentals/tokenization.md","tags":[{"inline":true,"label":"ai-fundamentals","permalink":"/docs/tags/ai-fundamentals"},{"inline":true,"label":"definition","permalink":"/docs/tags/definition"},{"inline":true,"label":"tokenization","permalink":"/docs/tags/tokenization"}],"version":"current","frontMatter":{"id":"tokenization","title":"What is Tokenization?","tags":["ai-fundamentals","definition","tokenization"]},"sidebar":"tutorialSidebar","previous":{"title":"What is Superintelligence?","permalink":"/docs/topics/ai-fundamentals/Superintelligence"},"next":{"title":"What is AI?","permalink":"/docs/topics/ai-fundamentals/what-is-ai"}}');var i=s(4848),r=s(8453);const o={id:"tokenization",title:"What is Tokenization?",tags:["ai-fundamentals","definition","tokenization"]},d=void 0,l={},a=[{value:"\ud83d\udd0d <strong>What is Tokenization?</strong>",id:"-what-is-tokenization",level:3},{value:"\ud83d\udee0\ufe0f <strong>Types of Tokenization</strong>",id:"\ufe0f-types-of-tokenization",level:3},{value:"\ud83e\uddea <strong>Subword Tokenization Methods</strong>",id:"-subword-tokenization-methods",level:3},{value:"Tokenization concepts",id:"tokenization-concepts",level:3},{value:"\ud83d\udfe9 <strong>Padding</strong> \u2705",id:"-padding-",level:3},{value:"\ud83d\udd04 <strong>Shuffling</strong>",id:"-shuffling",level:3},{value:"\ud83d\udd01 <strong>Iteration</strong>",id:"-iteration",level:3},{value:"\ud83e\udde9 <strong>Batching</strong>",id:"-batching",level:3},{value:"Data loader creations",id:"data-loader-creations",level:2},{value:"\ud83e\uddf1 1. <strong>Batching</strong>",id:"-1-batching",level:2},{value:"\ud83e\udde9 2. <strong>Padding</strong>",id:"-2-padding",level:2},{value:"\ud83d\udd00 3. <strong>Shuffling</strong>",id:"-3-shuffling",level:2},{value:"\ud83d\udd01 4. <strong>Iteration</strong>",id:"-4-iteration",level:2},{value:"Summary Table",id:"summary-table",level:2},{value:"\u2705 So in summary:",id:"-so-in-summary",level:3},{value:"\ud83e\uddf0 <strong>Tokenization in PyTorch (torchtext)</strong>",id:"-tokenization-in-pytorch-torchtext",level:3},{value:"\ud83c\udfaf <strong>Why It Matters to You</strong>",id:"-why-it-matters-to-you",level:3},{value:"\u2728 Bitty Bonus Recap Spell",id:"-bitty-bonus-recap-spell",level:3}];function h(e){const n={blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.h3,{id:"-what-is-tokenization",children:["\ud83d\udd0d ",(0,i.jsx)(n.strong,{children:"What is Tokenization?"})]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"The process of breaking text into smaller parts (tokens) so a model can understand and process it."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Example:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:'"IBM taught me tokenization"'})," \u2192 ",(0,i.jsx)(n.code,{children:'["IBM", "taught", "me", "tokenization"]'})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"\ufe0f-types-of-tokenization",children:["\ud83d\udee0\ufe0f ",(0,i.jsx)(n.strong,{children:"Types of Tokenization"})]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Description"}),(0,i.jsx)(n.th,{children:"Pros"}),(0,i.jsx)(n.th,{children:"Cons"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Word-based"})}),(0,i.jsx)(n.td,{children:"Splits text into individual words"}),(0,i.jsx)(n.td,{children:"Preserves meaning"}),(0,i.jsx)(n.td,{children:"Big vocabulary \u2192 memory-heavy"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Character-based"})}),(0,i.jsx)(n.td,{children:"Splits into single characters"}),(0,i.jsx)(n.td,{children:"Tiny vocab"}),(0,i.jsx)(n.td,{children:"Context loss, inefficient"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Subword-based"})}),(0,i.jsx)(n.td,{children:"Breaks rare words into chunks, leaves common words whole"}),(0,i.jsx)(n.td,{children:"Balance between size & context"}),(0,i.jsx)(n.td,{children:"More complex implementation"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-subword-tokenization-methods",children:["\ud83e\uddea ",(0,i.jsx)(n.strong,{children:"Subword Tokenization Methods"})]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Method"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"WordPiece"})}),(0,i.jsxs)(n.td,{children:["Merges or splits symbols based on usefulness; used in ",(0,i.jsx)(n.strong,{children:"BERT"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Unigram"})}),(0,i.jsx)(n.td,{children:"Starts with many possible tokens, narrows down based on frequency"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"SentencePiece"})}),(0,i.jsx)(n.td,{children:"Breaks raw text without needing pre-tokenization; assigns unique IDs"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"Examples:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:'"token##ization"'})," \u2192 ",(0,i.jsx)(n.strong,{children:"WordPiece"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:'"_token" "ization"'})," \u2192 ",(0,i.jsx)(n.strong,{children:"Unigram/SentencePiece"})," (Underscore = new word after space)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"tokenization-concepts",children:"Tokenization concepts"}),"\n",(0,i.jsxs)(n.h3,{id:"-padding-",children:["\ud83d\udfe9 ",(0,i.jsx)(n.strong,{children:"Padding"})," \u2705"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsxs)(n.p,{children:["Padding is the process of ",(0,i.jsx)(n.strong,{children:"making all sequences the same length"})," by adding a special token (e.g., ",(0,i.jsx)(n.code,{children:"0"}),") to the end (or beginning) of shorter sequences."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it matters:"})}),"\n",(0,i.jsxs)(n.p,{children:["Neural networks like LSTMs, Transformers, or CNNs expect inputs to be in ",(0,i.jsx)(n.strong,{children:"fixed-sized tensors"}),". Without padding, variable-length inputs will cause shape mismatch errors."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83d\udccc You use padding to fix unequal lengths between sequences in a batch."})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-shuffling",children:["\ud83d\udd04 ",(0,i.jsx)(n.strong,{children:"Shuffling"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsx)(n.p,{children:"Shuffling randomly rearranges the order of data samples in your dataset during training."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it matters:"})}),"\n",(0,i.jsxs)(n.p,{children:["If the data is fed in the same order every epoch (e.g., all positive reviews, then all negative), the model may learn spurious patterns from the ",(0,i.jsx)(n.strong,{children:"order"})," rather than the ",(0,i.jsx)(n.strong,{children:"content"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83d\udccc Use shuffling to improve generalization and avoid learning order-based biases."})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-iteration",children:["\ud83d\udd01 ",(0,i.jsx)(n.strong,{children:"Iteration"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsxs)(n.p,{children:["Iteration refers to the process of ",(0,i.jsx)(n.strong,{children:"looping through your dataset"})," using an iterator (like a Python ",(0,i.jsx)(n.code,{children:"for"})," loop). In PyTorch, ",(0,i.jsx)(n.code,{children:"DataLoader"})," objects are iterators."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it matters:"})}),"\n",(0,i.jsxs)(n.p,{children:["While it's necessary for data loading, ",(0,i.jsx)(n.strong,{children:"iteration itself doesn\u2019t solve any issues"})," like length mismatches or overfitting \u2014 it's just the mechanism for going through the data."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83d\udccc Iteration is how you read data, not how you preprocess or adjust it."})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-batching",children:["\ud83e\udde9 ",(0,i.jsx)(n.strong,{children:"Batching"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsx)(n.p,{children:"Batching groups multiple samples into a single batch before feeding it into the model, instead of one sample at a time."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it matters:"})}),"\n",(0,i.jsxs)(n.p,{children:["Batching speeds up training (parallelism) and helps with convergence, but ",(0,i.jsx)(n.strong,{children:"it doesn\u2019t solve sequence length differences"})," \u2014 it assumes the input sequences are already aligned in shape."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83d\udccc You still need padding before batching works correctly."})}),"\n",(0,i.jsx)(n.h2,{id:"data-loader-creations",children:"Data loader creations"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-1-batching",children:["\ud83e\uddf1 1. ",(0,i.jsx)(n.strong,{children:"Batching"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsxs)(n.p,{children:["Grouping multiple samples into a single batch so they can be processed ",(0,i.jsx)(n.strong,{children:"in parallel"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it's used:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improves training speed by leveraging vectorized operations"}),"\n",(0,i.jsx)(n.li,{children:"Reduces memory usage per update (compared to processing all samples at once)"}),"\n",(0,i.jsx)(n.li,{children:"Stabilizes gradient updates"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example:"}),"\nIf your dataset has 1000 samples and your batch size is 32, the model trains on 32 samples at a time, updating weights after each batch."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-2-padding",children:["\ud83e\udde9 2. ",(0,i.jsx)(n.strong,{children:"Padding"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsxs)(n.p,{children:["Making sequences ",(0,i.jsx)(n.strong,{children:"equal length"})," by adding special padding tokens (e.g., zeros) at the end of shorter sequences."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it's used:"})}),"\n",(0,i.jsxs)(n.p,{children:["Neural networks require fixed-size tensors. If your sentences are different lengths (which they usually are), you need to ",(0,i.jsx)(n.strong,{children:"pad"})," them so they can be batched together."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python\nCopyEdit\n[\n  [1, 2, 3],\n  [4, 5]\n]\n\n"})}),"\n",(0,i.jsx)(n.p,{children:"\u2192 Padded:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python\nCopyEdit\n[\n  [1, 2, 3],\n  [4, 5, 0]\n]\n\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"In PyTorch:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python\nCopyEdit\nfrom torch.nn.utils.rnn import pad_sequence\n\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-3-shuffling",children:["\ud83d\udd00 3. ",(0,i.jsx)(n.strong,{children:"Shuffling"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsx)(n.p,{children:"Randomly changing the order of your dataset for each epoch."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it's used:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Prevents the model from ",(0,i.jsx)(n.strong,{children:"memorizing order-based patterns"})]}),"\n",(0,i.jsx)(n.li,{children:"Improves generalization"}),"\n",(0,i.jsxs)(n.li,{children:["Helps ",(0,i.jsx)(n.strong,{children:"avoid bias"})," introduced by grouped data (e.g., positive reviews followed by negative ones)"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python\nCopyEdit\nDataLoader(dataset, shuffle=True)\n\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-4-iteration",children:["\ud83d\udd01 4. ",(0,i.jsx)(n.strong,{children:"Iteration"})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What it is:"})}),"\n",(0,i.jsxs)(n.p,{children:["Looping through your dataset one sample or batch at a time, typically with a ",(0,i.jsx)(n.code,{children:"for"})," loop or iterator."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why it's important:"})}),"\n",(0,i.jsx)(n.p,{children:"It's how the model consumes the data during training."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"python\nCopyEdit\nfor batch in dataloader:\n    model(batch)\n\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary-table",children:"Summary Table"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Concept"}),(0,i.jsx)(n.th,{children:"Purpose"}),(0,i.jsx)(n.th,{children:"Solves"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Batching"})}),(0,i.jsx)(n.td,{children:"Efficient parallel processing"}),(0,i.jsx)(n.td,{children:"Speed, memory"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Padding"})}),(0,i.jsx)(n.td,{children:"Makes input tensors the same length"}),(0,i.jsx)(n.td,{children:"Varying sequence lengths"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Shuffling"})}),(0,i.jsx)(n.td,{children:"Prevents learning dataset order"}),(0,i.jsx)(n.td,{children:"Bias/generalization"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Iteration"})}),(0,i.jsx)(n.td,{children:"Loads data sample-by-sample or batch-by-batch"}),(0,i.jsx)(n.td,{children:"Accessing data"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"-so-in-summary",children:"\u2705 So in summary:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Concept"}),(0,i.jsx)(n.th,{children:"Purpose"}),(0,i.jsx)(n.th,{children:"Solves Varying Length?"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Padding"})}),(0,i.jsx)(n.td,{children:"Makes sequences equal length with filler tokens"}),(0,i.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Shuffling"})}),(0,i.jsx)(n.td,{children:"Prevents overfitting to input order"}),(0,i.jsx)(n.td,{children:"\u274c No"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Iteration"})}),(0,i.jsx)(n.td,{children:"Loops through dataset"}),(0,i.jsx)(n.td,{children:"\u274c No"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Batching"})}),(0,i.jsx)(n.td,{children:"Feeds multiple samples at once"}),(0,i.jsx)(n.td,{children:"\u274c No (needs padding)"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-tokenization-in-pytorch-torchtext",children:["\ud83e\uddf0 ",(0,i.jsx)(n.strong,{children:"Tokenization in PyTorch (torchtext)"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"get_tokenizer()"})," \u2013 applies tokenizer (e.g., word or subword)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"build_vocab_from_iterator()"})," \u2013 builds vocab and maps tokens to indices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vocab[token]"})," \u2013 returns token\u2019s index"]}),"\n",(0,i.jsxs)(n.li,{children:["Special tokens: ",(0,i.jsx)(n.code,{children:"BOS"}),", ",(0,i.jsx)(n.code,{children:"EOS"}),", ",(0,i.jsx)(n.code,{children:"PAD"}),", ",(0,i.jsx)(n.code,{children:"UNK"})," \u2192 added for sentence marking and padding"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-why-it-matters-to-you",children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Why It Matters to You"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["If you\u2019re using ",(0,i.jsx)(n.strong,{children:"Hugging Face"}),", these methods are what sit underneath your tokenizer magic"]}),"\n",(0,i.jsxs)(n.li,{children:["In ",(0,i.jsx)(n.strong,{children:"MythosQuest"})," or ",(0,i.jsx)(n.strong,{children:"Spellweaver"}),", if you do custom embeddings or want to fine-tune, understanding tokenization helps you prepare your data properly"]}),"\n",(0,i.jsx)(n.li,{children:"Especially important when designing prompt structure or pre/post-processing layers"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"-bitty-bonus-recap-spell",children:"\u2728 Bitty Bonus Recap Spell"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"\u201cTokenization is the spell that breaks language into its component runes. Choose your rune-style wisely: words for clarity, characters for precision, subwords for balance.\u201d"}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>d});var t=s(6540);const i={},r=t.createContext(i);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);