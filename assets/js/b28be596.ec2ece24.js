"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[6366],{5466:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"topics/deep-learning/nlp/self-attention","title":"What Is Self-Attention?","description":"Self-attention allows a model to look at all the other words in a sentence (or a document, or code...) and decide how important each of them is for understanding a particular word.","source":"@site/docs/topics/deep-learning/nlp/self-attention.md","sourceDirName":"topics/deep-learning/nlp","slug":"/topics/deep-learning/nlp/self-attention","permalink":"/docs/topics/deep-learning/nlp/self-attention","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/nlp/self-attention.md","tags":[{"inline":true,"label":"transformers","permalink":"/docs/tags/transformers"},{"inline":true,"label":"nlp","permalink":"/docs/tags/nlp"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"self-attention","title":"What Is Self-Attention?","tags":["transformers","nlp","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"rag-introduction","permalink":"/docs/topics/deep-learning/nlp/rag-introduction"},"next":{"title":"Transformer Architecture ","permalink":"/docs/topics/deep-learning/nlp/transformer-architecture"}}');var r=t(4848),i=t(8453);const o={id:"self-attention",title:"What Is Self-Attention?",tags:["transformers","nlp","neural networks"]},l="\ud83d\udd0d What Is Self-Attention?",a={},d=[{value:"\ud83e\udde0 How It Works: Step-by-Step (with Slides)",id:"-how-it-works-step-by-step-with-slides",level:3},{value:"1. <strong>Projection into Q, K, V (Queries, Keys, Values)</strong>",id:"1-projection-into-q-k-v-queries-keys-values",level:3},{value:"2. <strong>Computing Attention Scores (aka Relevance)</strong>",id:"2-computing-attention-scores-aka-relevance",level:3},{value:"3. <strong>Weighted Sum \u2192 Output</strong>",id:"3-weighted-sum--output",level:3},{value:"\ud83d\udd01 Multi-Head Attention",id:"-multi-head-attention",level:3},{value:"\ud83c\udf00 Efficiency: Sparse &amp; Grouped Attention",id:"-efficiency-sparse--grouped-attention",level:3},{value:"\ud83e\udde0 Grouped Attention",id:"-grouped-attention",level:3},{value:"\ud83c\udf0c Sparse Attention",id:"-sparse-attention",level:3},{value:"\ud83e\udd16 Summary: Why Self-Attention Is Powerful",id:"-summary-why-self-attention-is-powerful",level:3}];function c(e){const n={blockquote:"blockquote",em:"em",h1:"h1",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"-what-is-self-attention",children:"\ud83d\udd0d What Is Self-Attention?"})}),"\n",(0,r.jsxs)(n.p,{children:["Self-attention allows a model to look at ",(0,r.jsx)(n.strong,{children:"all the other words in a sentence"})," (or a document, or code...) and ",(0,r.jsx)(n.strong,{children:"decide how important each of them is"})," for understanding a particular word."]}),"\n",(0,r.jsx)(n.p,{children:"Imagine reading:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:'"The dog chased the llama because it was fast."'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You need to know:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Does \u201cit\u201d refer to the dog or the llama?"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["That\u2019s where ",(0,r.jsx)(n.strong,{children:"self-attention"})," steps in. It weighs ",(0,r.jsx)(n.strong,{children:"\u201cit\u201d\u2019s"})," relation to ",(0,r.jsx)(n.strong,{children:"dog"})," and ",(0,r.jsx)(n.strong,{children:"llama"})," and assigns a ",(0,r.jsx)(n.em,{children:"relevance score"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"-how-it-works-step-by-step-with-slides",children:"\ud83e\udde0 How It Works: Step-by-Step (with Slides)"}),"\n",(0,r.jsxs)(n.h3,{id:"1-projection-into-q-k-v-queries-keys-values",children:["1. ",(0,r.jsx)(n.strong,{children:"Projection into Q, K, V (Queries, Keys, Values)"})]}),"\n",(0,r.jsxs)(n.p,{children:["Every input token is turned into ",(0,r.jsx)(n.strong,{children:"three vectors"})," using learned matrices:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Query (Q)"}),": What are we looking for?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key (K)"}),": What do we have to offer?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value (V)"}),": What is the actual content we want to share?"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"All of this happens in parallel for every word in a sentence."}),"\n",(0,r.jsxs)(n.h3,{id:"2-computing-attention-scores-aka-relevance",children:["2. ",(0,r.jsx)(n.strong,{children:"Computing Attention Scores (aka Relevance)"})]}),"\n",(0,r.jsxs)(n.p,{children:["Let\u2019s say the model is processing the word ",(0,r.jsx)(n.strong,{children:"\u201cit\u201d"}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u201cIt\u201d becomes a ",(0,r.jsx)(n.strong,{children:"Query"})," vector."]}),"\n",(0,r.jsxs)(n.li,{children:["Every other word becomes a ",(0,r.jsx)(n.strong,{children:"Key"})," vector."]}),"\n",(0,r.jsxs)(n.li,{children:["The dot product of Query \xd7 Key gives a ",(0,r.jsx)(n.strong,{children:"score"}),": How relevant is this word to \u201cit\u201d?"]}),"\n",(0,r.jsx)(n.li,{children:"Apply softmax = normalized relevance weights."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["These scores are then used to ",(0,r.jsx)(n.strong,{children:"weight the Value vectors"})," of each word. Words that are more relevant contribute more to the final output vector."]}),"\n",(0,r.jsxs)(n.h3,{id:"3-weighted-sum--output",children:["3. ",(0,r.jsx)(n.strong,{children:"Weighted Sum \u2192 Output"})]}),"\n",(0,r.jsxs)(n.p,{children:["The model multiplies each Value by its attention weight and ",(0,r.jsx)(n.strong,{children:"adds them all together"})," to form an output vector that is ",(0,r.jsx)(n.strong,{children:"context-aware"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["So the vector for \u201cit\u201d now ",(0,r.jsx)(n.strong,{children:"knows what \u201cit\u201d is referring to."})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"-multi-head-attention",children:"\ud83d\udd01 Multi-Head Attention"}),"\n",(0,r.jsxs)(n.p,{children:["One attention head may capture ",(0,r.jsx)(n.strong,{children:"subject-verb"})," relationships. Another may look at ",(0,r.jsx)(n.strong,{children:"noun-adjective"})," dependencies. To cover multiple types of patterns, we split attention into ",(0,r.jsx)(n.strong,{children:"multiple heads"}),"\u2014each learns a different type of relationship."]}),"\n",(0,r.jsxs)(n.p,{children:["Each head does its own QKV magic, and then their outputs are ",(0,r.jsx)(n.strong,{children:"concatenated and linearly transformed"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"-efficiency-sparse--grouped-attention",children:"\ud83c\udf00 Efficiency: Sparse & Grouped Attention"}),"\n",(0,r.jsx)(n.p,{children:"As sequences get longer, computing attention becomes expensive."}),"\n",(0,r.jsx)(n.h3,{id:"-grouped-attention",children:"\ud83e\udde0 Grouped Attention"}),"\n",(0,r.jsxs)(n.p,{children:["You can ",(0,r.jsx)(n.strong,{children:"group"})," attention heads to reuse the same Keys/Values and save computation. Each group has its own Queries but shares context. More efficient, almost no loss in performance."]}),"\n",(0,r.jsx)(n.h3,{id:"-sparse-attention",children:"\ud83c\udf0c Sparse Attention"}),"\n",(0,r.jsx)(n.p,{children:"Full attention = quadratic cost (every token attends to every other). Sparse attention = look only at nearby or important tokens."}),"\n",(0,r.jsx)(n.p,{children:"Example:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Global"}),": Attend to all previous tokens (slow)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Local"}),": Only attend to nearby tokens (like a sliding window)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Strided/Fix"}),": Attend in patterns (like every 3rd token or critical anchors)."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These optimizations help scale to long documents (e.g., 100K tokens) while keeping costs manageable."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"-summary-why-self-attention-is-powerful",children:"\ud83e\udd16 Summary: Why Self-Attention Is Powerful"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understands context across a whole sequence (not just neighbors like RNNs)."}),"\n",(0,r.jsx)(n.li,{children:"Parallelizable, unlike sequential RNNs."}),"\n",(0,r.jsx)(n.li,{children:"Can be made efficient with grouped or sparse attention."}),"\n",(0,r.jsx)(n.li,{children:"Each head learns different linguistic or structural patterns."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);