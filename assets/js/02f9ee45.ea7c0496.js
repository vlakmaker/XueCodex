"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[3733],{7408:e=>{e.exports=JSON.parse('{"tag":{"label":"relu","permalink":"/docs/tags/relu","allTagsPath":"/docs/tags","count":2,"items":[{"id":"topics/deep-learning/activation-functions-derivatives","title":"Activation Functions & Derivatives","description":"Activation functions are what make neural networks more than just glorified linear regressions. They add non-linearity \u2014 which gives neural networks their superpower: the ability to learn complex patterns and behaviors. Their derivatives are critical for training the network effectively through backpropagation.","permalink":"/docs/topics/deep-learning/activation-functions-derivatives"},{"id":"topics/deep-learning/activation-functions","title":"Activation Functions for Humans: Sigmoid, Tanh, and ReLU (Deep Dive)","description":"\ud83e\udd16 What Are Activation Functions?","permalink":"/docs/topics/deep-learning/activation-functions"}],"unlisted":false}}')}}]);