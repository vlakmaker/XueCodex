"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[4247],{1258:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"topics/deep-learning/activation-functions-derivatives","title":"Activation Functions & Derivatives","description":"Activation functions are what make neural networks more than just glorified linear regressions. They add non-linearity \u2014 which gives neural networks their superpower: the ability to learn complex patterns and behaviors. Their derivatives are critical for training the network effectively through backpropagation.","source":"@site/docs/topics/deep-learning/activation-functions-derivatives.md","sourceDirName":"topics/deep-learning","slug":"/topics/deep-learning/activation-functions-derivatives","permalink":"/docs/topics/deep-learning/activation-functions-derivatives","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/activation-functions-derivatives.md","tags":[{"inline":true,"label":"deep learning","permalink":"/docs/tags/deep-learning"},{"inline":true,"label":"activation sfunctions","permalink":"/docs/tags/activation-sfunctions"},{"inline":true,"label":"sigmoid","permalink":"/docs/tags/sigmoid"},{"inline":true,"label":"tahn","permalink":"/docs/tags/tahn"},{"inline":true,"label":"relu","permalink":"/docs/tags/relu"}],"version":"current","frontMatter":{"id":"activation-functions-derivatives","title":"Activation Functions & Derivatives","tags":["deep learning","activation sfunctions","sigmoid","tahn","relu"]},"sidebar":"tutorialSidebar","previous":{"title":"What is Machine Learning","permalink":"/docs/topics/machine-learning/what-is-machine-learning"},"next":{"title":"Activation Functions for Humans: Sigmoid, Tanh, and ReLU (Deep Dive)","permalink":"/docs/topics/deep-learning/activation-functions"}}');var s=n(4848),r=n(8453);const a={id:"activation-functions-derivatives",title:"Activation Functions & Derivatives",tags:["deep learning","activation sfunctions","sigmoid","tahn","relu"]},l="\ud83e\udde0 Activation Functions & Derivatives",o={},d=[{value:"\ud83d\udd01 Why Do We Use Activation Functions?",id:"-why-do-we-use-activation-functions",level:2},{value:"\ud83d\udd0d Common Activation Functions",id:"-common-activation-functions",level:2},{value:"1. \ud83c\udf00 Sigmoid Function",id:"1--sigmoid-function",level:3},{value:"\u2705 Use Case:",id:"-use-case",level:3},{value:"\u274c Limitations:",id:"-limitations",level:3},{value:"\ud83e\uddee Derivative:",id:"-derivative",level:3},{value:"2. \ud83e\uddff Tanh Function",id:"2--tanh-function",level:3},{value:"\u2705 Use Case:",id:"-use-case-1",level:3},{value:"\u274c Limitations:",id:"-limitations-1",level:3},{value:"\ud83e\uddee Derivative:",id:"-derivative-1",level:3},{value:"3. \u26a1 ReLU (Rectified Linear Unit)",id:"3--relu-rectified-linear-unit",level:3},{value:"\u2705 Use Case:",id:"-use-case-2",level:3},{value:"\u274c Limitations:",id:"-limitations-2",level:3},{value:"\ud83e\uddee Derivative:",id:"-derivative-2",level:3},{value:"4. \ud83d\udca7 Leaky ReLU",id:"4--leaky-relu",level:3},{value:"\u2705 Use Case:",id:"-use-case-3",level:3},{value:"\ud83e\uddee Derivative:",id:"-derivative-3",level:3},{value:"5. \ud83d\udd01 Linear Activation",id:"5--linear-activation",level:3},{value:"\u2705 Use Case:",id:"-use-case-4",level:3},{value:"\u274c Don\u2019t Use In Hidden Layers:",id:"-dont-use-in-hidden-layers",level:3},{value:"\ud83d\udd01 Why Derivatives Matter (Backpropagation)",id:"-why-derivatives-matter-backpropagation",level:2},{value:"\u2696\ufe0f Summary Table: Activation Functions &amp; Their Derivatives",id:"\ufe0f-summary-table-activation-functions--their-derivatives",level:2},{value:"\ud83e\udde0 TL;DR:",id:"-tldr",level:2}];function c(e){const i={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"-activation-functions--derivatives",children:"\ud83e\udde0 Activation Functions & Derivatives"})}),"\n",(0,s.jsxs)(i.p,{children:["Activation functions are what make neural networks more than just glorified linear regressions. They add non-linearity \u2014 which gives neural networks their superpower: the ability to learn complex patterns and behaviors. Their derivatives are critical for training the network effectively through ",(0,s.jsx)(i.strong,{children:"backpropagation"}),"."]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"-why-do-we-use-activation-functions",children:"\ud83d\udd01 Why Do We Use Activation Functions?"}),"\n",(0,s.jsx)(i.p,{children:"Neurons in a network receive inputs and combine them using weighted sums. But if we just kept doing weighted sums from layer to layer \u2014 the whole network would collapse into one big linear transformation. That\u2019s boring!"}),"\n",(0,s.jsxs)(i.p,{children:["Activation functions inject ",(0,s.jsx)(i.strong,{children:"non-linearity"})," into the mix, letting networks:"]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Learn curves and twists"}),"\n",(0,s.jsx)(i.li,{children:"Make more nuanced decisions"}),"\n",(0,s.jsx)(i.li,{children:"Model reality more effectively"}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"-common-activation-functions",children:"\ud83d\udd0d Common Activation Functions"}),"\n",(0,s.jsx)(i.p,{children:"Let\u2019s break them down with intuitive visuals, math, and practical insights."}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h3,{id:"1--sigmoid-function",children:"1. \ud83c\udf00 Sigmoid Function"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Formula"}),":"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Output range"}),": (0, 1)"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"It looks like an S-curve \u2014 softly squashing large negative or positive values toward 0 or 1."}),"\n",(0,s.jsx)(i.h3,{id:"-use-case",children:"\u2705 Use Case:"}),"\n",(0,s.jsx)(i.p,{children:"Perfect for binary classification \u2014 the output behaves like a probability: \u201chow likely is it a cat?\u201d"}),"\n",(0,s.jsx)(i.h3,{id:"-limitations",children:"\u274c Limitations:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Vanishing gradient problem"}),": at extreme values, the slope becomes nearly 0, so the network stops learning efficiently."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Not zero-centered"}),": everything is always positive, so updates can zigzag inefficiently."]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"-derivative",children:"\ud83e\uddee Derivative:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Max slope is at , where , and ",(0,s.jsx)(i.code,{children:"slope = 0.25"})]}),"\n",(0,s.jsx)(i.li,{children:"At or , slope ~ 0 \u279d slow learning"}),"\n"]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h3,{id:"2--tanh-function",children:"2. \ud83e\uddff Tanh Function"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Formula"}),":"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Output range"}),": (-1, 1)"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Similar shape to sigmoid, but centered around 0, so outputs can be negative too."}),"\n",(0,s.jsx)(i.h3,{id:"-use-case-1",children:"\u2705 Use Case:"}),"\n",(0,s.jsx)(i.p,{children:"Hidden layers in older architectures, especially when zero-centering helps optimization."}),"\n",(0,s.jsx)(i.h3,{id:"-limitations-1",children:"\u274c Limitations:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Still suffers from ",(0,s.jsx)(i.strong,{children:"vanishing gradients"})," at the extremes."]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"-derivative-1",children:"\ud83e\uddee Derivative:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Max ",(0,s.jsx)(i.code,{children:"slope = 1 at"})]}),"\n",(0,s.jsx)(i.li,{children:"At , ,"}),"\n",(0,s.jsx)(i.li,{children:"Still fades at extremes, but performs better than sigmoid."}),"\n"]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h3,{id:"3--relu-rectified-linear-unit",children:"3. \u26a1 ReLU (Rectified Linear Unit)"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Formula"}),":"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Output range"}),": [0, \u221e)"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"ReLU is the tough love coach of activation functions \u2014 if you\u2019re below 0, it gives you nothing. If you\u2019re above 0, it lets you grow."}),"\n",(0,s.jsx)(i.h3,{id:"-use-case-2",children:"\u2705 Use Case:"}),"\n",(0,s.jsx)(i.p,{children:"Most popular choice for hidden layers in modern neural nets (especially deep learning)"}),"\n",(0,s.jsx)(i.h3,{id:"-limitations-2",children:"\u274c Limitations:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Dying ReLU problem"}),": once a neuron gets stuck with negative inputs, it may stop updating."]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"-derivative-2",children:"\ud83e\uddee Derivative:"}),"\n",(0,s.jsx)(i.p,{children:"Fast and simple, but non-zero only when"}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h3,{id:"4--leaky-relu",children:"4. \ud83d\udca7 Leaky ReLU"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Formula"}),":"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"A soft ReLU variant that lets a small gradient flow when"}),"\n",(0,s.jsx)(i.h3,{id:"-use-case-3",children:"\u2705 Use Case:"}),"\n",(0,s.jsx)(i.p,{children:'Prevents "dead neurons" in ReLU networks. Great if you want stability but don\u2019t want to sacrifice performance.'}),"\n",(0,s.jsx)(i.h3,{id:"-derivative-3",children:"\ud83e\uddee Derivative:"}),"\n",(0,s.jsx)(i.p,{children:"Always has a small slope \u2014 it never completely shuts down."}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h3,{id:"5--linear-activation",children:"5. \ud83d\udd01 Linear Activation"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Formula"}),":"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Output range"}),": (\u2212\u221e, \u221e)"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"-use-case-4",children:"\u2705 Use Case:"}),"\n",(0,s.jsxs)(i.p,{children:["Only for output layers in ",(0,s.jsx)(i.strong,{children:"regression"})," problems (e.g., predicting house prices)."]}),"\n",(0,s.jsx)(i.h3,{id:"-dont-use-in-hidden-layers",children:"\u274c Don\u2019t Use In Hidden Layers:"}),"\n",(0,s.jsx)(i.p,{children:"If you stack layers with no activation, it\u2019s just matrix multiplication \u279d collapses into one linear transformation:"}),"\n",(0,s.jsx)(i.p,{children:"So your entire network becomes as dumb as a linear regression. No thanks."}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"-why-derivatives-matter-backpropagation",children:"\ud83d\udd01 Why Derivatives Matter (Backpropagation)"}),"\n",(0,s.jsxs)(i.p,{children:["Backpropagation uses derivatives to ",(0,s.jsx)(i.strong,{children:"figure out how wrong each weight is"}),". That\u2019s how the network learns!"]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["A ",(0,s.jsx)(i.strong,{children:"steep slope"})," \u2192 big correction"]}),"\n",(0,s.jsxs)(i.li,{children:["A ",(0,s.jsx)(i.strong,{children:"flat slope"})," \u2192 tiny or no learning"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"That\u2019s why ReLU and Leaky ReLU are preferred \u2014 they don\u2019t suffer from tiny derivatives."}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"\ufe0f-summary-table-activation-functions--their-derivatives",children:"\u2696\ufe0f Summary Table: Activation Functions & Their Derivatives"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Function"}),(0,s.jsx)(i.th,{children:"Output Range"}),(0,s.jsx)(i.th,{children:"Derivative"}),(0,s.jsx)(i.th,{children:"Zero-Centered"}),(0,s.jsx)(i.th,{children:"Vanishing Gradient"}),(0,s.jsx)(i.th,{children:"Notes"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Sigmoid"}),(0,s.jsx)(i.td,{children:"(0, 1)"}),(0,s.jsx)(i.td,{}),(0,s.jsx)(i.td,{children:"\u274c No"}),(0,s.jsx)(i.td,{children:"\u2705 Yes"}),(0,s.jsx)(i.td,{children:"Good for binary output, slow to train"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Tanh"}),(0,s.jsx)(i.td,{children:"(-1, 1)"}),(0,s.jsx)(i.td,{}),(0,s.jsx)(i.td,{children:"\u2705 Yes"}),(0,s.jsx)(i.td,{children:"\u2705 Yes (less)"}),(0,s.jsx)(i.td,{children:"Better gradient flow than sigmoid"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"ReLU"}),(0,s.jsx)(i.td,{children:"[0, \u221e)"}),(0,s.jsx)(i.td,{children:"0 if , 1 if"}),(0,s.jsx)(i.td,{children:"\u274c No"}),(0,s.jsx)(i.td,{children:"\u274c No (but can die)"}),(0,s.jsx)(i.td,{children:"Fast, most commonly used"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Leaky ReLU"}),(0,s.jsx)(i.td,{children:"[~0, \u221e)"}),(0,s.jsx)(i.td,{children:"0.01 or 1"}),(0,s.jsx)(i.td,{children:"\u274c No"}),(0,s.jsx)(i.td,{children:"\u274c No"}),(0,s.jsx)(i.td,{children:"Solves ReLU dying problem"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"Linear (ID)"}),(0,s.jsx)(i.td,{children:"(\u2212\u221e, \u221e)"}),(0,s.jsx)(i.td,{children:"1"}),(0,s.jsx)(i.td,{children:"\u2705 Yes"}),(0,s.jsx)(i.td,{children:"\u274c No"}),(0,s.jsx)(i.td,{children:"Only use for regression output layer"})]})]})]}),"\n",(0,s.jsx)(i.hr,{}),"\n",(0,s.jsx)(i.h2,{id:"-tldr",children:"\ud83e\udde0 TL;DR:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["Activation functions add ",(0,s.jsx)(i.strong,{children:"non-linearity"})," \u2014 without them, you can\u2019t learn complex patterns."]}),"\n",(0,s.jsxs)(i.li,{children:["Their ",(0,s.jsx)(i.strong,{children:"derivatives"})," are what fuel learning during backpropagation."]}),"\n",(0,s.jsxs)(i.li,{children:["Each function behaves differently:","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Sigmoid"}),": slow, but probabilistic"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Tanh"}),": better gradient flow"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"ReLU"}),": fast, sparse, simple"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Leaky ReLU"}),": robust against dead neurons"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Linear"}),": useful in output layer for continuous prediction"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"\ud83d\udc49 Think of it like this:"}),"\n",(0,s.jsx)(i.p,{children:'> "Use ReLU to think, use linear to speak." \ud83e\udde0\ud83d\udde3\ufe0f\n>'})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>l});var t=n(6540);const s={},r=t.createContext(s);function a(e){const i=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:i},e.children)}}}]);