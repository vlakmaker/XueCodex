"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[2946],{2876:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"topics/deep-learning/training-document-classifier","title":"Training a Document Classifier","description":"Training a Document Classifier","source":"@site/docs/topics/deep-learning/training-document-classifier.md","sourceDirName":"topics/deep-learning","slug":"/topics/deep-learning/training-document-classifier","permalink":"/docs/topics/deep-learning/training-document-classifier","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/training-document-classifier.md","tags":[{"inline":true,"label":"deep learning","permalink":"/docs/tags/deep-learning"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"},{"inline":true,"label":"document classifier","permalink":"/docs/tags/document-classifier"}],"version":"current","frontMatter":{"id":"training-document-classifier","title":"Training a Document Classifier","tags":["deep learning","neural networks","document classifier"]},"sidebar":"tutorialSidebar","previous":{"title":"rag-introduction","permalink":"/docs/topics/deep-learning/rag-introduction"},"next":{"title":"Weight Initialization in Neural Networks","permalink":"/docs/topics/deep-learning/weight-initialization"}}');var r=n(4848),t=n(8453);const l={id:"training-document-classifier",title:"Training a Document Classifier",tags:["deep learning","neural networks","document classifier"]},o=void 0,a={},d=[{value:"<strong>Training a Document Classifier</strong>",id:"training-a-document-classifier",level:2},{value:"\ud83e\udde0 <strong>1. Neural Networks Learn Through Parameters (\u03b8)</strong>",id:"-1-neural-networks-learn-through-parameters-\u03b8",level:3},{value:"\ud83d\udcc9 <strong>2. What Is a Loss Function? (Hint: It Measures Mistakes)</strong>",id:"-2-what-is-a-loss-function-hint-it-measures-mistakes",level:3},{value:"\ud83c\udfaf <strong>3. Enter Cross-Entropy Loss</strong>",id:"-3-enter-cross-entropy-loss",level:3},{value:"\ud83d\udd38 How it works:",id:"-how-it-works",level:3},{value:"\ud83d\udccc Formula-wise (simplified):",id:"-formula-wise-simplified",level:3},{value:"\ud83d\udcda <strong>4. Monte Carlo Sampling</strong>",id:"-4-monte-carlo-sampling",level:3},{value:"\ud83d\udee0\ufe0f <strong>5. Optimization: How the Model Learns</strong>",id:"\ufe0f-5-optimization-how-the-model-learns",level:3},{value:"\ud83d\udd01 <strong>Gradient Descent</strong>",id:"-gradient-descent",level:3},{value:"\u2705 Steps in Practice:",id:"-steps-in-practice",level:3},{value:"\ud83e\uddee <strong>6. Logits \u2192 Softmax \u2192 Argmax</strong>",id:"-6-logits--softmax--argmax",level:3},{value:"\ud83d\udd04 <strong>7. Learning Rate Schedulers &amp; Gradient Clipping</strong>",id:"-7-learning-rate-schedulers--gradient-clipping",level:3},{value:"\ud83e\uddea <strong>8. Train / Validation / Test Sets</strong>",id:"-8-train--validation--test-sets",level:3},{value:"\u2705 <strong>Recap \u2013 Bitty Style</strong>",id:"-recap--bitty-style",level:2}];function c(e){const s={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.h2,{id:"training-a-document-classifier",children:(0,r.jsx)(s.strong,{children:"Training a Document Classifier"})}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"-1-neural-networks-learn-through-parameters-\u03b8",children:["\ud83e\udde0 ",(0,r.jsx)(s.strong,{children:"1. Neural Networks Learn Through Parameters (\u03b8)"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["A ",(0,r.jsx)(s.strong,{children:"neural network"})," is just a stack of mathematical operations using ",(0,r.jsx)(s.em,{children:"parameters"})," (called ",(0,r.jsx)(s.strong,{children:"\u03b8"}),", theta)."]}),"\n",(0,r.jsx)(s.li,{children:"These parameters = weights that are learned and adjusted during training."}),"\n",(0,r.jsx)(s.li,{children:"The goal is to tweak \u03b8 so your predictions (\u0177) get closer to the actual labels (y)."}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"-2-what-is-a-loss-function-hint-it-measures-mistakes",children:["\ud83d\udcc9 ",(0,r.jsx)(s.strong,{children:"2. What Is a Loss Function? (Hint: It Measures Mistakes)"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["A ",(0,r.jsx)(s.strong,{children:"loss function"})," measures how far off the model is from the correct answer."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"Think of it like this:"}),"\n",(0,r.jsxs)(s.p,{children:["\u2192 ",(0,r.jsx)(s.strong,{children:"High loss"})," = bad predictions \ud83d\ude16"]}),"\n",(0,r.jsxs)(s.p,{children:["\u2192 ",(0,r.jsx)(s.strong,{children:"Low loss"})," = model doing well \ud83d\ude0e"]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["We don\u2019t manually teach the model what's wrong \u2014 the ",(0,r.jsx)(s.strong,{children:"loss function"})," tells it where it messed up."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"-3-enter-cross-entropy-loss",children:["\ud83c\udfaf ",(0,r.jsx)(s.strong,{children:"3. Enter Cross-Entropy Loss"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["Used for ",(0,r.jsx)(s.strong,{children:"classification tasks"}),", especially when you want the model to pick between multiple categories."]}),"\n",(0,r.jsxs)(s.li,{children:["Based on comparing:","\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"True distribution (y):"})," The correct class (e.g., \u201csports\u201d)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Predicted distribution (\u0177):"})," The probabilities the model assigns to each class after ",(0,r.jsx)(s.strong,{children:"softmax"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"-how-it-works",children:"\ud83d\udd38 How it works:"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:["Your model spits out ",(0,r.jsx)(s.strong,{children:"logits"})," (raw scores for each class)."]}),"\n",(0,r.jsxs)(s.li,{children:["Apply ",(0,r.jsx)(s.strong,{children:"softmax"}),": turns logits into a probability distribution (all values between 0\u20131 and sum to 1)."]}),"\n",(0,r.jsx)(s.li,{children:"Cross-entropy loss measures how well your predicted distribution matches the correct class."}),"\n",(0,r.jsx)(s.li,{children:"It punishes confident wrong answers more than unsure ones."}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"-formula-wise-simplified",children:"\ud83d\udccc Formula-wise (simplified):"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{children:"pgsql\nCopyEdit\nCross-Entropy = -log(P(correct class))\n\n"})}),"\n",(0,r.jsx)(s.p,{children:"If the model is 90% sure the answer is correct: low loss."}),"\n",(0,r.jsx)(s.p,{children:"If it\u2019s 10% sure or confident in the wrong class: high loss."}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"-4-monte-carlo-sampling",children:["\ud83d\udcda ",(0,r.jsx)(s.strong,{children:"4. Monte Carlo Sampling"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["Fancy phrase for: ",(0,r.jsx)(s.strong,{children:'"When we don\u2019t know the full distribution, just average over examples."'})]}),"\n",(0,r.jsx)(s.li,{children:"It\u2019s how we approximate the \u201ctrue\u201d loss across a batch of training samples."}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"\ufe0f-5-optimization-how-the-model-learns",children:["\ud83d\udee0\ufe0f ",(0,r.jsx)(s.strong,{children:"5. Optimization: How the Model Learns"})]}),"\n",(0,r.jsxs)(s.p,{children:["The way we ",(0,r.jsx)(s.strong,{children:"minimize the loss"})," is through:"]}),"\n",(0,r.jsxs)(s.h3,{id:"-gradient-descent",children:["\ud83d\udd01 ",(0,r.jsx)(s.strong,{children:"Gradient Descent"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Iteratively update parameters to reduce loss:"}),"\n"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:"python\nCopyEdit\n\u03b8 \u2190 \u03b8 - \u03b7 * \u2207Loss\n\n"})}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"\u03b8 = current weights"}),"\n",(0,r.jsx)(s.li,{children:"\u03b7 = learning rate (how big a step to take)"}),"\n",(0,r.jsx)(s.li,{children:"\u2207Loss = gradient (slope of the loss function)"}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"-steps-in-practice",children:"\u2705 Steps in Practice:"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Forward pass:"})," Run inputs through the model \u2192 get predictions \u2192 compute loss."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Backward pass:"})," Calculate gradients using ",(0,r.jsx)(s.code,{children:".backward()"}),"."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Update parameters:"})," Use an ",(0,r.jsx)(s.strong,{children:"optimizer"})," like ",(0,r.jsx)(s.code,{children:"SGD"})," to move \u03b8 in the right direction."]}),"\n",(0,r.jsx)(s.li,{children:(0,r.jsx)(s.strong,{children:"Repeat."})}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"-6-logits--softmax--argmax",children:["\ud83e\uddee ",(0,r.jsx)(s.strong,{children:"6. Logits \u2192 Softmax \u2192 Argmax"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Logits = raw model output per class"}),"\n",(0,r.jsx)(s.li,{children:"Softmax = converts logits to probabilities"}),"\n",(0,r.jsx)(s.li,{children:"Argmax = picks the class with the highest probability"}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"-7-learning-rate-schedulers--gradient-clipping",children:["\ud83d\udd04 ",(0,r.jsx)(s.strong,{children:"7. Learning Rate Schedulers & Gradient Clipping"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scheduler:"})," Reduces the learning rate after each epoch (to fine-tune learning)."]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Gradient clipping:"})," Prevents gradients from exploding (literally very large values that destabilize learning)."]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h3,{id:"-8-train--validation--test-sets",children:["\ud83e\uddea ",(0,r.jsx)(s.strong,{children:"8. Train / Validation / Test Sets"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Train:"})," Used to ",(0,r.jsx)(s.em,{children:"learn"})," parameters"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Validation:"})," Used to ",(0,r.jsx)(s.em,{children:"tune"})," hyperparameters (like learning rate, # of neurons, etc.)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Test:"})," Final evaluation to check real-world performance"]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsxs)(s.h2,{id:"-recap--bitty-style",children:["\u2705 ",(0,r.jsx)(s.strong,{children:"Recap \u2013 Bitty Style"})]}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:"Concept"}),(0,r.jsx)(s.th,{children:"What It Means in Simple Terms"})]})}),(0,r.jsxs)(s.tbody,{children:[(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"\u03b8 (parameters)"})}),(0,r.jsx)(s.td,{children:"The knobs the model turns to improve itself"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Loss function"})}),(0,r.jsx)(s.td,{children:"Tells the model how bad it did"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Cross-entropy"})}),(0,r.jsx)(s.td,{children:"Measures how far off the prediction is from the truth"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Softmax"})}),(0,r.jsx)(s.td,{children:"Turns scores into probabilities"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Argmax"})}),(0,r.jsx)(s.td,{children:"Picks the class with the highest probability"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Gradient descent"})}),(0,r.jsx)(s.td,{children:"The step-by-step update method to reduce errors"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Optimizer (SGD)"})}),(0,r.jsx)(s.td,{children:"Applies those updates in practice"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Learning rate"})}),(0,r.jsx)(s.td,{children:"Controls how fast you update the weights"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Gradient clipping"})}),(0,r.jsx)(s.td,{children:"Keeps things stable when learning gets wild"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Train/Val/Test"})}),(0,r.jsx)(s.td,{children:"Each has its job in helping the model grow & generalize"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Monte Carlo sampling"})}),(0,r.jsx)(s.td,{children:"Averaging over samples to estimate things we can\u2019t measure exactly"})]})]})]})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>l,x:()=>o});var i=n(6540);const r={},t=i.createContext(r);function l(e){const s=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);