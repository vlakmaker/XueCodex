"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[184],{5730:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"topics/deep-learning/llm-ngam-nn","title":"Language Modeling with N-Grams and Neural Networks","description":"\ud83e\udde0 Overview","source":"@site/docs/topics/deep-learning/llm-ngram-nn.md","sourceDirName":"topics/deep-learning","slug":"/topics/deep-learning/llm-ngam-nn","permalink":"/docs/topics/deep-learning/llm-ngam-nn","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/llm-ngram-nn.md","tags":[{"inline":true,"label":"deep learning","permalink":"/docs/tags/deep-learning"},{"inline":true,"label":"n-gram","permalink":"/docs/tags/n-gram"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"llm-ngam-nn","title":"Language Modeling with N-Grams and Neural Networks","tags":["deep learning","n-gram","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"Encoder-Decoder RNN Models for Translation","permalink":"/docs/topics/deep-learning/encoder-decoder-translation"},"next":{"title":"Neural Network Forward Pass","permalink":"/docs/topics/deep-learning/neural-network-forward-pass"}}');var s=i(4848),d=i(8453);const l={id:"llm-ngam-nn",title:"Language Modeling with N-Grams and Neural Networks",tags:["deep learning","n-gram","neural networks"]},t="Language Modeling with N-Grams and Neural Networks",o={},a=[{value:"\ud83e\udde0 Overview",id:"-overview",level:2},{value:"\ud83d\udcda What Is Language Modeling?",id:"-what-is-language-modeling",level:2},{value:"\ud83d\udd22 N-Grams: The Basics",id:"-n-grams-the-basics",level:2},{value:"\u27a4 Definition:",id:"-definition",level:3},{value:"\u27a4 Bi-Gram Example:",id:"-bi-gram-example",level:3},{value:"\ud83d\udd02 Tri-Gram Model",id:"-tri-gram-model",level:2},{value:"\u2699\ufe0f General N-Gram Model",id:"\ufe0f-general-n-gram-model",level:2},{value:"\ud83e\uddf1 Transition to Neural Language Models",id:"-transition-to-neural-language-models",level:2},{value:"\ud83d\udd38 One-Hot to Embedding",id:"-one-hot-to-embedding",level:3},{value:"\ud83d\udd38 Input Vector",id:"-input-vector",level:3},{value:"\ud83d\udd38 Architecture",id:"-architecture",level:3},{value:"\ud83d\udd38 Prediction",id:"-prediction",level:3},{value:"\ud83d\udeab Limitations of Basic Neural N-Gram Models",id:"-limitations-of-basic-neural-n-gram-models",level:2},{value:"\u2705 Recap",id:"-recap",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"language-modeling-with-n-grams-and-neural-networks",children:"Language Modeling with N-Grams and Neural Networks"})}),"\n",(0,s.jsx)(n.h2,{id:"-overview",children:"\ud83e\udde0 Overview"}),"\n",(0,s.jsxs)(n.p,{children:["This KI explains the core concepts of language modeling using ",(0,s.jsx)(n.strong,{children:"n-grams"}),", including ",(0,s.jsx)(n.strong,{children:"bi-gram"})," and ",(0,s.jsx)(n.strong,{children:"tri-gram"})," models, and their evolution into ",(0,s.jsx)(n.strong,{children:"neural network-based language models"}),". It also covers limitations of traditional n-grams and how neural models using ",(0,s.jsx)(n.strong,{children:"embeddings"})," and ",(0,s.jsx)(n.strong,{children:"softmax"})," improve predictive performance."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-what-is-language-modeling",children:"\ud83d\udcda What Is Language Modeling?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Language modeling"})," is the task of predicting the next word in a sequence, given the previous word(s). It is foundational in many NLP tasks, including:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Text generation"}),"\n",(0,s.jsx)(n.li,{children:"Speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Machine translation"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-n-grams-the-basics",children:"\ud83d\udd22 N-Grams: The Basics"}),"\n",(0,s.jsx)(n.h3,{id:"-definition",children:"\u27a4 Definition:"}),"\n",(0,s.jsxs)(n.p,{children:["An ",(0,s.jsx)(n.strong,{children:"n-gram"})," is a contiguous sequence of ",(0,s.jsx)(n.code,{children:"n"})," words from a given sample of text."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unigram (n=1)"}),": Uses no context."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bigram (n=2)"}),": Predicts based on the previous one word."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Trigram (n=3)"}),": Predicts based on the previous two words."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-bi-gram-example",children:"\u27a4 Bi-Gram Example:"}),"\n",(0,s.jsx)(n.p,{children:'"I like vacations" and "I hate surgery"'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Context: ",(0,s.jsx)(n.code,{children:"I like"})," \u2192 Predict ",(0,s.jsx)(n.code,{children:"vacations"})]}),"\n",(0,s.jsxs)(n.li,{children:["Context: ",(0,s.jsx)(n.code,{children:"I hate"})," \u2192 Predict ",(0,s.jsx)(n.code,{children:"surgery"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Bi-gram model estimates:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"P(vacations | like) = 1\nP(surgery | like) = 0\n"})}),"\n",(0,s.jsx)(n.p,{children:"Context window size = 1"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-tri-gram-model",children:"\ud83d\udd02 Tri-Gram Model"}),"\n",(0,s.jsx)(n.p,{children:"Tri-gram considers two previous words:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Context: ",(0,s.jsx)(n.code,{children:"surgeons like"})," \u2192 Predict ",(0,s.jsx)(n.code,{children:"surgery"})]}),"\n",(0,s.jsxs)(n.li,{children:["Context: ",(0,s.jsx)(n.code,{children:"I like"})," \u2192 Predict ",(0,s.jsx)(n.code,{children:"vacations"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Now the probability depends on ",(0,s.jsx)(n.strong,{children:"two words"}),", increasing specificity:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"P(surgery | surgeons, like) = 1\nP(surgery | I, like) = 0\n"})}),"\n",(0,s.jsx)(n.p,{children:"Context window size = 2"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-general-n-gram-model",children:"\u2699\ufe0f General N-Gram Model"}),"\n",(0,s.jsx)(n.p,{children:"N-gram models generalize this further:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Context window = ",(0,s.jsx)(n.code,{children:"n - 1"})]}),"\n",(0,s.jsxs)(n.li,{children:["Predict next word ",(0,s.jsx)(n.code,{children:"w_t"})," given: ",(0,s.jsx)(n.code,{children:"w_{t-n+1}, ..., w_{t-1}"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Limitation"}),": As ",(0,s.jsx)(n.code,{children:"n"})," increases:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Memory and computation cost grow"}),"\n",(0,s.jsx)(n.li,{children:"Training data becomes sparse"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-transition-to-neural-language-models",children:"\ud83e\uddf1 Transition to Neural Language Models"}),"\n",(0,s.jsxs)(n.p,{children:["To overcome sparsity and context limitations, we use ",(0,s.jsx)(n.strong,{children:"neural networks"}),":"]}),"\n",(0,s.jsx)(n.h3,{id:"-one-hot-to-embedding",children:"\ud83d\udd38 One-Hot to Embedding"}),"\n",(0,s.jsx)(n.p,{children:"Words are no longer one-hot encoded. Instead:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Each word is mapped to a dense vector (embedding)"}),"\n",(0,s.jsx)(n.li,{children:"These embeddings capture semantic similarity"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-input-vector",children:"\ud83d\udd38 Input Vector"}),"\n",(0,s.jsx)(n.p,{children:"If context size = 2 and vocabulary size = 6:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["You concatenate embeddings: ",(0,s.jsx)(n.code,{children:"embedding(w_{t-2}) + embedding(w_{t-1})"})]}),"\n",(0,s.jsxs)(n.li,{children:["Resulting input size = ",(0,s.jsx)(n.code,{children:"2 \xd7 embedding_dim"})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-architecture",children:"\ud83d\udd38 Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["A simple ",(0,s.jsx)(n.strong,{children:"feedforward neural network"})," with:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Input: Concatenated context embeddings"}),"\n",(0,s.jsx)(n.li,{children:"Hidden layer(s): With ReLU or similar activations"}),"\n",(0,s.jsx)(n.li,{children:"Output layer: One neuron per word in vocabulary"}),"\n",(0,s.jsxs)(n.li,{children:["Activation: ",(0,s.jsx)(n.strong,{children:"Softmax"})," to turn scores into probabilities"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-prediction",children:"\ud83d\udd38 Prediction"}),"\n",(0,s.jsxs)(n.p,{children:["Predict the word ",(0,s.jsx)(n.code,{children:"w_t"})," by choosing:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"argmax(P(w_t | context))\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-limitations-of-basic-neural-n-gram-models",children:"\ud83d\udeab Limitations of Basic Neural N-Gram Models"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fixed context window"}),": Can't model dependencies outside it"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No sequence awareness"}),": Doesn\u2019t know position/timing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No memory"}),": Doesn\u2019t track prior information like RNNs or transformers"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-recap",children:"\u2705 Recap"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Concept"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"N-Gram"}),(0,s.jsxs)(n.td,{children:["Predicts next word from last ",(0,s.jsx)(n.code,{children:"n-1"})," words"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Bi-Gram"}),(0,s.jsx)(n.td,{children:"Context = 1 previous word"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Tri-Gram"}),(0,s.jsx)(n.td,{children:"Context = 2 previous words"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Neural N-Gram"}),(0,s.jsx)(n.td,{children:"Learns prediction using embeddings + feedforward NN"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Softmax"}),(0,s.jsx)(n.td,{children:"Converts raw scores to probabilities"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Limitation"}),(0,s.jsx)(n.td,{children:"Cannot capture long-range or sequential dependencies"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>t});var r=i(6540);const s={},d=r.createContext(s);function l(e){const n=r.useContext(d);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(d.Provider,{value:n},e.children)}}}]);