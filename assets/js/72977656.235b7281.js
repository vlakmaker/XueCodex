"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[2522],{4453:e=>{e.exports=JSON.parse('{"tag":{"label":"transformers","permalink":"/docs/tags/transformers","allTagsPath":"/docs/tags","count":3,"items":[{"id":"topics/deep-learning/nlp/transformer-architecture","title":"Transformer Architecture ","description":"---","permalink":"/docs/topics/deep-learning/nlp/transformer-architecture"},{"id":"topics/deep-learning/nlp/transformer-attention","title":"Transformers and Attention \u2013 Explained with Visuals","description":"---","permalink":"/docs/topics/deep-learning/nlp/transformer-attention"},{"id":"topics/deep-learning/nlp/self-attention","title":"What Is Self-Attention?","description":"Self-attention allows a model to look at all the other words in a sentence (or a document, or code...) and decide how important each of them is for understanding a particular word.","permalink":"/docs/topics/deep-learning/nlp/self-attention"}],"unlisted":false}}')}}]);