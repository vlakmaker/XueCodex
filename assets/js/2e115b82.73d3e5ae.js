"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[8929],{4957:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>r,default:()=>c,frontMatter:()=>s,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"topics/deep-learning/weight-initialization","title":"Weight Initialization in Neural Networks","description":"You\u2019ve learned how to do forward propagation, backpropagation, and gradient descent. But now comes a crucial design decision: How do we initialize the weights?","source":"@site/docs/topics/deep-learning/weight-initialization.md","sourceDirName":"topics/deep-learning","slug":"/topics/deep-learning/weight-initialization","permalink":"/docs/topics/deep-learning/weight-initialization","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/weight-initialization.md","tags":[{"inline":true,"label":"machine-learning","permalink":"/docs/tags/machine-learning"},{"inline":true,"label":"deep learning","permalink":"/docs/tags/deep-learning"},{"inline":true,"label":"weights","permalink":"/docs/tags/weights"}],"version":"current","frontMatter":{"id":"weight-initialization","title":"Weight Initialization in Neural Networks","tags":["machine-learning","deep learning","weights"]},"sidebar":"tutorialSidebar","previous":{"title":"Training a Document Classifier","permalink":"/docs/topics/deep-learning/training-document-classifier"},"next":{"title":"weights","permalink":"/docs/topics/deep-learning/weights"}}');var l=n(4848),a=n(8453);const s={id:"weight-initialization",title:"Weight Initialization in Neural Networks",tags:["machine-learning","deep learning","weights"]},r="\ud83c\udfaf  Weight Initialization in Neural Networks",d={},o=[{value:"\u2744\ufe0f What Happens If You Initialize All Weights to Zero?",id:"\ufe0f-what-happens-if-you-initialize-all-weights-to-zero",level:2},{value:"\ud83e\udde0 Summary:",id:"-summary",level:3},{value:"\ud83d\udca5 Why This Happens:",id:"-why-this-happens",level:3},{value:"\u274c Result:",id:"-result",level:3},{value:"\ud83c\udfb2 Random Initialization to the Rescue!",id:"-random-initialization-to-the-rescue",level:2},{value:"\ud83d\udd01 What\u2019s Different:",id:"-whats-different",level:3},{value:"\ud83d\udcc9 Why Small Values?",id:"-why-small-values",level:3},{value:"\u2705 Bonus: Xavier and He Initialization",id:"-bonus-xavier-and-he-initialization",level:3},{value:"\ud83d\udd0d Slide Visual Recap",id:"-slide-visual-recap",level:2},{value:"Slide 1: Zero Initialization \u274c",id:"slide-1-zero-initialization-",level:3},{value:"Slide 2: Random Initialization \u2705",id:"slide-2-random-initialization-",level:3},{value:"\ud83e\uddfe TL;DR",id:"-tldr",level:2},{value:"\ud83d\udccc Codex Rule of Thumb:",id:"-codex-rule-of-thumb",level:2}];function h(e){const i={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(i.header,{children:(0,l.jsx)(i.h1,{id:"--weight-initialization-in-neural-networks",children:"\ud83c\udfaf  Weight Initialization in Neural Networks"})}),"\n",(0,l.jsxs)(i.p,{children:["You\u2019ve learned how to do forward propagation, backpropagation, and gradient descent. But now comes a crucial design decision: ",(0,l.jsx)(i.strong,{children:"How do we initialize the weights?"})]}),"\n",(0,l.jsx)(i.h2,{id:"\ufe0f-what-happens-if-you-initialize-all-weights-to-zero",children:"\u2744\ufe0f What Happens If You Initialize All Weights to Zero?"}),"\n",(0,l.jsx)(i.h3,{id:"-summary",children:"\ud83e\udde0 Summary:"}),"\n",(0,l.jsx)(i.p,{children:"If all weights in a layer are initialized to 0, all neurons in that layer will compute the exact same output. As a result, during training:"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"The gradients for each neuron will be the same."}),"\n",(0,l.jsx)(i.li,{children:"All neurons will update identically."}),"\n",(0,l.jsxs)(i.li,{children:["There will be ",(0,l.jsx)(i.strong,{children:"no diversity"}),", and they will learn the same features."]}),"\n"]}),"\n",(0,l.jsxs)(i.p,{children:["This is known as the ",(0,l.jsx)(i.strong,{children:"symmetry problem"})," \u2014 and it\u2019s bad because it defeats the purpose of having multiple neurons."]}),"\n",(0,l.jsx)(i.h3,{id:"-why-this-happens",children:"\ud83d\udca5 Why This Happens:"}),"\n",(0,l.jsx)(i.p,{children:"Neurons are designed to specialize. If you initialize them the same way and give them the same inputs, they\u2019ll just echo each other."}),"\n",(0,l.jsx)(i.h3,{id:"-result",children:"\u274c Result:"}),"\n",(0,l.jsx)(i.p,{children:"The model fails to learn complex representations. It might as well be a linear model."}),"\n",(0,l.jsx)(i.h2,{id:"-random-initialization-to-the-rescue",children:"\ud83c\udfb2 Random Initialization to the Rescue!"}),"\n",(0,l.jsx)(i.h3,{id:"-whats-different",children:"\ud83d\udd01 What\u2019s Different:"}),"\n",(0,l.jsxs)(i.p,{children:["Instead of setting weights to 0, we randomly initialize them with ",(0,l.jsx)(i.strong,{children:"small values"})," (e.g. multiply random values by 0.01)."]}),"\n",(0,l.jsx)(i.pre,{children:(0,l.jsx)(i.code,{children:"W[1] = np.random.`randn(layer_dims[1], layer_dims[0])` * 0.01\n"})}),"\n",(0,l.jsx)(i.p,{children:"This ensures:"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"Each neuron starts with different weights"}),"\n",(0,l.jsxs)(i.li,{children:["The model ",(0,l.jsx)(i.strong,{children:"breaks symmetry"})]}),"\n",(0,l.jsxs)(i.li,{children:["Neurons can learn ",(0,l.jsx)(i.strong,{children:"different features"})]}),"\n"]}),"\n",(0,l.jsx)(i.h3,{id:"-why-small-values",children:"\ud83d\udcc9 Why Small Values?"}),"\n",(0,l.jsx)(i.p,{children:"If weights are too large:"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsxs)(i.li,{children:["Activations like ",(0,l.jsx)(i.code,{children:"sigmoid"})," and ",(0,l.jsx)(i.code,{children:"tanh"})," saturate (flatten out)"]}),"\n",(0,l.jsx)(i.li,{children:"Gradients vanish \u2192 learning slows down"}),"\n"]}),"\n",(0,l.jsx)(i.p,{children:"If weights are too small:"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"It might learn slowly, but at least it learns safely"}),"\n"]}),"\n",(0,l.jsx)(i.h3,{id:"-bonus-xavier-and-he-initialization",children:"\u2705 Bonus: Xavier and He Initialization"}),"\n",(0,l.jsx)(i.p,{children:"Modern deep learning uses smarter strategies:"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"Xavier initialization"})," for ",(0,l.jsx)(i.code,{children:"tanh"})]}),"\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"He initialization"})," for ",(0,l.jsx)(i.code,{children:"ReLU"})]}),"\n"]}),"\n",(0,l.jsx)(i.p,{children:"These scale weights based on the number of neurons in a layer to maintain stable gradient flow."}),"\n",(0,l.jsx)(i.h2,{id:"-slide-visual-recap",children:"\ud83d\udd0d Slide Visual Recap"}),"\n",(0,l.jsx)(i.h3,{id:"slide-1-zero-initialization-",children:"Slide 1: Zero Initialization \u274c"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"All neurons receive same input \u2192 same output \u2192 same gradient"}),"\n",(0,l.jsx)(i.li,{children:"All weights update identically \u2192 no symmetry breaking"}),"\n"]}),"\n",(0,l.jsx)(i.h3,{id:"slide-2-random-initialization-",children:"Slide 2: Random Initialization \u2705"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"Weights are random and small"}),"\n",(0,l.jsx)(i.li,{children:"Each neuron starts unique"}),"\n",(0,l.jsx)(i.li,{children:"Diverse features can be learned"}),"\n",(0,l.jsx)(i.li,{children:"Model trains effectively"}),"\n"]}),"\n",(0,l.jsx)(i.h2,{id:"-tldr",children:"\ud83e\uddfe TL;DR"}),"\n",(0,l.jsxs)(i.table,{children:[(0,l.jsx)(i.thead,{children:(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.th,{children:"Initialization"}),(0,l.jsx)(i.th,{children:"Symmetry Broken?"}),(0,l.jsx)(i.th,{children:"Risk of Vanishing Gradient"}),(0,l.jsx)(i.th,{children:"Learning Outcome"})]})}),(0,l.jsxs)(i.tbody,{children:[(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Zeros"})}),(0,l.jsx)(i.td,{children:"\u274c No"}),(0,l.jsx)(i.td,{children:"No"}),(0,l.jsx)(i.td,{children:"All neurons same"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Random Large"})}),(0,l.jsx)(i.td,{children:"\u2705 Yes"}),(0,l.jsx)(i.td,{children:"\u2705 Yes"}),(0,l.jsx)(i.td,{children:"Unstable training"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Random Small"})}),(0,l.jsx)(i.td,{children:"\u2705 Yes"}),(0,l.jsx)(i.td,{children:"\u274c Safer"}),(0,l.jsx)(i.td,{children:"Stable training"})]})]})]}),"\n",(0,l.jsx)(i.h2,{id:"-codex-rule-of-thumb",children:"\ud83d\udccc Codex Rule of Thumb:"}),"\n",(0,l.jsx)(i.p,{children:'> "Don\u2019t let neurons be clones. Randomize to specialize."\n>'})]})}function c(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,l.jsx)(i,{...e,children:(0,l.jsx)(h,{...e})}):h(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>s,x:()=>r});var t=n(6540);const l={},a=t.createContext(l);function s(e){const i=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:s(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);