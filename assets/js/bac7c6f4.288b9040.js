"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[7571],{2703:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"topics/deep-learning/word-embeddings","title":"Word Embeddings & Sequence Models in NLP","description":"Word2Vec Overview","source":"@site/docs/topics/deep-learning/word-embeddings.md","sourceDirName":"topics/deep-learning","slug":"/topics/deep-learning/word-embeddings","permalink":"/docs/topics/deep-learning/word-embeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/word-embeddings.md","tags":[{"inline":true,"label":"embeddings","permalink":"/docs/tags/embeddings"},{"inline":true,"label":"word2vec","permalink":"/docs/tags/word-2-vec"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"word-embeddings","title":"Word Embeddings & Sequence Models in NLP","tags":["embeddings","word2vec","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"What are Neural Networks?","permalink":"/docs/topics/deep-learning/neural-networks"},"next":{"title":"AI Engineering","permalink":"/docs/topics/ai-engineering/"}}');var i=r(4848),t=r(8453);const d={id:"word-embeddings",title:"Word Embeddings & Sequence Models in NLP",tags:["embeddings","word2vec","neural networks"]},o="Word Embeddings & Sequence Models in NLP",l={},c=[{value:"Word2Vec Overview",id:"word2vec-overview",level:2},{value:"\ud83d\udd00 Two Architectures:",id:"-two-architectures",level:3},{value:"CBOW (Continuous Bag of Words)",id:"cbow-continuous-bag-of-words",level:2},{value:"Skip-Gram",id:"skip-gram",level:2},{value:"GloVe (Global Vectors for Word Representation)",id:"glove-global-vectors-for-word-representation",level:2},{value:"Sequence-to-Sequence (Seq2Seq) Models",id:"sequence-to-sequence-seq2seq-models",level:2},{value:"Structure:",id:"structure",level:3},{value:"Recurrent Neural Networks (RNNs)",id:"recurrent-neural-networks-rnns",level:2},{value:"Characteristics:",id:"characteristics",level:3},{value:"Limitations:",id:"limitations",level:3},{value:"GRUs &amp; LSTMs",id:"grus--lstms",level:2},{value:"GRU (Gated Recurrent Unit):",id:"gru-gated-recurrent-unit",level:3},{value:"LSTM (Long Short-Term Memory):",id:"lstm-long-short-term-memory",level:3},{value:"Additional Concepts Often Glossed Over",id:"additional-concepts-often-glossed-over",level:2},{value:"Questions to Explore",id:"questions-to-explore",level:2},{value:"Summary",id:"summary",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"word-embeddings--sequence-models-in-nlp",children:"Word Embeddings & Sequence Models in NLP"})}),"\n",(0,i.jsx)(n.h2,{id:"word2vec-overview",children:"Word2Vec Overview"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Word2Vec"})," refers to a family of models that learns to represent words as dense, continuous vectors \u2014 capturing semantic relationships and meaning."]}),"\n",(0,i.jsx)(n.h3,{id:"-two-architectures",children:"\ud83d\udd00 Two Architectures:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Input"}),(0,i.jsx)(n.th,{children:"Predicts"}),(0,i.jsx)(n.th,{children:"Ideal for"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"CBOW"}),(0,i.jsx)(n.td,{children:"Context words"}),(0,i.jsx)(n.td,{children:"Target word"}),(0,i.jsx)(n.td,{children:"Frequent words"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Skip-Gram"}),(0,i.jsx)(n.td,{children:"Target word"}),(0,i.jsx)(n.td,{children:"Context words"}),(0,i.jsx)(n.td,{children:"Rare words"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"Each uses a simple feedforward neural net:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": One-hot encoded word"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hidden Layer"}),": Learns embeddings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output Layer"}),": Predicts word via softmax (or approximated with negative sampling)"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"cbow-continuous-bag-of-words",children:"CBOW (Continuous Bag of Words)"}),"\n",(0,i.jsx)(n.p,{children:"CBOW predicts a center word using its context. It averages the embeddings of context words and passes the result to a neural network to predict the target word."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Context words (e.g., ",(0,i.jsx)(n.code,{children:"I ___ dogs"}),' \u2192 "love")']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),": Target word"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Drawback"}),": Loses word order information"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"skip-gram",children:"Skip-Gram"}),"\n",(0,i.jsx)(n.p,{children:"Skip-Gram is the reverse of CBOW \u2014 it predicts context words given a target word."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),': One word (e.g., "love")']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output"}),': Multiple context words (e.g., "I", "dogs")']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advantage"}),": Performs well for rare words and small datasets"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"glove-global-vectors-for-word-representation",children:"GloVe (Global Vectors for Word Representation)"}),"\n",(0,i.jsx)(n.p,{children:"GloVe is a pretrained word embedding model trained on word co-occurrence statistics from large corpora."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Source"}),": Trained on datasets like Wikipedia, Common Crawl"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),": Load into PyTorch for better initialization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Access"}),": ",(0,i.jsx)(n.code,{children:"torchtext.vocab.GloVe"})]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"sequence-to-sequence-seq2seq-models",children:"Sequence-to-Sequence (Seq2Seq) Models"}),"\n",(0,i.jsx)(n.p,{children:"Seq2Seq models handle variable-length inputs and outputs. They're widely used in:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Machine Translation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Summarization"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Conversational Agents (Chatbots)"})}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"structure",children:"Structure:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Encoder"}),": Converts input sequence to a context vector"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Decoder"}),": Generates output sequence from that vector"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Supports:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Sequence-to-sequence"}),"\n",(0,i.jsx)(n.li,{children:"Sequence-to-label"}),"\n",(0,i.jsx)(n.li,{children:"Label-to-sequence"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"recurrent-neural-networks-rnns",children:"Recurrent Neural Networks (RNNs)"}),"\n",(0,i.jsxs)(n.p,{children:["RNNs process sequences by maintaining a ",(0,i.jsx)(n.strong,{children:"hidden state"})," that evolves over time."]}),"\n",(0,i.jsx)(n.h3,{id:"characteristics",children:"Characteristics:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ideal for time series and language"}),"\n",(0,i.jsx)(n.li,{children:"Each output depends on previous inputs"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"limitations",children:"Limitations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Struggles with long-term dependencies"}),"\n",(0,i.jsx)(n.li,{children:"Suffers from vanishing gradients"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"grus--lstms",children:"GRUs & LSTMs"}),"\n",(0,i.jsx)(n.h3,{id:"gru-gated-recurrent-unit",children:"GRU (Gated Recurrent Unit):"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Update Gate"}),": Controls how much past information to keep"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reset Gate"}),": Controls how much past info to forget"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"lstm-long-short-term-memory",children:"LSTM (Long Short-Term Memory):"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Forget Gate"}),": Decides what info to discard"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input Gate"}),": Controls what new info to add"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output Gate"}),": Decides what part of memory to output"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Both improve long-term memory handling in sequence tasks."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"additional-concepts-often-glossed-over",children:"Additional Concepts Often Glossed Over"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Concept"}),(0,i.jsx)(n.th,{children:"Explanation"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Negative Sampling"})}),(0,i.jsx)(n.td,{children:"Trains softmax efficiently using a few negative examples"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Padding"})}),(0,i.jsx)(n.td,{children:"Used to batch sequences of different lengths"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Top-k Sampling"})}),(0,i.jsx)(n.td,{children:"More fluent generation than greedy decoding"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Attention"})}),(0,i.jsx)(n.td,{children:"Lets models focus on relevant input tokens (used in Transformers)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Embeddings vs One-Hot"})}),(0,i.jsx)(n.td,{children:"Embeddings capture similarity and reduce dimensionality"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"questions-to-explore",children:"Questions to Explore"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Why does Skip-Gram outperform CBOW for rare words?"}),"\n",(0,i.jsx)(n.li,{children:"When should you use GloVe vs training your own embeddings?"}),"\n",(0,i.jsx)(n.li,{children:"How do Seq2Seq models generalize across languages or tasks?"}),"\n",(0,i.jsx)(n.li,{children:"What are the advantages of GRUs over LSTMs, and vice versa?"}),"\n",(0,i.jsx)(n.li,{children:"How can we visualize and interpret word vectors?"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Word2Vec and GloVe are fundamental to word embeddings"}),"\n",(0,i.jsx)(n.li,{children:"CBOW and Skip-Gram offer different trade-offs"}),"\n",(0,i.jsx)(n.li,{children:"Seq2Seq models allow mapping inputs to outputs flexibly"}),"\n",(0,i.jsx)(n.li,{children:"RNNs are enhanced by GRU and LSTM architectures for memory"}),"\n",(0,i.jsx)(n.li,{children:"Pretrained embeddings like GloVe offer powerful starting points"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This foundation is essential for deeper work with language models, transformers, and applied NLP systems."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>d,x:()=>o});var s=r(6540);const i={},t=s.createContext(i);function d(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);