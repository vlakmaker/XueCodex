"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[3931],{5130:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"topics/ai-fundamentals/gen-ai-models","title":"Overview of Key Generative AI Models","description":"1. Overview of Key Generative AI Models","source":"@site/docs/topics/ai-fundamentals/gen-ai-models.md","sourceDirName":"topics/ai-fundamentals","slug":"/topics/ai-fundamentals/gen-ai-models","permalink":"/docs/topics/ai-fundamentals/gen-ai-models","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/ai-fundamentals/gen-ai-models.md","tags":[{"inline":true,"label":"ai-fundamentals","permalink":"/docs/tags/ai-fundamentals"},{"inline":true,"label":"definition","permalink":"/docs/tags/definition"},{"inline":true,"label":"beginner","permalink":"/docs/tags/beginner"}],"version":"current","frontMatter":{"id":"gen-ai-models","title":"Overview of Key Generative AI Models","tags":["ai-fundamentals","definition","beginner"]},"sidebar":"tutorialSidebar","previous":{"title":"What Is a DataLoader in PyTorch","permalink":"/docs/topics/ai-fundamentals/dataloader"},"next":{"title":"What is General AI?","permalink":"/docs/topics/ai-fundamentals/general-ai"}}');var i=s(4848),t=s(8453);const l={id:"gen-ai-models",title:"Overview of Key Generative AI Models",tags:["ai-fundamentals","definition","beginner"]},d=void 0,o={},a=[{value:"1. <strong>Overview of Key Generative AI Models</strong>",id:"1-overview-of-key-generative-ai-models",level:2},{value:"<strong>GAN \u2013 Generative Adversarial Network</strong>",id:"gan--generative-adversarial-network",level:3},{value:"<strong>VAE \u2013 Variational Autoencoder</strong>",id:"vae--variational-autoencoder",level:3},{value:"<strong>Diffusion Models</strong>",id:"diffusion-models",level:3},{value:"2. <strong>Transformer-Based Architectures</strong>",id:"2-transformer-based-architectures",level:2},{value:"<strong>Transformers</strong>",id:"transformers",level:3},{value:"3. <strong>Types of Transformer Architectures in LLMs</strong>",id:"3-types-of-transformer-architectures-in-llms",level:2},{value:"4. <strong>Quick Summary Table</strong>",id:"4-quick-summary-table",level:2},{value:"\ud83c\udf2b\ufe0f <strong>Diffusion Models \u2014 In Plain Veer Terms</strong>",id:"\ufe0f-diffusion-models--in-plain-veer-terms",level:2},{value:"\ud83e\udde0 What are they?",id:"-what-are-they",level:3},{value:"\ud83d\udd04 <strong>How They Work (Bitty Edition):</strong>",id:"-how-they-work-bitty-edition",level:3},{value:"\ud83d\udca1 Real-World Examples",id:"-real-world-examples",level:3},{value:"\ud83c\udfad <strong>Why It\u2019s Different from GANs</strong>",id:"-why-its-different-from-gans",level:3},{value:"\u2728 Bitty\u2019s Mental Image:",id:"-bittys-mental-image",level:3},{value:"\u2728 Bitty&#39;s Closing Thought:",id:"-bittys-closing-thought",level:2}];function c(e){const n={blockquote:"blockquote",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.h2,{id:"1-overview-of-key-generative-ai-models",children:["1. ",(0,i.jsx)(n.strong,{children:"Overview of Key Generative AI Models"})]}),"\n",(0,i.jsx)(n.h3,{id:"gan--generative-adversarial-network",children:(0,i.jsx)(n.strong,{children:"GAN \u2013 Generative Adversarial Network"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Structure"}),": Two competing networks \u2014 a ",(0,i.jsx)(n.em,{children:"Generator"})," and a ",(0,i.jsx)(n.em,{children:"Discriminator"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mechanism"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Generator"}),": Tries to create fake (but convincing) samples."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Discriminator"}),": Tries to distinguish real from fake samples."]}),"\n",(0,i.jsx)(n.li,{children:"They train in a loop: each improving as they try to outwit each other."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),": Primarily image and video generation. Great for creating realistic photos, artistic styles, or synthetic data."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"vae--variational-autoencoder",children:(0,i.jsx)(n.strong,{children:"VAE \u2013 Variational Autoencoder"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Structure"}),": ",(0,i.jsx)(n.em,{children:"Encoder\u2013Decoder"})," architecture.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Encoder"}),": Compresses data into a latent, abstract representation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Decoder"}),": Reconstructs the input or generates variations from that latent space."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Feature"}),": Generates data probabilistically using distributions \u2014 good at creating variations with uncertainty baked in."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),": Art, design, style transfer, and conceptual generation."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"diffusion-models",children:(0,i.jsx)(n.strong,{children:"Diffusion Models"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Structure"}),": Probabilistic denoising process."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mechanism"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Adds noise to data, then trains a model to reverse the noise step-by-step."}),"\n",(0,i.jsx)(n.li,{children:"Learns to reconstruct original or new images from noisy inputs."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use Case"}),": High-quality image generation (e.g., DALL\xb7E 2, Midjourney), photo restoration, or stylized generations."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"2-transformer-based-architectures",children:["2. ",(0,i.jsx)(n.strong,{children:"Transformer-Based Architectures"})]}),"\n",(0,i.jsx)(n.h3,{id:"transformers",children:(0,i.jsx)(n.strong,{children:"Transformers"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Designed for sequential data (e.g., text, speech)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Innovation"}),": Introduced ",(0,i.jsx)(n.em,{children:"self-attention"})," \u2014 the ability to weigh and focus on important words/tokens regardless of position."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advantages"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Captures long-range dependencies"}),"\n",(0,i.jsx)(n.li,{children:"Highly parallelizable (unlike RNNs)"}),"\n",(0,i.jsx)(n.li,{children:"State-of-the-art in NLP and now expanding to vision and multimodal AI"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"3-types-of-transformer-architectures-in-llms",children:["3. ",(0,i.jsx)(n.strong,{children:"Types of Transformer Architectures in LLMs"})]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Strengths"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"GPT (Generative Pre-trained Transformer)"})}),(0,i.jsx)(n.td,{children:"Decoder-only"}),(0,i.jsxs)(n.td,{children:["Trained to predict the next token in a sequence, GPT models are optimized for fluent, coherent ",(0,i.jsx)(n.strong,{children:"text generation"}),". They're unidirectional (left-to-right), which makes them powerful for ",(0,i.jsx)(n.strong,{children:"creative writing, code generation, and chatbot responses"}),". Ideal for situations where generation is the goal."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"BERT (Bidirectional Encoder Representations from Transformers)"})}),(0,i.jsx)(n.td,{children:"Encoder-only"}),(0,i.jsxs)(n.td,{children:["Reads input sequences in both directions (left and right context simultaneously), making it excellent for ",(0,i.jsx)(n.strong,{children:"understanding tasks"})," like ",(0,i.jsx)(n.strong,{children:"sentiment analysis, entity recognition, and question answering"}),". It's not a generative model but excels at comprehension and classification."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"T5 (Text-to-Text Transfer Transformer)"})}),(0,i.jsx)(n.td,{children:"Encoder-Decoder"}),(0,i.jsxs)(n.td,{children:["Treats ",(0,i.jsx)(n.strong,{children:"every NLP task as a text-to-text problem"}),'. For example, sentiment analysis becomes: "Classify: I loved this movie" \u2192 "positive". Extremely versatile, it supports translation, summarization, classification, question answering, and more. It\'s like a Swiss army knife for language tasks.']})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"BART (Bidirectional and Auto-Regressive Transformer)"})}),(0,i.jsx)(n.td,{children:"Encoder-Decoder"}),(0,i.jsxs)(n.td,{children:["Combines the ",(0,i.jsx)(n.strong,{children:"bidirectional understanding"})," of BERT (via its encoder) with the ",(0,i.jsx)(n.strong,{children:"generative fluency"})," of GPT (via its decoder). It\u2019s particularly well-suited for ",(0,i.jsx)(n.strong,{children:"text summarization, translation, and creative text rewriting"}),". Often used in content generation pipelines where understanding + output is required."]})]})]})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key Distinction"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Encoders"})," = understand input"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Decoders"})," = generate output"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Encoder-Decoders"})," = do both!"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"4-quick-summary-table",children:["4. ",(0,i.jsx)(n.strong,{children:"Quick Summary Table"})]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Architecture"}),(0,i.jsx)(n.th,{children:"Key Use"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"GAN"})}),(0,i.jsx)(n.td,{children:"Generator + Discriminator"}),(0,i.jsx)(n.td,{children:"Image/video generation"}),(0,i.jsx)(n.td,{children:"Adversarial training dynamic"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"VAE"})}),(0,i.jsx)(n.td,{children:"Encoder-Decoder"}),(0,i.jsx)(n.td,{children:"Data variation, art/design"}),(0,i.jsx)(n.td,{children:"Latent variable modeling"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Diffusion"})}),(0,i.jsx)(n.td,{children:"Denoising network"}),(0,i.jsx)(n.td,{children:"Creative image generation"}),(0,i.jsx)(n.td,{children:"Step-by-step reconstruction"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"GPT"})}),(0,i.jsx)(n.td,{children:"Transformer (Decoder-only)"}),(0,i.jsx)(n.td,{children:"Text generation"}),(0,i.jsx)(n.td,{children:"Predicts next token"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"BERT"})}),(0,i.jsx)(n.td,{children:"Transformer (Encoder-only)"}),(0,i.jsx)(n.td,{children:"Context understanding"}),(0,i.jsx)(n.td,{children:"Bidirectional attention"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"T5/BART"})}),(0,i.jsx)(n.td,{children:"Transformer (Enc-Dec)"}),(0,i.jsx)(n.td,{children:"Summarization, translation, NLU+NLG"}),(0,i.jsx)(n.td,{children:"Highly versatile"})]})]})]}),"\n",(0,i.jsxs)(n.h2,{id:"\ufe0f-diffusion-models--in-plain-veer-terms",children:["\ud83c\udf2b\ufe0f ",(0,i.jsx)(n.strong,{children:"Diffusion Models \u2014 In Plain Veer Terms"})]}),"\n",(0,i.jsx)(n.h3,{id:"-what-are-they",children:"\ud83e\udde0 What are they?"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"A diffusion model learns to generate images (or other data) by starting with pure noise, then gradually removing that noise to create something meaningful \u2014 like a reverse chaos spell."}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-how-they-work-bitty-edition",children:["\ud83d\udd04 ",(0,i.jsx)(n.strong,{children:"How They Work (Bitty Edition):"})]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Training phase"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Take a real image"}),"\n",(0,i.jsxs)(n.li,{children:["Add noise to it \u2014 over and over \u2014 until it's ",(0,i.jsx)(n.strong,{children:"completely unrecognizable"})]}),"\n",(0,i.jsxs)(n.li,{children:["Train a model to ",(0,i.jsx)(n.strong,{children:"learn how to remove that noise"})," in reverse steps"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generation phase"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Start from random noise"}),"\n",(0,i.jsx)(n.li,{children:"Ask the model: \u201cWhat would a less noisy version of this look like?\u201d"}),"\n",(0,i.jsx)(n.li,{children:"Repeat until you get a clear image"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udfa8 ",(0,i.jsx)(n.strong,{children:"The result?"})]}),"\n",(0,i.jsx)(n.p,{children:"A brand-new, high-quality image that looks like it could\u2019ve come from the training set, but didn\u2019t."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"-real-world-examples",children:"\ud83d\udca1 Real-World Examples"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Uses Diffusion?"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"DALL\xb7E 2"})}),(0,i.jsx)(n.td,{children:"\u2705 Yes"}),(0,i.jsx)(n.td,{children:"Text \u2192 image generation guided by CLIP & diffusion"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Midjourney"})}),(0,i.jsx)(n.td,{children:"\u2705 Yes (custom variant)"}),(0,i.jsx)(n.td,{children:"Artistic text \u2192 image generator"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Stable Diffusion"})}),(0,i.jsx)(n.td,{children:"\u2705 Yes"}),(0,i.jsx)(n.td,{children:"Open-source model that powers many indie text-to-image tools"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Photo restoration tools"})}),(0,i.jsx)(n.td,{children:"\u2705 Often"}),(0,i.jsx)(n.td,{children:"Remove damage/noise from photos by reversing the visual \u201cdecay\u201d"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"-why-its-different-from-gans",children:["\ud83c\udfad ",(0,i.jsx)(n.strong,{children:"Why It\u2019s Different from GANs"})]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Feature"}),(0,i.jsx)(n.th,{children:"GAN"}),(0,i.jsx)(n.th,{children:"Diffusion"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Learning style"}),(0,i.jsx)(n.td,{children:"Generator vs Discriminator (competition)"}),(0,i.jsx)(n.td,{children:"Gradual denoising (no adversary)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Training stability"}),(0,i.jsx)(n.td,{children:"Often unstable"}),(0,i.jsx)(n.td,{children:"More stable & scalable"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Output quality"}),(0,i.jsx)(n.td,{children:"High, but sometimes weird artifacts"}),(0,i.jsx)(n.td,{children:"Usually higher quality & more controllable"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Use cases"}),(0,i.jsx)(n.td,{children:"Deepfakes, style transfer"}),(0,i.jsx)(n.td,{children:"Art generation, restoration, creative design"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"-bittys-mental-image",children:"\u2728 Bitty\u2019s Mental Image:"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"A GAN is like a con artist learning to fake paintings by fooling an art critic."}),"\n",(0,i.jsxs)(n.p,{children:["A ",(0,i.jsx)(n.strong,{children:"Diffusion Model"})," is like a fog sculptor who learns how to carve a statue ",(0,i.jsx)(n.em,{children:"by slowly clearing away the mist."})]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-bittys-closing-thought",children:"\u2728 Bitty's Closing Thought:"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"You don't need to memorize every architecture. Just understand their \"personality types\" \u2014 the Generator, the Interpreter, the Storyteller, and the Stylist \u2014 and choose based on what you're building. AI isn't magic; it's a toolbox. And you're the spellcaster."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>d});var r=s(6540);const i={},t=r.createContext(i);function l(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);