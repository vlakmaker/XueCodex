"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[9527],{3938:e=>{e.exports=JSON.parse('{"tag":{"label":"nlp","permalink":"/docs/tags/nlp","allTagsPath":"/docs/tags","count":7,"items":[{"id":"topics/deep-learning/nlp/attentoin-mechanism","title":"Attention Mechanism \u2014 A Gentle but Deep Dive","description":"1\ufe0f\u20e3 What Is the Attention Mechanism and Why Is It Important?","permalink":"/docs/topics/deep-learning/nlp/attentoin-mechanism"},{"id":"topics/deep-learning/nlp/language-ai-tasks","title":"Language AI Tasks (NLP Task Overview)","description":"1. \ud83e\udde0 Understanding (Analysis)","permalink":"/docs/topics/deep-learning/nlp/language-ai-tasks"},{"id":"topics/deep-learning/nlp/bag-of-words","title":"Language as a Bag-of-Words","description":"Conceptual Walkthrough with \u201cMy cat is cute\u201d","permalink":"/docs/topics/deep-learning/nlp/bag-of-words"},{"id":"topics/deep-learning/nlp/Multi-head attention","title":"Multihead Attention","description":"\ud83e\udde0 1. What Is Scaled Dot-Product Attention?","permalink":"/docs/topics/deep-learning/nlp/Multi-head attention"},{"id":"topics/deep-learning/nlp/transformer-architecture","title":"Transformer Architecture ","description":"---","permalink":"/docs/topics/deep-learning/nlp/transformer-architecture"},{"id":"topics/deep-learning/nlp/transformer-attention","title":"Transformers and Attention \u2013 Explained with Visuals","description":"---","permalink":"/docs/topics/deep-learning/nlp/transformer-attention"},{"id":"topics/deep-learning/nlp/self-attention","title":"What Is Self-Attention?","description":"Self-attention allows a model to look at all the other words in a sentence (or a document, or code...) and decide how important each of them is for understanding a particular word.","permalink":"/docs/topics/deep-learning/nlp/self-attention"}],"unlisted":false}}')}}]);