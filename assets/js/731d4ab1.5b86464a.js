"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[392],{4434:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"topics/deep-learning/nlp/embeddings","title":"What Are Embeddings","description":"Embeddings are how we turn symbolic data (like words, sentences, images, or even users) into vectors of numbers \u2014 in a way that captures meaning, similarity, or structure.","source":"@site/docs/topics/deep-learning/nlp/embeddings.md","sourceDirName":"topics/deep-learning/nlp","slug":"/topics/deep-learning/nlp/embeddings","permalink":"/docs/topics/deep-learning/nlp/embeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/nlp/embeddings.md","tags":[{"inline":true,"label":"embeddings","permalink":"/docs/tags/embeddings"},{"inline":true,"label":"word2vec","permalink":"/docs/tags/word-2-vec"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"embeddings","title":"What Are Embeddings","tags":["embeddings","word2vec","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"Converting Words to Features in NLP","permalink":"/docs/topics/deep-learning/nlp/convert-words-features"},"next":{"title":"Encoders, Decoders, and Attention","permalink":"/docs/topics/deep-learning/nlp/encoder-decoder"}}');var d=s(4848),t=s(8453);const i={id:"embeddings",title:"What Are Embeddings",tags:["embeddings","word2vec","neural networks"]},o="\ud83e\udde0 What Are Embeddings?",l={},a=[{value:"\ud83d\udd22 Why Not Just Use Words Directly?",id:"-why-not-just-use-words-directly",level:3},{value:"\ud83d\udcab Embeddings Fix That",id:"-embeddings-fix-that",level:2},{value:"\ud83e\udded Embeddings In Practice",id:"-embeddings-in-practice",level:2},{value:"\ud83e\uddea Example: Word Embeddings",id:"-example-word-embeddings",level:2},{value:"\ud83d\udd0d TL;DR \u2013 What Are Embeddings?",id:"-tldr--what-are-embeddings",level:2}];function c(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.header,{children:(0,d.jsx)(n.h1,{id:"-what-are-embeddings",children:"\ud83e\udde0 What Are Embeddings?"})}),"\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.strong,{children:"Embeddings"})," are how we turn symbolic data (like words, sentences, images, or even users) into ",(0,d.jsx)(n.strong,{children:"vectors of numbers"})," \u2014 in a way that captures ",(0,d.jsx)(n.strong,{children:"meaning, similarity, or structure"}),"."]}),"\n",(0,d.jsxs)(n.blockquote,{children:["\n",(0,d.jsx)(n.p,{children:"Think of embeddings as giving your data a GPS coordinate in meaning-space."}),"\n"]}),"\n",(0,d.jsx)(n.p,{children:"They\u2019re the bridge between raw input and neural network understanding."}),"\n",(0,d.jsx)(n.hr,{}),"\n",(0,d.jsx)(n.h3,{id:"-why-not-just-use-words-directly",children:"\ud83d\udd22 Why Not Just Use Words Directly?"}),"\n",(0,d.jsx)(n.p,{children:"Computers can\u2019t understand raw text like \u201cdragon\u201d or \u201chonor.\u201d We need to turn them into numbers. One way is one-hot encoding:"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"ini\nCopyEdit\ndragon = [0, 0, 0, 1, 0, 0, 0]\nhonor  = [0, 1, 0, 0, 0, 0, 0]\n\n"})}),"\n",(0,d.jsxs)(n.p,{children:["But that doesn\u2019t say anything about how ",(0,d.jsx)(n.strong,{children:"similar"}),' "dragon" and "honor" are \u2014 they\u2019re just orthogonal blobs.']}),"\n",(0,d.jsx)(n.hr,{}),"\n",(0,d.jsx)(n.h2,{id:"-embeddings-fix-that",children:"\ud83d\udcab Embeddings Fix That"}),"\n",(0,d.jsxs)(n.p,{children:["An ",(0,d.jsx)(n.strong,{children:"embedding"})," maps each item (like a word) to a ",(0,d.jsx)(n.strong,{children:"dense vector of real numbers"}),". The vector is learned by a model and captures ",(0,d.jsx)(n.strong,{children:"semantic or structural relationships"}),"."]}),"\n",(0,d.jsxs)(n.blockquote,{children:["\n",(0,d.jsx)(n.p,{children:'So instead of random position, "dragon" and "honor" are placed closer in vector space if they appear in similar contexts.'}),"\n"]}),"\n",(0,d.jsx)(n.hr,{}),"\n",(0,d.jsx)(n.h2,{id:"-embeddings-in-practice",children:"\ud83e\udded Embeddings In Practice"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Domain"}),(0,d.jsx)(n.th,{children:"Embedding Example"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"NLP"}),(0,d.jsx)(n.td,{children:"Words \u2192 vectors (Word2Vec, GloVe, BERT)"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Images"}),(0,d.jsx)(n.td,{children:"Images \u2192 vectors (ResNet, CLIP)"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Recommenders"}),(0,d.jsx)(n.td,{children:"Users & items \u2192 embedding vectors"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"Graphs"}),(0,d.jsx)(n.td,{children:"Nodes \u2192 embeddings (e.g., Node2Vec, GNNs)"})]})]})]}),"\n",(0,d.jsx)(n.p,{children:"These vectors are often used to:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsx)(n.li,{children:"Compare similarity (cosine distance, dot product)"}),"\n",(0,d.jsx)(n.li,{children:"Feed into deeper neural layers"}),"\n",(0,d.jsx)(n.li,{children:"Store in vector databases for fast search (e.g., RAG)"}),"\n"]}),"\n",(0,d.jsx)(n.hr,{}),"\n",(0,d.jsx)(n.h2,{id:"-example-word-embeddings",children:"\ud83e\uddea Example: Word Embeddings"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Word"}),(0,d.jsx)(n.th,{children:"Embedding Vector (truncated)"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"king"}),(0,d.jsx)(n.td,{children:"[0.61, -0.12, 0.43, ..., 0.09]"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"queen"}),(0,d.jsx)(n.td,{children:"[0.59, -0.11, 0.45, ..., 0.10]"})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"banana"}),(0,d.jsx)(n.td,{children:"[-0.28, 0.77, 0.13, ..., -0.09]"})]})]})]}),"\n",(0,d.jsx)(n.p,{children:"You can see:"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"king"})," and ",(0,d.jsx)(n.code,{children:"queen"})," are ",(0,d.jsx)(n.strong,{children:"close"})]}),"\n",(0,d.jsxs)(n.li,{children:[(0,d.jsx)(n.code,{children:"banana"})," is somewhere else entirely"]}),"\n"]}),"\n",(0,d.jsx)(n.hr,{}),"\n",(0,d.jsx)(n.h2,{id:"-tldr--what-are-embeddings",children:"\ud83d\udd0d TL;DR \u2013 What Are Embeddings?"}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["A ",(0,d.jsx)(n.strong,{children:"vector representation"})," of some discrete input (like a word)"]}),"\n",(0,d.jsxs)(n.li,{children:["Learned by a model to ",(0,d.jsx)(n.strong,{children:"capture relationships"})]}),"\n",(0,d.jsx)(n.li,{children:"Useful for search, similarity, classification, and as input to neural networks"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var r=s(6540);const d={},t=r.createContext(d);function i(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:i(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);