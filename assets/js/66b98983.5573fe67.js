"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[7542],{8453:(e,n,r)=>{r.d(n,{R:()=>d,x:()=>c});var s=r(6540);const i={},t=s.createContext(i);function d(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),s.createElement(t.Provider,{value:n},e.children)}},9688:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"topics/deep-learning/encoder-decoder-translation","title":"Encoder-Decoder RNN Models for Translation","description":"\ud83e\udde0 Overview","source":"@site/docs/topics/deep-learning/encoder-decoder-translation.md","sourceDirName":"topics/deep-learning","slug":"/topics/deep-learning/encoder-decoder-translation","permalink":"/docs/topics/deep-learning/encoder-decoder-translation","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/encoder-decoder-translation.md","tags":[{"inline":true,"label":"deep learning","permalink":"/docs/tags/deep-learning"},{"inline":true,"label":"encoder","permalink":"/docs/tags/encoder"},{"inline":true,"label":"decoder","permalink":"/docs/tags/decoder"}],"version":"current","frontMatter":{"id":"encoder-decoder-translation","title":"Encoder-Decoder RNN Models for Translation","tags":["deep learning","encoder","decoder"]},"sidebar":"tutorialSidebar","previous":{"title":"The Drivetrain Approach: A Strategic Framework for AI Development","permalink":"/docs/topics/deep-learning/drivetrain-approach"},"next":{"title":"Language Modeling with N-Grams and Neural Networks","permalink":"/docs/topics/deep-learning/llm-ngam-nn"}}');var i=r(4848),t=r(8453);const d={id:"encoder-decoder-translation",title:"Encoder-Decoder RNN Models for Translation",tags:["deep learning","encoder","decoder"]},c="\ud83d\udd04 Encoder-Decoder RNN Models for Translation",o={},l=[{value:"\ud83e\udde0 Overview",id:"-overview",level:2},{value:"\ud83e\udde9 Components",id:"-components",level:2},{value:"\ud83d\udd39 Encoder:",id:"-encoder",level:3},{value:"\ud83d\udd39 Decoder:",id:"-decoder",level:3},{value:"\ud83d\udd39 Teacher Forcing (Training Technique):",id:"-teacher-forcing-training-technique",level:3},{value:"\ud83d\udee0\ufe0f PyTorch Implementation Summary",id:"\ufe0f-pytorch-implementation-summary",level:2},{value:"\ud83c\udfd7\ufe0f Encoder Class:",id:"\ufe0f-encoder-class",level:3},{value:"\ud83c\udfd7\ufe0f Decoder Class:",id:"\ufe0f-decoder-class",level:3},{value:"\ud83d\udd01 Seq2Seq Wrapper Class:",id:"-seq2seq-wrapper-class",level:3},{value:"\ud83d\udd0d Recap",id:"-recap",level:2},{value:"\u2705 Key Takeaways",id:"-key-takeaways",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"-encoder-decoder-rnn-models-for-translation",children:"\ud83d\udd04 Encoder-Decoder RNN Models for Translation"})}),"\n",(0,i.jsx)(n.h2,{id:"-overview",children:"\ud83e\udde0 Overview"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Encoder-Decoder RNN architecture"})," is used in ",(0,i.jsx)(n.strong,{children:"sequence-to-sequence (Seq2Seq)"})," tasks \u2014 like ",(0,i.jsx)(n.strong,{children:"machine translation"})," \u2014 where an ",(0,i.jsx)(n.strong,{children:"input sequence"})," (e.g., a sentence in English) is transformed into a ",(0,i.jsx)(n.strong,{children:"target sequence"})," (e.g., the same sentence in French). The input and output sequences can have different lengths."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-components",children:"\ud83e\udde9 Components"}),"\n",(0,i.jsx)(n.h3,{id:"-encoder",children:"\ud83d\udd39 Encoder:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A stack of ",(0,i.jsx)(n.strong,{children:"RNNs or LSTMs"})," that processes the input sequence token by token."]}),"\n",(0,i.jsxs)(n.li,{children:["At each timestep, the input token is passed through:","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedding Layer"})," \u2192 creates a dense representation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RNN cell (e.g., LSTM)"})," \u2192 processes the embedding and updates the hidden & cell states."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Only the final hidden (and cell) states"})," are retained."]}),"\n",(0,i.jsxs)(n.li,{children:["The encoder does ",(0,i.jsx)(n.strong,{children:"not output"})," a sequence \u2014 its job is to ",(0,i.jsx)(n.strong,{children:"encode"})," the input."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"-decoder",children:"\ud83d\udd39 Decoder:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Also a stack of ",(0,i.jsx)(n.strong,{children:"RNNs or LSTMs"}),"."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Starts from the encoder's final hidden and cell states."})}),"\n",(0,i.jsxs)(n.li,{children:["Takes in a start token and generates ",(0,i.jsx)(n.strong,{children:"one word at a time"})," using:","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Previous hidden/cell state"}),"\n",(0,i.jsxs)(n.li,{children:["Previous predicted token (or ground truth during training with ",(0,i.jsx)(n.strong,{children:"teacher forcing"}),")"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Each RNN step predicts one token."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"-teacher-forcing-training-technique",children:"\ud83d\udd39 Teacher Forcing (Training Technique):"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Feeds the actual target token (ground truth) as input for the next timestep instead of the model's previous prediction."}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Improves convergence"})," by preventing early-stage compounding errors."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"\ufe0f-pytorch-implementation-summary",children:"\ud83d\udee0\ufe0f PyTorch Implementation Summary"}),"\n",(0,i.jsx)(n.h3,{id:"\ufe0f-encoder-class",children:"\ud83c\udfd7\ufe0f Encoder Class:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Inherits from ",(0,i.jsx)(n.code,{children:"torch.nn.Module"})]}),"\n",(0,i.jsxs)(n.li,{children:["Layers:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"nn.Embedding"}),": Converts tokens to vectors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"nn.LSTM"}),": Processes sequences and returns ",(0,i.jsx)(n.code,{children:"(hidden_state, cell_state)"})]}),"\n",(0,i.jsxs)(n.li,{children:["Optional ",(0,i.jsx)(n.code,{children:"Dropout"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Returns:"})," Final ",(0,i.jsx)(n.code,{children:"hidden"})," and ",(0,i.jsx)(n.code,{children:"cell"})," states (not the full output sequence)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"\ufe0f-decoder-class",children:"\ud83c\udfd7\ufe0f Decoder Class:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Layers:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"nn.Embedding"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"nn.LSTM"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"nn.Linear"}),": Maps output to vocabulary size"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Softmax"}),": Converts to probability distribution"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["At each timestep, decoder generates ",(0,i.jsx)(n.strong,{children:"one token"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"-seq2seq-wrapper-class",children:"\ud83d\udd01 Seq2Seq Wrapper Class:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Combines Encoder & Decoder"}),"\n",(0,i.jsxs)(n.li,{children:["Handles ",(0,i.jsx)(n.strong,{children:"teacher forcing"})]}),"\n",(0,i.jsx)(n.li,{children:"Tracks batch size, output dimensions"}),"\n",(0,i.jsxs)(n.li,{children:["Loop over target sequence:","\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Predict next token"}),"\n",(0,i.jsx)(n.li,{children:"Use teacher forcing or argmax"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-recap",children:"\ud83d\udd0d Recap"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Component"}),(0,i.jsx)(n.th,{children:"Role"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Encoder"}),(0,i.jsx)(n.td,{children:"Processes input sequence, encodes into context vector (hidden states)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Decoder"}),(0,i.jsx)(n.td,{children:"Generates output sequence one token at a time using encoder context"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"LSTM"}),(0,i.jsx)(n.td,{children:"Adds memory capabilities to handle long sequences"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Teacher Forcing"}),(0,i.jsx)(n.td,{children:"Trains decoder with ground truth instead of its own past predictions"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-key-takeaways",children:"\u2705 Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Encoder-Decoder RNNs are foundational for NLP tasks like translation."}),"\n",(0,i.jsx)(n.li,{children:"Input/output lengths can vary."}),"\n",(0,i.jsx)(n.li,{children:"The hidden state carries the context from encoder to decoder."}),"\n",(0,i.jsx)(n.li,{children:"Teacher forcing improves training but is only used during training."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["This architecture is often replaced today by ",(0,i.jsx)(n.strong,{children:"Transformers"}),", but remains critical to understanding modern NLP."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}}}]);