"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[6366],{5466:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"topics/deep-learning/nlp/self-attention","title":"What Is Self-Attention?","description":"Self-attention allows a model to look at all the other words in a sentence (or a document, or code...) and decide how important each of them is for understanding a particular word.","source":"@site/docs/topics/deep-learning/nlp/self-attention.md","sourceDirName":"topics/deep-learning/nlp","slug":"/topics/deep-learning/nlp/self-attention","permalink":"/docs/topics/deep-learning/nlp/self-attention","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/nlp/self-attention.md","tags":[{"inline":true,"label":"transformers","permalink":"/docs/tags/transformers"},{"inline":true,"label":"nlp","permalink":"/docs/tags/nlp"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"self-attention","title":"What Is Self-Attention?","tags":["transformers","nlp","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"rag-introduction","permalink":"/docs/topics/deep-learning/nlp/rag-introduction"},"next":{"title":"Transformer Architecture ","permalink":"/docs/topics/deep-learning/nlp/transformer-architecture"}}');var i=s(4848),r=s(8453);const l={id:"self-attention",title:"What Is Self-Attention?",tags:["transformers","nlp","neural networks"]},a="\ud83d\udd0d What Is Self-Attention?",o={},c=[{value:"\ud83e\udde0 How It Works: Step-by-Step",id:"-how-it-works-step-by-step",level:2},{value:"1. <strong>Projection into Q, K, V (Query, Key, Value)</strong>",id:"1-projection-into-q-k-v-query-key-value",level:3},{value:"2. <strong>Attention Scores via Dot Product</strong>",id:"2-attention-scores-via-dot-product",level:3},{value:"\ud83d\udcd0 Full Formula",id:"-full-formula",level:3},{value:"\ud83d\udca1 Why Divide by \u221ad_k?",id:"-why-divide-by-d_k",level:2},{value:"\ud83e\uddea Simple Conceptual Example",id:"-simple-conceptual-example",level:2},{value:"\ud83c\udf00 Multi-Head Attention",id:"-multi-head-attention",level:2},{value:"\ud83e\udded Why Self-Attention Matters",id:"-why-self-attention-matters",level:2},{value:"\u23f1\ufe0f Positional Encoding",id:"\ufe0f-positional-encoding",level:2},{value:"\ud83e\uddea Mini Practicum: Predicting \u201chate\u201d from \u201cnot like\u201d",id:"-mini-practicum-predicting-hate-from-not-like",level:2},{value:"\ud83e\udd13 Summary",id:"-summary",level:2}];function d(e){const n={annotation:"annotation",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",math:"math",mi:"mi",mrow:"mrow",msub:"msub",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"-what-is-self-attention",children:"\ud83d\udd0d What Is Self-Attention?"})}),"\n",(0,i.jsxs)(n.p,{children:["Self-attention allows a model to look at ",(0,i.jsx)(n.strong,{children:"all the other words in a sentence"})," (or a document, or code...) and ",(0,i.jsx)(n.strong,{children:"decide how important each of them is"})," for understanding a particular word."]}),"\n",(0,i.jsx)(n.p,{children:"Imagine reading:"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:'"The dog chased the llama because it was fast."'}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You need to ask:",(0,i.jsx)(n.br,{}),"\n",'\ud83e\udde0 "Does ',(0,i.jsx)(n.em,{children:"it"})," refer to ",(0,i.jsx)(n.em,{children:"the dog"})," or ",(0,i.jsx)(n.em,{children:"the llama"}),'?"']}),"\n",(0,i.jsxs)(n.p,{children:["That\u2019s where ",(0,i.jsx)(n.strong,{children:"self-attention"})," comes in. It helps assign ",(0,i.jsx)(n.em,{children:"relevance scores"})," between tokens so the model can understand these dependencies."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-how-it-works-step-by-step",children:"\ud83e\udde0 How It Works: Step-by-Step"}),"\n",(0,i.jsxs)(n.h3,{id:"1-projection-into-q-k-v-query-key-value",children:["1. ",(0,i.jsx)(n.strong,{children:"Projection into Q, K, V (Query, Key, Value)"})]}),"\n",(0,i.jsx)(n.p,{children:"Every token in the input sequence is passed through three learned linear transformations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Query (Q):"})," What am I looking for?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Key (K):"})," What does each word offer?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Value (V):"})," What content do I retrieve if a match is found?"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Each token gets its own Q, K, and V vectors."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h3,{id:"2-attention-scores-via-dot-product",children:["2. ",(0,i.jsx)(n.strong,{children:"Attention Scores via Dot Product"})]}),"\n",(0,i.jsxs)(n.p,{children:["To compute how much ",(0,i.jsx)(n.em,{children:"attention"})," one word pays to others:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Compute dot product between ",(0,i.jsx)(n.strong,{children:"Q"})," of the current word and ",(0,i.jsx)(n.strong,{children:"K"})," of all words."]}),"\n",(0,i.jsxs)(n.li,{children:["Scale the result by the square root of the key dimension ",(0,i.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Expected group as argument to '\\sqrt' at end of input: \\sqrt",style:{color:"#cc0000"},children:"\\sqrt"}),"d_k$$."]}),"\n",(0,i.jsxs)(n.li,{children:["Apply ",(0,i.jsx)(n.strong,{children:"softmax"})," to turn scores into weights."]}),"\n",(0,i.jsxs)(n.li,{children:["Multiply weights with ",(0,i.jsx)(n.strong,{children:"V"})," to get the final contextual embedding."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"-full-formula",children:"\ud83d\udcd0 Full Formula"}),"\n",(0,i.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Can't use function '$' in math mode at position 54: \u2026max}\\left(\\frac$\u0332QK^T${\\sqrt$d_k\u2026",style:{color:"#cc0000"},children:"\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac$QK^T${\\sqrt$d_k$}\\right)V"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Expected group after '^' at position 17: \u2026 \\in \\mathbb{R}^\u0332",style:{color:"#cc0000"},children:"Q \\in \\mathbb{R}^"}),"n \\times d_k$$"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Expected group after '^' at position 17: \u2026 \\in \\mathbb{R}^\u0332",style:{color:"#cc0000"},children:"K \\in \\mathbb{R}^"}),"n \\times d_k$$"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Expected group after '^' at position 17: \u2026 \\in \\mathbb{R}^\u0332",style:{color:"#cc0000"},children:"V \\in \\mathbb{R}^"}),"n \\times d_v$$"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsx)(n.mi,{children:"n"})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"n"})]})})}),(0,i.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"n"})]})})]})," = sequence length, ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsxs)(n.msub,{children:[(0,i.jsx)(n.mi,{children:"d"}),(0,i.jsx)(n.mi,{children:"k"})]})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"d_k"})]})})}),(0,i.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.8444em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(n.span,{className:"mord",children:[(0,i.jsx)(n.span,{className:"mord mathnormal",children:"d"}),(0,i.jsx)(n.span,{className:"msupsub",children:(0,i.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(n.span,{className:"vlist-r",children:[(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.3361em"},children:(0,i.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(n.span,{className:"mord mathnormal mtight",style:{marginRight:"0.03148em"},children:"k"})})]})}),(0,i.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(n.span,{className:"vlist-r",children:(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(n.span,{})})})]})})]})]})})]})," = key/query dim, ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsxs)(n.msub,{children:[(0,i.jsx)(n.mi,{children:"d"}),(0,i.jsx)(n.mi,{children:"v"})]})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"d_v"})]})})}),(0,i.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.8444em",verticalAlign:"-0.15em"}}),(0,i.jsxs)(n.span,{className:"mord",children:[(0,i.jsx)(n.span,{className:"mord mathnormal",children:"d"}),(0,i.jsx)(n.span,{className:"msupsub",children:(0,i.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(n.span,{className:"vlist-r",children:[(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.1514em"},children:(0,i.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(n.span,{className:"mord mathnormal mtight",style:{marginRight:"0.03588em"},children:"v"})})]})}),(0,i.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(n.span,{className:"vlist-r",children:(0,i.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(n.span,{})})})]})})]})]})})]})," = value dim"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The result is a matrix of ",(0,i.jsx)(n.strong,{children:"contextualized embeddings"}),": one for each word, now aware of its neighbors."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-why-divide-by-d_k",children:"\ud83d\udca1 Why Divide by \u221ad_k?"}),"\n",(0,i.jsxs)(n.p,{children:["Without scaling, large dot product values would cause the ",(0,i.jsx)(n.strong,{children:"softmax"})," to become too peaky, resulting in near one-hot distributions.",(0,i.jsx)(n.br,{}),"\n","The division stabilizes gradients and ensures better learning."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-simple-conceptual-example",children:"\ud83e\uddea Simple Conceptual Example"}),"\n",(0,i.jsxs)(n.p,{children:["For a 3-token input:",(0,i.jsx)(n.br,{}),"\n","Tokens = ",(0,i.jsx)(n.code,{children:'"I"'}),", ",(0,i.jsx)(n.code,{children:'"love"'}),", ",(0,i.jsx)(n.code,{children:'"cats"'})]}),"\n",(0,i.jsxs)(n.p,{children:["Each token gets Q, K, V (vectors like ",(0,i.jsx)(n.code,{children:"[0.1, 0.3]"}),")"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Compute ",(0,i.jsx)(n.code,{children:"Q \xd7 K\u1d40"})," \u2192 similarity scores"]}),"\n",(0,i.jsxs)(n.li,{children:["Apply ",(0,i.jsx)(n.code,{children:"softmax"})," \u2192 attention weights (e.g., ",(0,i.jsx)(n.code,{children:"[0.2, 0.3, 0.5]"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"Multiply by V \u2192 weighted sum \u2192 contextual embedding for token"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-multi-head-attention",children:"\ud83c\udf00 Multi-Head Attention"}),"\n",(0,i.jsx)(n.p,{children:"Instead of just one set of Q/K/V projections, the model uses multiple \u201cheads\u201d:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Each head learns different relationships (e.g., grammar, meaning)"}),"\n",(0,i.jsxs)(n.li,{children:["The outputs of all heads are ",(0,i.jsx)(n.strong,{children:"concatenated and linearly transformed"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This lets the model capture different types of context simultaneously."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-why-self-attention-matters",children:"\ud83e\udded Why Self-Attention Matters"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\ud83d\udcac Understands ",(0,i.jsx)(n.strong,{children:"global relationships"}),", not just nearby words"]}),"\n",(0,i.jsxs)(n.li,{children:["\u26a1 Fully ",(0,i.jsx)(n.strong,{children:"parallelizable"})," (unlike RNNs)"]}),"\n",(0,i.jsx)(n.li,{children:"\ud83e\udde0 Easily scaled up (transformers can handle huge sequences)"}),"\n",(0,i.jsxs)(n.li,{children:["\ud83d\udd01 Enables ",(0,i.jsx)(n.strong,{children:"contextual embeddings"})," \u2014 words are represented ",(0,i.jsx)(n.em,{children:"in context"})]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"\ufe0f-positional-encoding",children:"\u23f1\ufe0f Positional Encoding"}),"\n",(0,i.jsxs)(n.p,{children:["Because attention alone is order-agnostic, we inject ",(0,i.jsx)(n.strong,{children:"position info"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\ud83d\udd01 ",(0,i.jsx)(n.strong,{children:"Sinusoidal positional encoding"})," (Transformer paper)"]}),"\n",(0,i.jsxs)(n.li,{children:["\ud83e\udde0 ",(0,i.jsx)(n.strong,{children:"Learnable positional embeddings"})," (BERT, GPT)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["These are ",(0,i.jsx)(n.strong,{children:"added to the token embeddings"})," before attention so the model knows word order."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-mini-practicum-predicting-hate-from-not-like",children:"\ud83e\uddea Mini Practicum: Predicting \u201chate\u201d from \u201cnot like\u201d"}),"\n",(0,i.jsxs)(n.p,{children:["Input: ",(0,i.jsx)(n.code,{children:'"not like"'}),(0,i.jsx)(n.br,{}),"\n","Model should predict: ",(0,i.jsx)(n.code,{children:'"hate"'})]}),"\n",(0,i.jsx)(n.p,{children:"Steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Embed tokens \u2192 ",(0,i.jsx)(n.code,{children:"[x_not, x_like]"})]}),"\n",(0,i.jsxs)(n.li,{children:["Compute ",(0,i.jsx)(n.code,{children:"Q = x \xd7 W_Q"}),", ",(0,i.jsx)(n.code,{children:"K = x \xd7 W_K"}),", ",(0,i.jsx)(n.code,{children:"V = x \xd7 W_V"})]}),"\n",(0,i.jsxs)(n.li,{children:["Get ",(0,i.jsx)(n.code,{children:"attention_weights = softmax(Q \xd7 K\u1d40 / \u221ad_k)"})]}),"\n",(0,i.jsxs)(n.li,{children:["Compute ",(0,i.jsx)(n.code,{children:"H' = attention_weights \xd7 V"})]}),"\n",(0,i.jsxs)(n.li,{children:["Final output: ",(0,i.jsx)(n.code,{children:"H = H' \xd7 W_o"})]}),"\n",(0,i.jsxs)(n.li,{children:["Pass through linear classifier \u2192 vocabulary \u2192 predict ",(0,i.jsx)(n.code,{children:'"hate"'})]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-summary",children:"\ud83e\udd13 Summary"}),"\n",(0,i.jsx)(n.p,{children:"Self-attention lets a model:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Compare all tokens with all others via dot product"}),"\n",(0,i.jsx)(n.li,{children:"Learn long-range dependencies"}),"\n",(0,i.jsxs)(n.li,{children:["Replace sequential memory with direct ",(0,i.jsx)(n.em,{children:"contextual lookup"})]}),"\n",(0,i.jsx)(n.li,{children:"Power the core of the Transformer architecture"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var t=s(6540);const i={},r=t.createContext(i);function l(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);