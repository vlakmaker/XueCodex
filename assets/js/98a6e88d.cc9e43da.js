"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[8974],{2925:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>d,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"topics/deep-learning/nlp/transformer-attention","title":"Transformers and Attention \u2013 Explained with Visuals","description":"---","source":"@site/docs/topics/deep-learning/nlp/transformers-attention.md","sourceDirName":"topics/deep-learning/nlp","slug":"/topics/deep-learning/nlp/transformer-attention","permalink":"/docs/topics/deep-learning/nlp/transformer-attention","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/deep-learning/nlp/transformers-attention.md","tags":[{"inline":true,"label":"transformers","permalink":"/docs/tags/transformers"},{"inline":true,"label":"nlp","permalink":"/docs/tags/nlp"},{"inline":true,"label":"neural networks","permalink":"/docs/tags/neural-networks"}],"version":"current","frontMatter":{"id":"transformer-attention","title":"Transformers and Attention \u2013 Explained with Visuals","tags":["transformers","nlp","neural networks"]},"sidebar":"tutorialSidebar","previous":{"title":"Transformer Architecture ","permalink":"/docs/topics/deep-learning/nlp/transformer-architecture"},"next":{"title":"Word Embeddings & Sequence Models in NLP","permalink":"/docs/topics/deep-learning/nlp/word-embeddings"}}');var s=t(4848),i=t(8453);const d={id:"transformer-attention",title:"Transformers and Attention \u2013 Explained with Visuals",tags:["transformers","nlp","neural networks"]},l="\ud83e\udd16 Transformers and Attention \u2013 Explained with Visuals",o={},a=[{value:"\ud83e\udde0 Why Were Transformers Created?",id:"-why-were-transformers-created",level:2},{value:"\ud83d\udd0d What Is Attention? (Recap)",id:"-what-is-attention-recap",level:2},{value:"\u2699\ufe0f Anatomy of a Transformer (Slide 1\u20132)",id:"\ufe0f-anatomy-of-a-transformer-slide-12",level:2},{value:"Encoder Side (Understanding Input)",id:"encoder-side-understanding-input",level:3},{value:"Decoder Side (Generating Output)",id:"decoder-side-generating-output",level:3},{value:"\ud83d\udd76\ufe0f What Is Masked Self-Attention? (Slide 2)",id:"\ufe0f-what-is-masked-self-attention-slide-2",level:2},{value:"\ud83e\uddf1 Encoder-Only vs Decoder-Only vs Encoder-Decoder (Slide 3\u20136)",id:"-encoder-only-vs-decoder-only-vs-encoder-decoder-slide-36",level:2},{value:"\ud83d\udfe2 <strong>Encoder-Only</strong> \u2192 BERT (Representation Model)",id:"-encoder-only--bert-representation-model",level:3},{value:"\ud83d\udd34 <strong>Decoder-Only</strong> \u2192 GPT (Generative Model)",id:"-decoder-only--gpt-generative-model",level:3},{value:"\ud83d\udd37 <strong>Encoder-Decoder</strong> \u2192 T5, BART, Translation models",id:"-encoder-decoder--t5-bart-translation-models",level:3},{value:"\ud83e\udd39 Summary Table",id:"-summary-table",level:2},{value:"\ud83d\udd25 Why Transformers Win",id:"-why-transformers-win",level:2}];function c(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"-transformers-and-attention--explained-with-visuals",children:"\ud83e\udd16 Transformers and Attention \u2013 Explained with Visuals"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-why-were-transformers-created",children:"\ud83e\udde0 Why Were Transformers Created?"}),"\n",(0,s.jsxs)(n.p,{children:["Before transformers, sequence models like ",(0,s.jsx)(n.strong,{children:"RNNs"})," and ",(0,s.jsx)(n.strong,{children:"LSTMs"})," were used to process language. But they had limitations:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\ud83d\udc0c ",(0,s.jsx)(n.strong,{children:"Slow"}),": RNNs process one word at a time (no parallelism)"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83e\udde0 ",(0,s.jsx)(n.strong,{children:"Memory bottlenecks"}),": They struggle with long-range dependencies"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83c\udfd7\ufe0f ",(0,s.jsx)(n.strong,{children:"Hard to scale"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Then came the ",(0,s.jsx)(n.strong,{children:"Transformer"})," architecture (Vaswani et al., 2017), with its famous tagline:"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'"Attention is All You Need"'})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Instead of processing tokens sequentially, transformers use ",(0,s.jsx)(n.strong,{children:"self-attention"})," to process the entire input at once, capturing dependencies across the sequence."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-what-is-attention-recap",children:"\ud83d\udd0d What Is Attention? (Recap)"}),"\n",(0,s.jsx)(n.p,{children:"Attention is the idea that:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Some parts of the input are more relevant to each output than others."})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Instead of compressing everything into a single fixed vector, attention lets each word \u201clook around\u201d at other words and weigh them."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["\u201cThe cat sat on the ",(0,s.jsx)(n.strong,{children:"mat"}),"\u201d \u2192 to understand \u201csat,\u201d the model attends to \u201ccat.\u201d"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-anatomy-of-a-transformer-slide-12",children:"\u2699\ufe0f Anatomy of a Transformer (Slide 1\u20132)"}),"\n",(0,s.jsx)(n.h3,{id:"encoder-side-understanding-input",children:"Encoder Side (Understanding Input)"}),"\n",(0,s.jsxs)(n.p,{children:["Input: ",(0,s.jsx)(n.code,{children:'"I love llamas"'})]}),"\n",(0,s.jsxs)(n.p,{children:["Each word is embedded and passed into multiple ",(0,s.jsx)(n.strong,{children:"encoder layers"}),", each with:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\ud83d\udd0d ",(0,s.jsx)(n.strong,{children:"Self-Attention"})," \u2013 each word attends to all other words in the sentence"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2699\ufe0f ",(0,s.jsx)(n.strong,{children:"Feedforward Network"})," \u2013 learns complex representations"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This creates ",(0,s.jsx)(n.strong,{children:"contextualized embeddings"}),": \u201clove\u201d knows it relates to \u201cI\u201d and \u201cllamas.\u201d"]}),"\n",(0,s.jsx)(n.h3,{id:"decoder-side-generating-output",children:"Decoder Side (Generating Output)"}),"\n",(0,s.jsx)(n.p,{children:"The decoder takes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Previously generated words \u2192 e.g. \u201cIk\u201d, \u201chou\u201d (Dutch for "I love")'}),"\n",(0,s.jsx)(n.li,{children:"Plus encoded context (from the encoder)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"And generates the next word \u2014 \u201cvan.\u201d"}),"\n",(0,s.jsx)(n.p,{children:"The decoder includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\ud83d\udd76\ufe0f ",(0,s.jsx)(n.strong,{children:"Masked Self-Attention"})," (explained next)"]}),"\n",(0,s.jsxs)(n.li,{children:["\ud83d\udd04 ",(0,s.jsx)(n.strong,{children:"Encoder-Decoder Attention"})," (uses encoder output)"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2699\ufe0f ",(0,s.jsx)(n.strong,{children:"Feedforward NN"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-what-is-masked-self-attention-slide-2",children:"\ud83d\udd76\ufe0f What Is Masked Self-Attention? (Slide 2)"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"You don\u2019t want a model to \u201ccheat\u201d by looking at future words during generation."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Masked self-attention"})," prevents the model from peeking ahead. When generating the third word, it only sees the first two:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Generating Word"}),(0,s.jsx)(n.th,{children:"Can Attend To"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1st"}),(0,s.jsx)(n.td,{children:"[Start token]"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2nd"}),(0,s.jsx)(n.td,{children:"[1st]"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3rd"}),(0,s.jsx)(n.td,{children:"[1st, 2nd]"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["It\u2019s how ",(0,s.jsx)(n.strong,{children:"autoregressive"})," models like GPT maintain left-to-right generation."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-encoder-only-vs-decoder-only-vs-encoder-decoder-slide-36",children:"\ud83e\uddf1 Encoder-Only vs Decoder-Only vs Encoder-Decoder (Slide 3\u20136)"}),"\n",(0,s.jsxs)(n.h3,{id:"-encoder-only--bert-representation-model",children:["\ud83d\udfe2 ",(0,s.jsx)(n.strong,{children:"Encoder-Only"})," \u2192 BERT (Representation Model)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Task: ",(0,s.jsx)(n.strong,{children:"Understand"})," text (classification, QA, embeddings)"]}),"\n",(0,s.jsxs)(n.li,{children:["Uses ",(0,s.jsx)(n.strong,{children:"bidirectional self-attention"})," (sees left + right context)"]}),"\n",(0,s.jsxs)(n.li,{children:["Trained with ",(0,s.jsx)(n.strong,{children:"masked language modeling"})," (Slide 4):","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Randomly mask a word"}),"\n",(0,s.jsx)(n.li,{children:'Predict the missing word (e.g., "I [MASK] llamas" \u2192 "am")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fine-tuned"})," for downstream tasks (Slide 5):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Sentiment classification"}),"\n",(0,s.jsx)(n.li,{children:"Named entity recognition"}),"\n",(0,s.jsx)(n.li,{children:"Sentence similarity"}),"\n"]}),"\n",(0,s.jsxs)(n.h3,{id:"-decoder-only--gpt-generative-model",children:["\ud83d\udd34 ",(0,s.jsx)(n.strong,{children:"Decoder-Only"})," \u2192 GPT (Generative Model)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Task: ",(0,s.jsx)(n.strong,{children:"Generate"})," text (completion, stories, chat)"]}),"\n",(0,s.jsxs)(n.li,{children:["Uses ",(0,s.jsx)(n.strong,{children:"masked self-attention"})," only (left-to-right)"]}),"\n",(0,s.jsx)(n.li,{children:"No encoder at all"}),"\n"]}),"\n",(0,s.jsxs)(n.h3,{id:"-encoder-decoder--t5-bart-translation-models",children:["\ud83d\udd37 ",(0,s.jsx)(n.strong,{children:"Encoder-Decoder"})," \u2192 T5, BART, Translation models"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Input goes through the encoder"}),"\n",(0,s.jsx)(n.li,{children:"Decoder generates outputs using attention over the encoder"}),"\n",(0,s.jsxs)(n.li,{children:["Used for: ",(0,s.jsx)(n.strong,{children:"translation, summarization, question answering"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-summary-table",children:"\ud83e\udd39 Summary Table"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model Type"}),(0,s.jsx)(n.th,{children:"Uses Encoder"}),(0,s.jsx)(n.th,{children:"Uses Decoder"}),(0,s.jsx)(n.th,{children:"Attention Type"}),(0,s.jsx)(n.th,{children:"Example Models"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Encoder-Only"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"}),(0,s.jsx)(n.td,{children:"\u274c No"}),(0,s.jsx)(n.td,{children:"Bidirectional Self-Attention"}),(0,s.jsx)(n.td,{children:"BERT, RoBERTa"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Decoder-Only"}),(0,s.jsx)(n.td,{children:"\u274c No"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"}),(0,s.jsx)(n.td,{children:"Masked Self-Attention"}),(0,s.jsx)(n.td,{children:"GPT, GPT-2/3/4"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Encoder-Decoder"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"}),(0,s.jsx)(n.td,{children:"Self-Attn + Encoder-Dec Attn"}),(0,s.jsx)(n.td,{children:"T5, BART, MT5"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-why-transformers-win",children:"\ud83d\udd25 Why Transformers Win"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Parallelizable (not step-by-step like RNNs)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Scales well with data and compute"}),"\n",(0,s.jsxs)(n.li,{children:["\u2705 Captures ",(0,s.jsx)(n.strong,{children:"global context"})," with attention"]}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Foundation of LLMs like GPT, BERT, Claude, Gemini, etc."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>l});var r=t(6540);const s={},i=r.createContext(s);function d(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);