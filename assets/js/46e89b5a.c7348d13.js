"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[4349],{8917:e=>{e.exports=JSON.parse('{"tag":{"label":"neural-networks","permalink":"/docs/tags/neural-networks","allTagsPath":"/docs/tags","count":16,"items":[{"id":"topics/deep-learning/nlp/attentoin-mechanism","title":"Attention Mechanism \u2014 A Gentle but Deep Dive","description":"1\ufe0f\u20e3 What Is the Attention Mechanism and Why Is It Important?","permalink":"/docs/topics/deep-learning/nlp/attentoin-mechanism"},{"id":"topics/deep-learning/nlp/convert-words-features","title":"Converting Words to Features in NLP","description":"\ud83d\udfe2 1. One-Hot Encoding","permalink":"/docs/topics/deep-learning/nlp/convert-words-features"},{"id":"topics/deep-learning/training/document-categorization-prediction","title":"Converting Words to Features in NLP","description":"---","permalink":"/docs/topics/deep-learning/training/document-categorization-prediction"},{"id":"topics/deep-learning/nlp/encoder-decoder","title":"Encoders, Decoders, and Attention","description":"---","permalink":"/docs/topics/deep-learning/nlp/encoder-decoder"},{"id":"topics/deep-learning/nlp/language-ai-tasks","title":"Language AI Tasks (NLP Task Overview)","description":"1. \ud83e\udde0 Understanding (Analysis)","permalink":"/docs/topics/deep-learning/nlp/language-ai-tasks"},{"id":"topics/deep-learning/nlp/bag-of-words","title":"Language as a Bag-of-Words","description":"Conceptual Walkthrough with \u201cMy cat is cute\u201d","permalink":"/docs/topics/deep-learning/nlp/bag-of-words"},{"id":"topics/deep-learning/sequence-models/llm-ngam-nn","title":"Language Modeling with N-Grams and Neural Networks","description":"\ud83e\udde0 Overview","permalink":"/docs/topics/deep-learning/sequence-models/llm-ngam-nn"},{"id":"topics/deep-learning/nlp/Multi-head attention","title":"Multihead Attention","description":"\ud83e\udde0 1. What Is Scaled Dot-Product Attention?","permalink":"/docs/topics/deep-learning/nlp/Multi-head attention"},{"id":"topics/deep-learning/neural-networks/neural-network-forward-pass","title":"Neural Network Forward Pass","description":"\ud83d\udd01 Quick Recap: What is a Neural Network?","permalink":"/docs/topics/deep-learning/neural-networks/neural-network-forward-pass"},{"id":"topics/deep-learning/training/training-document-classifier","title":"Training a Document Classifier","description":"Training a Document Classifier","permalink":"/docs/topics/deep-learning/training/training-document-classifier"},{"id":"topics/deep-learning/nlp/transformer-architecture","title":"Transformer Architecture ","description":"---","permalink":"/docs/topics/deep-learning/nlp/transformer-architecture"},{"id":"topics/deep-learning/nlp/transformer-attention","title":"Transformers and Attention \u2013 Explained with Visuals","description":"---","permalink":"/docs/topics/deep-learning/nlp/transformer-attention"},{"id":"topics/deep-learning/nlp/embeddings","title":"What Are Embeddings","description":"Embeddings are how we turn symbolic data (like words, sentences, images, or even users) into vectors of numbers \u2014 in a way that captures meaning, similarity, or structure.","permalink":"/docs/topics/deep-learning/nlp/embeddings"},{"id":"topics/deep-learning/neural-networks/neural-networks","title":"What are Neural Networks?","description":"Understanding Neural Networks, Weights, and Stochastic Gradient Descent","permalink":"/docs/topics/deep-learning/neural-networks/neural-networks"},{"id":"topics/deep-learning/nlp/self-attention","title":"What Is Self-Attention?","description":"Self-attention allows a model to look at all the other words in a sentence (or a document, or code...) and decide how important each of them is for understanding a particular word.","permalink":"/docs/topics/deep-learning/nlp/self-attention"},{"id":"topics/deep-learning/nlp/word-embeddings","title":"Word Embeddings & Sequence Models in NLP","description":"Word2Vec Overview","permalink":"/docs/topics/deep-learning/nlp/word-embeddings"}],"unlisted":false}}')}}]);