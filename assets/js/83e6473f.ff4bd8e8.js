"use strict";(self.webpackChunkxuecodex=self.webpackChunkxuecodex||[]).push([[7589],{2021:(e,i,s)=>{s.r(i),s.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"topics/machine-learning/logistic-regression-derivatives","title":"Logistic Regression Derivatives (Simple Explanation)","description":"This page explains how to compute derivatives in logistic regression using a computation graph, broken down into easy-to-understand steps.","source":"@site/docs/topics/machine-learning/logistic-regression-derivatives.md","sourceDirName":"topics/machine-learning","slug":"/topics/machine-learning/logistic-regression-derivatives","permalink":"/XueCodex/docs/topics/machine-learning/logistic-regression-derivatives","draft":false,"unlisted":false,"editUrl":"https://github.com/vlakmaker/XueCodex/tree/main/docs/topics/machine-learning/logistic-regression-derivatives.md","tags":[{"inline":true,"label":"machine-learning","permalink":"/XueCodex/docs/tags/machine-learning"},{"inline":true,"label":"logistic regression","permalink":"/XueCodex/docs/tags/logistic-regression"},{"inline":true,"label":"derivatives","permalink":"/XueCodex/docs/tags/derivatives"}],"version":"current","frontMatter":{"id":"logistic-regression-derivatives","title":"Logistic Regression Derivatives (Simple Explanation)","tags":["machine-learning","logistic regression","derivatives"]},"sidebar":"tutorialSidebar","previous":{"title":"Logistic Regression Cost Function","permalink":"/XueCodex/docs/topics/machine-learning/logistic-regression-cost-function"},"next":{"title":"Logistic Regression","permalink":"/XueCodex/docs/topics/machine-learning/logistic-regression"}}');var r=s(4848),n=s(8453);const a={id:"logistic-regression-derivatives",title:"Logistic Regression Derivatives (Simple Explanation)",tags:["machine-learning","logistic regression","derivatives"]},o="Logistic Regression Derivatives (Simple Explanation)",l={},c=[{value:"\ud83e\uddec Goal",id:"-goal",level:2},{value:"\ud83d\udd04 Forward Pass (Prediction)",id:"-forward-pass-prediction",level:2},{value:"1. Compute the score <code>z</code>:",id:"1-compute-the-score-z",level:3},{value:"2. Apply the sigmoid function:",id:"2-apply-the-sigmoid-function",level:3},{value:"3. Compute the loss:",id:"3-compute-the-loss",level:3},{value:"\ud83d\udd04 Backward Pass (Learning)",id:"-backward-pass-learning",level:2},{value:"Step 1: From Loss to Sigmoid Output",id:"step-1-from-loss-to-sigmoid-output",level:3},{value:"Step 2: From Sigmoid Output to z",id:"step-2-from-sigmoid-output-to-z",level:3},{value:"Step 3: From z to Weights and Bias",id:"step-3-from-z-to-weights-and-bias",level:3},{value:"\ud83c\udf93 Gradient Descent Update",id:"-gradient-descent-update",level:2},{value:"\u2705 Summary",id:"-summary",level:2}];function d(e){const i={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"logistic-regression-derivatives-simple-explanation",children:"Logistic Regression Derivatives (Simple Explanation)"})}),"\n",(0,r.jsx)(i.p,{children:"This page explains how to compute derivatives in logistic regression using a computation graph, broken down into easy-to-understand steps."}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"-goal",children:"\ud83e\uddec Goal"}),"\n",(0,r.jsxs)(i.p,{children:["We want to find out how to ",(0,r.jsx)(i.strong,{children:"adjust the weights and bias"})," in a logistic regression model so it gets better at making predictions. We use something called ",(0,r.jsx)(i.strong,{children:"derivatives"})," (slopes) to guide the learning process."]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"-forward-pass-prediction",children:"\ud83d\udd04 Forward Pass (Prediction)"}),"\n",(0,r.jsxs)(i.h3,{id:"1-compute-the-score-z",children:["1. Compute the score ",(0,r.jsx)(i.code,{children:"z"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"z = w_1 x_1 + w_2 x_2 + b\n"})}),"\n",(0,r.jsxs)(i.p,{children:["This is a ",(0,r.jsx)(i.strong,{children:"weighted sum"})," of the inputs."]}),"\n",(0,r.jsx)(i.h3,{id:"2-apply-the-sigmoid-function",children:"2. Apply the sigmoid function:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"a = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n"})}),"\n",(0,r.jsxs)(i.p,{children:["This turns the score into a probability ",(0,r.jsx)(i.code,{children:"a"})," (e.g., how likely is this email to be spam?)."]}),"\n",(0,r.jsx)(i.h3,{id:"3-compute-the-loss",children:"3. Compute the loss:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"\\mathcal{L}(a, y)\n"})}),"\n",(0,r.jsxs)(i.p,{children:["This tells us ",(0,r.jsx)(i.strong,{children:"how wrong"})," our prediction ",(0,r.jsx)(i.code,{children:"a"})," is, compared to the true label ",(0,r.jsx)(i.code,{children:"y"}),"."]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"-backward-pass-learning",children:"\ud83d\udd04 Backward Pass (Learning)"}),"\n",(0,r.jsxs)(i.p,{children:["We now work ",(0,r.jsx)(i.strong,{children:"backwards"})," to calculate how much each part contributed to the error."]}),"\n",(0,r.jsx)(i.h3,{id:"step-1-from-loss-to-sigmoid-output",children:"Step 1: From Loss to Sigmoid Output"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"\\frac{\\partial \\mathcal{L}}{\\partial a} = -\\frac{y}{a} + \\frac{1 - y}{1 - a}\n"})}),"\n",(0,r.jsxs)(i.p,{children:["This tells us how much the loss changes when ",(0,r.jsx)(i.code,{children:"a"})," changes."]}),"\n",(0,r.jsx)(i.h3,{id:"step-2-from-sigmoid-output-to-z",children:"Step 2: From Sigmoid Output to z"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"\\frac{\\partial a}{\\partial z} = a(1 - a)\n"})}),"\n",(0,r.jsx)(i.p,{children:"This is the derivative of the sigmoid function. So:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"\\frac{\\partial \\mathcal{L}}{\\partial z} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z}\n"})}),"\n",(0,r.jsx)(i.h3,{id:"step-3-from-z-to-weights-and-bias",children:"Step 3: From z to Weights and Bias"}),"\n",(0,r.jsxs)(i.p,{children:["Since ",(0,r.jsx)(i.code,{children:"z = w_1 x_1 + w_2 x_2 + b"}),", we have:"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"\\frac{\\partial z}{\\partial w_1} = x_1\\qquad\n\\frac{\\partial z}{\\partial w_2} = x_2\\qquad\n\\frac{\\partial z}{\\partial b} = 1\n"})}),"\n",(0,r.jsx)(i.p,{children:"So:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"\\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{\\partial \\mathcal{L}}{\\partial z} \\cdot x_1\n\\frac{\\partial \\mathcal{L}}{\\partial w_2} = \\frac{\\partial \\mathcal{L}}{\\partial z} \\cdot x_2\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial z}\n"})}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"-gradient-descent-update",children:"\ud83c\udf93 Gradient Descent Update"}),"\n",(0,r.jsx)(i.p,{children:"We update our weights and bias using these gradients:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"w_1 := w_1 - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w_1}\n"})}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"w_2 := w_2 - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w_2}\n"})}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"b := b - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\n"})}),"\n",(0,r.jsxs)(i.p,{children:["Where ",(0,r.jsx)(i.code,{children:"\\alpha"})," is the ",(0,r.jsx)(i.strong,{children:"learning rate"})," (controls the step size)."]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"-summary",children:"\u2705 Summary"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Forward pass"}),": Make a prediction using weights and sigmoid."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Loss"}),": Measure how wrong the prediction was."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Backward pass"}),": Compute how much each weight and bias contributed to the error."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Update"}),": Adjust weights and bias to reduce future errors."]}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["This is the heart of training a logistic regression model using ",(0,r.jsx)(i.strong,{children:"derivatives"})," and ",(0,r.jsx)(i.strong,{children:"gradient descent"}),"."]})]})}function h(e={}){const{wrapper:i}={...(0,n.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,i,s)=>{s.d(i,{R:()=>a,x:()=>o});var t=s(6540);const r={},n=t.createContext(r);function a(e){const i=t.useContext(n);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(n.Provider,{value:i},e.children)}}}]);