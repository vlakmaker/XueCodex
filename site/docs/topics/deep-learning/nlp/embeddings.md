---
id: "embeddings"
title: What Are Embeddings
tags: [embeddings, word2vec, neural networks]
---
# ğŸ§  What Are Embeddings?

**Embeddings** are how we turn symbolic data (like words, sentences, images, or even users) into **vectors of numbers** â€” in a way that captures **meaning, similarity, or structure**.

> Think of embeddings as giving your data a GPS coordinate in meaning-space.
> 

Theyâ€™re the bridge between raw input and neural network understanding.

---

### ğŸ”¢ Why Not Just Use Words Directly?

Computers canâ€™t understand raw text like â€œdragonâ€ or â€œhonor.â€ We need to turn them into numbers. One way is one-hot encoding:

```
ini
CopyEdit
dragon = [0, 0, 0, 1, 0, 0, 0]
honor  = [0, 1, 0, 0, 0, 0, 0]

```

But that doesnâ€™t say anything about how **similar** "dragon" and "honor" are â€” theyâ€™re just orthogonal blobs.

---

## ğŸ’« Embeddings Fix That

An **embedding** maps each item (like a word) to a **dense vector of real numbers**. The vector is learned by a model and captures **semantic or structural relationships**.

> So instead of random position, "dragon" and "honor" are placed closer in vector space if they appear in similar contexts.
> 

---

## ğŸ§­ Embeddings In Practice

| Domain | Embedding Example |
| --- | --- |
| NLP | Words â†’ vectors (Word2Vec, GloVe, BERT) |
| Images | Images â†’ vectors (ResNet, CLIP) |
| Recommenders | Users & items â†’ embedding vectors |
| Graphs | Nodes â†’ embeddings (e.g., Node2Vec, GNNs) |

These vectors are often used to:

- Compare similarity (cosine distance, dot product)
- Feed into deeper neural layers
- Store in vector databases for fast search (e.g., RAG)

---

## ğŸ§ª Example: Word Embeddings

| Word | Embedding Vector (truncated) |
| --- | --- |
| king | [0.61, -0.12, 0.43, ..., 0.09] |
| queen | [0.59, -0.11, 0.45, ..., 0.10] |
| banana | [-0.28, 0.77, 0.13, ..., -0.09] |

You can see:

- `king` and `queen` are **close**
- `banana` is somewhere else entirely

---

## ğŸ” TL;DR â€“ What Are Embeddings?

- A **vector representation** of some discrete input (like a word)
- Learned by a model to **capture relationships**
- Useful for search, similarity, classification, and as input to neural networks