---
id: "self-attention"
title: "What Is Self-Attention?"
tags: [transformers, nlp, neural networks]
---
# ğŸ” What Is Self-Attention?

Self-attention allows a model to look at **all the other words in a sentence** (or a document, or code...) and **decide how important each of them is** for understanding a particular word.

Imagine reading:

> "The dog chased the llama because it was fast."

You need to ask:  
ğŸ§  "Does *it* refer to *the dog* or *the llama*?"  

Thatâ€™s where **self-attention** comes in. It helps assign *relevance scores* between tokens so the model can understand these dependencies.

---

## ğŸ§  How It Works: Step-by-Step

### 1. **Projection into Q, K, V (Query, Key, Value)**

Every token in the input sequence is passed through three learned linear transformations:

- **Query (Q):** What am I looking for?
- **Key (K):** What does each word offer?
- **Value (V):** What content do I retrieve if a match is found?

Each token gets its own Q, K, and V vectors.

---

### 2. **Attention Scores via Dot Product**

To compute how much *attention* one word pays to others:

1. Compute dot product between **Q** of the current word and **K** of all words.
2. Scale the result by the square root of the key dimension $\sqrt{d_k}$.
3. Apply **softmax** to turn scores into weights.
4. Multiply weights with **V** to get the final contextual embedding.

---

### ğŸ“ Full Formula

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

- $Q \in \mathbb{R}^{n \times d_k}$
- $K \in \mathbb{R}^{n \times d_k}$
- $V \in \mathbb{R}^{n \times d_v}$
- $n$ = sequence length, $d_k$ = key/query dim, $d_v$ = value dim

The result is a matrix of **contextualized embeddings**: one for each word, now aware of its neighbors.

---

## ğŸ’¡ Why Divide by âˆšd_k?

Without scaling, large dot product values would cause the **softmax** to become too peaky, resulting in near one-hot distributions.  
The division stabilizes gradients and ensures better learning.

---

## ğŸ§ª Simple Conceptual Example

For a 3-token input:  
Tokens = `"I"`, `"love"`, `"cats"`

Each token gets Q, K, V (vectors like `[0.1, 0.3]`)

1. Compute `Q Ã— Káµ€` â†’ similarity scores
2. Apply `softmax` â†’ attention weights (e.g., `[0.2, 0.3, 0.5]`)
3. Multiply by V â†’ weighted sum â†’ contextual embedding for token

---

## ğŸŒ€ Multi-Head Attention

Instead of just one set of Q/K/V projections, the model uses multiple â€œheadsâ€:

- Each head learns different relationships (e.g., grammar, meaning)
- The outputs of all heads are **concatenated and linearly transformed**

This lets the model capture different types of context simultaneously.

---

## ğŸ§­ Why Self-Attention Matters

- ğŸ’¬ Understands **global relationships**, not just nearby words
- âš¡ Fully **parallelizable** (unlike RNNs)
- ğŸ§  Easily scaled up (transformers can handle huge sequences)
- ğŸ” Enables **contextual embeddings** â€” words are represented *in context*

---

## â±ï¸ Positional Encoding

Because attention alone is order-agnostic, we inject **position info**:

- ğŸ” **Sinusoidal positional encoding** (Transformer paper)
- ğŸ§  **Learnable positional embeddings** (BERT, GPT)

These are **added to the token embeddings** before attention so the model knows word order.

---

## ğŸ§ª Mini Practicum: Predicting â€œhateâ€ from â€œnot likeâ€

Input: `"not like"`  
Model should predict: `"hate"`

Steps:
1. Embed tokens â†’ `[x_not, x_like]`
2. Compute `Q = x Ã— W_Q`, `K = x Ã— W_K`, `V = x Ã— W_V`
3. Get `attention_weights = softmax(Q Ã— Káµ€ / âˆšd_k)`
4. Compute `H' = attention_weights Ã— V`
5. Final output: `H = H' Ã— W_o`
6. Pass through linear classifier â†’ vocabulary â†’ predict `"hate"`

---

## ğŸ¤“ Summary

Self-attention lets a model:
- Compare all tokens with all others via dot product
- Learn long-range dependencies
- Replace sequential memory with direct *contextual lookup*
- Power the core of the Transformer architecture
