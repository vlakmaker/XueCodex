---
id: "transformer-attention"
title: "Transformers and Attention â€“ Explained with Visuals"
tags: [transformers, nlp, neural networks]
---
# ğŸ¤– Transformers and Attention â€“ Explained with Visuals

---

## ğŸ§  Why Were Transformers Created?

Before transformers, sequence models like **RNNs** and **LSTMs** were used to process language. But they had limitations:

- ğŸŒ **Slow**: RNNs process one word at a time (no parallelism)
- ğŸ§  **Memory bottlenecks**: They struggle with long-range dependencies
- ğŸ—ï¸ **Hard to scale**

Then came the **Transformer** architecture (Vaswani et al., 2017), with its famous tagline:

> **"Attention is All You Need"**

Instead of processing tokens sequentially, transformers use **self-attention** to process the entire input at once, capturing dependencies across the sequence.

---

## ğŸ” What Is Attention? (Recap)

Attention is the idea that:
> **Some parts of the input are more relevant to each output than others.**

Instead of compressing everything into a single fixed vector, attention lets each word â€œlook aroundâ€ at other words and weigh them.

> â€œThe cat sat on the **mat**â€ â†’ to understand â€œsat,â€ the model attends to â€œcat.â€

---

## âš™ï¸ Anatomy of a Transformer (Slide 1â€“2)

### Encoder Side (Understanding Input)
Input: `"I love llamas"`

Each word is embedded and passed into multiple **encoder layers**, each with:
- ğŸ” **Self-Attention** â€“ each word attends to all other words in the sentence
- âš™ï¸ **Feedforward Network** â€“ learns complex representations

This creates **contextualized embeddings**: â€œloveâ€ knows it relates to â€œIâ€ and â€œllamas.â€

### Decoder Side (Generating Output)
The decoder takes:
- Previously generated words â†’ e.g. â€œIkâ€, â€œhouâ€ (Dutch for "I love")
- Plus encoded context (from the encoder)

And generates the next word â€” â€œvan.â€

The decoder includes:
- ğŸ•¶ï¸ **Masked Self-Attention** (explained next)
- ğŸ”„ **Encoder-Decoder Attention** (uses encoder output)
- âš™ï¸ **Feedforward NN**

---

## ğŸ•¶ï¸ What Is Masked Self-Attention? (Slide 2)

> You donâ€™t want a model to â€œcheatâ€ by looking at future words during generation.

**Masked self-attention** prevents the model from peeking ahead. When generating the third word, it only sees the first two:

| Generating Word | Can Attend To |
|------------------|------------------|
| 1st              | [Start token]    |
| 2nd              | [1st]            |
| 3rd              | [1st, 2nd]       |

Itâ€™s how **autoregressive** models like GPT maintain left-to-right generation.

---

## ğŸ§± Encoder-Only vs Decoder-Only vs Encoder-Decoder (Slide 3â€“6)

### ğŸŸ¢ **Encoder-Only** â†’ BERT (Representation Model)
- Task: **Understand** text (classification, QA, embeddings)
- Uses **bidirectional self-attention** (sees left + right context)
- Trained with **masked language modeling** (Slide 4):
  - Randomly mask a word
  - Predict the missing word (e.g., "I [MASK] llamas" â†’ "am")

**Fine-tuned** for downstream tasks (Slide 5):
- Sentiment classification
- Named entity recognition
- Sentence similarity

### ğŸ”´ **Decoder-Only** â†’ GPT (Generative Model)
- Task: **Generate** text (completion, stories, chat)
- Uses **masked self-attention** only (left-to-right)
- No encoder at all

### ğŸ”· **Encoder-Decoder** â†’ T5, BART, Translation models
- Input goes through the encoder
- Decoder generates outputs using attention over the encoder
- Used for: **translation, summarization, question answering**

---

## ğŸ¤¹ Summary Table

| Model Type       | Uses Encoder | Uses Decoder | Attention Type              | Example Models |
|------------------|--------------|--------------|------------------------------|----------------|
| Encoder-Only     | âœ… Yes       | âŒ No        | Bidirectional Self-Attention | BERT, RoBERTa  |
| Decoder-Only     | âŒ No        | âœ… Yes       | Masked Self-Attention        | GPT, GPT-2/3/4 |
| Encoder-Decoder  | âœ… Yes       | âœ… Yes       | Self-Attn + Encoder-Dec Attn | T5, BART, MT5  |

---

## ğŸ”¥ Why Transformers Win

- âœ… Parallelizable (not step-by-step like RNNs)
- âœ… Scales well with data and compute
- âœ… Captures **global context** with attention
- âœ… Foundation of LLMs like GPT, BERT, Claude, Gemini, etc.
