---
id: "training-document-classifier"
title: "Training a Document Classifier"
tags: [deep learning, neural networks, document classifier]
---
## **Training a Document Classifier**

---

### ğŸ§  **1. Neural Networks Learn Through Parameters (Î¸)**

- A **neural network** is just a stack of mathematical operations using *parameters* (called **Î¸**, theta).
- These parameters = weights that are learned and adjusted during training.
- The goal is to tweak Î¸ so your predictions (Å·) get closer to the actual labels (y).

---

### ğŸ“‰ **2. What Is a Loss Function? (Hint: It Measures Mistakes)**

- A **loss function** measures how far off the model is from the correct answer.
- Think of it like this:
    
    â†’ **High loss** = bad predictions ğŸ˜–
    
    â†’ **Low loss** = model doing well ğŸ˜
    
- We donâ€™t manually teach the model what's wrong â€” the **loss function** tells it where it messed up.

---

### ğŸ¯ **3. Enter Cross-Entropy Loss**

- Used for **classification tasks**, especially when you want the model to pick between multiple categories.
- Based on comparing:
    - **True distribution (y):** The correct class (e.g., â€œsportsâ€)
    - **Predicted distribution (Å·):** The probabilities the model assigns to each class after **softmax**

### ğŸ”¸ How it works:

1. Your model spits out **logits** (raw scores for each class).
2. Apply **softmax**: turns logits into a probability distribution (all values between 0â€“1 and sum to 1).
3. Cross-entropy loss measures how well your predicted distribution matches the correct class.
4. It punishes confident wrong answers more than unsure ones.

### ğŸ“Œ Formula-wise (simplified):

```
pgsql
CopyEdit
Cross-Entropy = -log(P(correct class))

```

If the model is 90% sure the answer is correct: low loss.

If itâ€™s 10% sure or confident in the wrong class: high loss.

---

### ğŸ“š **4. Monte Carlo Sampling**

- Fancy phrase for: **"When we donâ€™t know the full distribution, just average over examples."**
- Itâ€™s how we approximate the â€œtrueâ€ loss across a batch of training samples.

---

### ğŸ› ï¸ **5. Optimization: How the Model Learns**

The way we **minimize the loss** is through:

### ğŸ” **Gradient Descent**

- Iteratively update parameters to reduce loss:

```python
python
CopyEdit
Î¸ â† Î¸ - Î· * âˆ‡Loss

```

- Î¸ = current weights
- Î· = learning rate (how big a step to take)
- âˆ‡Loss = gradient (slope of the loss function)

### âœ… Steps in Practice:

1. **Forward pass:** Run inputs through the model â†’ get predictions â†’ compute loss.
2. **Backward pass:** Calculate gradients using `.backward()`.
3. **Update parameters:** Use an **optimizer** like `SGD` to move Î¸ in the right direction.
4. **Repeat.**

---

### ğŸ§® **6. Logits â†’ Softmax â†’ Argmax**

- Logits = raw model output per class
- Softmax = converts logits to probabilities
- Argmax = picks the class with the highest probability

---

### ğŸ”„ **7. Learning Rate Schedulers & Gradient Clipping**

- **Scheduler:** Reduces the learning rate after each epoch (to fine-tune learning).
- **Gradient clipping:** Prevents gradients from exploding (literally very large values that destabilize learning).

---

### ğŸ§ª **8. Train / Validation / Test Sets**

- **Train:** Used to *learn* parameters
- **Validation:** Used to *tune* hyperparameters (like learning rate, # of neurons, etc.)
- **Test:** Final evaluation to check real-world performance

---

## âœ… **Recap â€“ Bitty Style**

| Concept | What It Means in Simple Terms |
| --- | --- |
| **Î¸ (parameters)** | The knobs the model turns to improve itself |
| **Loss function** | Tells the model how bad it did |
| **Cross-entropy** | Measures how far off the prediction is from the truth |
| **Softmax** | Turns scores into probabilities |
| **Argmax** | Picks the class with the highest probability |
| **Gradient descent** | The step-by-step update method to reduce errors |
| **Optimizer (SGD)** | Applies those updates in practice |
| **Learning rate** | Controls how fast you update the weights |
| **Gradient clipping** | Keeps things stable when learning gets wild |
| **Train/Val/Test** | Each has its job in helping the model grow & generalize |
| **Monte Carlo sampling** | Averaging over samples to estimate things we canâ€™t measure exactly |