---
id: weight-initialization
title: Weight Initialization in Neural Networks
tags: [machine-learning, deep learning, weights]
---

# ğŸ¯  Weight Initialization in Neural Networks

Youâ€™ve learned how to do forward propagation, backpropagation, and gradient descent. But now comes a crucial design decision: **How do we initialize the weights?**

## â„ï¸ What Happens If You Initialize All Weights to Zero?

### ğŸ§  Summary:

If all weights in a layer are initialized to 0, all neurons in that layer will compute the exact same output. As a result, during training:

- The gradients for each neuron will be the same.
- All neurons will update identically.
- There will be **no diversity**, and they will learn the same features.

This is known as the **symmetry problem** â€” and itâ€™s bad because it defeats the purpose of having multiple neurons.

### ğŸ’¥ Why This Happens:

Neurons are designed to specialize. If you initialize them the same way and give them the same inputs, theyâ€™ll just echo each other.

### âŒ Result:

The model fails to learn complex representations. It might as well be a linear model.

## ğŸ² Random Initialization to the Rescue!

### ğŸ” Whatâ€™s Different:

Instead of setting weights to 0, we randomly initialize them with **small values** (e.g. multiply random values by 0.01).

```
W[1] = np.random.`randn(layer_dims[1], layer_dims[0])` * 0.01
```

This ensures:

- Each neuron starts with different weights
- The model **breaks symmetry**
- Neurons can learn **different features**

### ğŸ“‰ Why Small Values?

If weights are too large:

- Activations like `sigmoid` and `tanh` saturate (flatten out)
- Gradients vanish â†’ learning slows down

If weights are too small:

- It might learn slowly, but at least it learns safely

### âœ… Bonus: Xavier and He Initialization

Modern deep learning uses smarter strategies:

- **Xavier initialization** for `tanh`
- **He initialization** for `ReLU`

These scale weights based on the number of neurons in a layer to maintain stable gradient flow.

## ğŸ” Slide Visual Recap

### Slide 1: Zero Initialization âŒ

- All neurons receive same input â†’ same output â†’ same gradient
- All weights update identically â†’ no symmetry breaking

### Slide 2: Random Initialization âœ…

- Weights are random and small
- Each neuron starts unique
- Diverse features can be learned
- Model trains effectively

## ğŸ§¾ TL;DR

| Initialization | Symmetry Broken? | Risk of Vanishing Gradient | Learning Outcome |
| --- | --- | --- | --- |
| **Zeros** | âŒ No | No | All neurons same |
| **Random Large** | âœ… Yes | âœ… Yes | Unstable training |
| **Random Small** | âœ… Yes | âŒ Safer | Stable training |

## ğŸ“Œ Codex Rule of Thumb:

&gt; "Donâ€™t let neurons be clones. Randomize to specialize."
&gt;