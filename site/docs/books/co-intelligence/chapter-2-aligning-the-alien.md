---
id: chapter-2-aligning-the-alien
title: Aligning the Alien
tags: [books, mollick, alignment, bias, ethics, training-data, copyright]
---

# 🧠 Co-Intelligence – Chapter 2: Aligning the Alien

## 🧭 Summary

Chapter 2 of *Co-Intelligence*, titled **"Aligning the Alien"**, tackles the crucial challenge of aligning artificial intelligence—especially Large Language Models (LLMs)—with human values, norms, and intent. Mollick frames LLMs as **"alien minds"** that can speak fluently in human language while lacking human context, emotion, or understanding. This misalignment between performance and comprehension is at the heart of many risks and responsibilities in current AI development.

---

## 📦 Key Concepts

### 1. 🧠 Why Alignment Is Hard

- LLMs **mimic intelligence**, not embody it. They roleplay based on prompts, not internal understanding.
- Their **"thoughts" and "feelings" are illusions**, crafted through probabilistic pattern generation rather than any real consciousness or intent.
- Yet because they speak so well, we anthropomorphize them—and risk trusting them too easily.

### 2. 📚 Data Hunger and Legal Grey Zones

- AI companies rely on **vast datasets** to train models, many of which contain **copyrighted material**.
- There's a **legal gray area**: the data is not copied directly but used to train weights. This may or may not fall under copyright infringement, and laws vary:
  - **Japan**: Allows AI training under looser copyright interpretations.
  - **EU**: Moving toward stricter data control and opt-out mechanisms.
- High-quality data (books, Wikipedia, research papers) may run out by **2026**, pushing companies toward **lower-quality or synthetic data** sources.

### 3. 🎭 Hallucinations, Bias, and Cultural Echoes

- LLMs reflect the **biases, errors, and contradictions** in their source material.
- They can't inherently distinguish **fact from fiction**, or **literal from figurative language**.
- Example: Models often answer “42” when asked for a random number—a cultural artifact from *The Hitchhiker’s Guide to the Galaxy* that appears often in training data.
- Result: Models **hallucinate**, **echo bias**, and **exaggerate popular narratives**.

### 4. ⚠️ Safety and Exploitability

- LLMs can be **misused** for spear-phishing, misinformation, or harmful instruction.
- Examples include:
  - Prompting an AI to give instructions on causing harm.
  - Scaling of manipulative or abusive outputs.
- Companies currently use **low-paid human labor**, often from the Global South, to fine-tune models and make them less toxic.

### 5. 🧭 Misalignment in Action

- The "paperclip maximizer" scenario is cited as a **thought experiment in AI alignment failure**—a model blindly pursuing a goal to catastrophic ends.
- Even today’s models, though narrow, **can be misaligned** if not supervised, monitored, or deployed carefully.

### 6. 👥 Society’s Role in Shaping AI

- Alignment isn't just technical—it’s **societal**.
- Public understanding of AI is **crucial**, so citizens can pressure companies and governments to steer AI toward **human-centered values**.
- Decisions made today will shape **generational trajectories**.

---

## 🧩 Core Insight

> **LLMs are alien minds trained on human data—but they don’t know what they know. Aligning them requires more than guardrails. It requires human judgment, legal frameworks, cultural awareness, and wide-scale education.**
